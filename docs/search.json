[
  {
    "objectID": "talks/zenith_seminar_2024.html#introduction",
    "href": "talks/zenith_seminar_2024.html#introduction",
    "title": "Disease and climate data fusion for modeling",
    "section": "Introduction",
    "text": "Introduction\n\nPostdoctoral researcher at Inria, Zenith team\nMaster’s at Public Health, Doctorate on Health Information\nData Science applied to Public Health"
  },
  {
    "objectID": "talks/zenith_seminar_2024.html#climate-sensitive-diseases",
    "href": "talks/zenith_seminar_2024.html#climate-sensitive-diseases",
    "title": "Disease and climate data fusion for modeling",
    "section": "Climate sensitive diseases",
    "text": "Climate sensitive diseases\n\nDirect relationship: floods, droughts, heat waves…\nIndirect relationship\n\n\n\n\n\n\n%%{\n  init: {\n    'theme': 'base',\n    'themeVariables': {\n      'fontSize': '30px'\n    }\n  }\n}%%\nflowchart LR\n\nclimate(Climate) --&gt; vector(Disease vectors) --&gt; health(Human health)\nclimate --&gt; health\nclimate --&gt; social(Social & economic \\n determinants) --&gt; health"
  },
  {
    "objectID": "talks/zenith_seminar_2024.html#a-time-lagged-relationship",
    "href": "talks/zenith_seminar_2024.html#a-time-lagged-relationship",
    "title": "Disease and climate data fusion for modeling",
    "section": "A time-lagged relationship",
    "text": "A time-lagged relationship\n\n\n\nVector life cycle from a time perspective\nClimate conditions from the past leads to the disease incidence of tomorrow"
  },
  {
    "objectID": "talks/zenith_seminar_2024.html#climate-data",
    "href": "talks/zenith_seminar_2024.html#climate-data",
    "title": "Disease and climate data fusion for modeling",
    "section": "Climate data",
    "text": "Climate data\n\n\n\nData sources\n\nIn situ: Weather stations, rain gauges\nRemote: Satellites, drones\n\n\nData products\n\nStatistical surface interpolations\nModel reanalysis"
  },
  {
    "objectID": "talks/zenith_seminar_2024.html#era5-land-reanalysis",
    "href": "talks/zenith_seminar_2024.html#era5-land-reanalysis",
    "title": "Disease and climate data fusion for modeling",
    "section": "ERA5-Land reanalysis",
    "text": "ERA5-Land reanalysis\n\n\n\nCopernicus, ECMWF\nGlobal coverage\nHourly data\n1950 to the present (one week delay)\nSpatial resolution ~9km\nSeveral climate indicators"
  },
  {
    "objectID": "talks/zenith_seminar_2024.html#data-structures",
    "href": "talks/zenith_seminar_2024.html#data-structures",
    "title": "Disease and climate data fusion for modeling",
    "section": "Data structures",
    "text": "Data structures\n\nClimate indicators: grid data\nDisease incidence: tabular, individual cases aggregated by spatial regions and time spans"
  },
  {
    "objectID": "talks/zenith_seminar_2024.html#fusioning-data",
    "href": "talks/zenith_seminar_2024.html#fusioning-data",
    "title": "Disease and climate data fusion for modeling",
    "section": "Fusioning data",
    "text": "Fusioning data"
  },
  {
    "objectID": "talks/zenith_seminar_2024.html#case-example",
    "href": "talks/zenith_seminar_2024.html#case-example",
    "title": "Disease and climate data fusion for modeling",
    "section": "Case example",
    "text": "Case example\nZonal Statistics of Climate Indicators from ERA5-Land for Brazilian Municipalities\n\n\n8 climate indicators: maximum, minimum and average temperature, total precipitation, surface pressure, dewpoint, \\(u\\) and \\(v\\) components of wind\n\n6 zonal statistics computation for the 5,570 Brazilian municipalities\n\nMinimum, maximum, average, sum, standard deviation, cell count\n\n\n\nTime coverage: 1950-2022, daily data"
  },
  {
    "objectID": "talks/zenith_seminar_2024.html#workflow",
    "href": "talks/zenith_seminar_2024.html#workflow",
    "title": "Disease and climate data fusion for modeling",
    "section": "Workflow",
    "text": "Workflow\n\n\n\n\n\n%%{\n  init: {\n    'theme': 'base',\n    'themeVariables': {\n      'fontSize': '30px'\n    }\n  }\n}%%\nflowchart TD\n\nera5(ERA5-Land \\n indicators) --&gt; hdata(Hourly data)\nbb(Latin America \\n bounding box) --&gt; hdata\n\nhdata --&gt; agg(Aggregation to \\n daily data)\n\nagg --&gt; mun(Municipal boundaries)\nmun --&gt; zs(Zonal statistics)"
  },
  {
    "objectID": "talks/zenith_seminar_2024.html#era5-land-daily-datasets",
    "href": "talks/zenith_seminar_2024.html#era5-land-daily-datasets",
    "title": "Disease and climate data fusion for modeling",
    "section": "ERA5-Land Daily datasets",
    "text": "ERA5-Land Daily datasets\n\n\n\nOpen data, available at Zenodo\n7,105 files, 658.7 GB\nReproducible R scripts\nPlans to continuously update this dataset and add more indicators\n\n\n\n\n\n\n\nhttps://rfsaldanha.github.io/data-projects/era5land-daily-latin-america.html"
  },
  {
    "objectID": "talks/zenith_seminar_2024.html#zonal-statistics",
    "href": "talks/zenith_seminar_2024.html#zonal-statistics",
    "title": "Disease and climate data fusion for modeling",
    "section": "Zonal statistics",
    "text": "Zonal statistics\n\n\n\nChallenges to handle the amount of data and computational tasks\n\nStrategy\n\nGroup the tasks into chunks and compute in parallel\n\nDAG (Directed-Acyclic Graph) approach to orchestrate computation, with the {targets} package\nSave results into columnar-oriented databases for fast data retrieval (duckdb and parquet)"
  },
  {
    "objectID": "talks/zenith_seminar_2024.html#section",
    "href": "talks/zenith_seminar_2024.html#section",
    "title": "Disease and climate data fusion for modeling",
    "section": "",
    "text": "6,085,749,761 records"
  },
  {
    "objectID": "talks/zenith_seminar_2024.html#temperature",
    "href": "talks/zenith_seminar_2024.html#temperature",
    "title": "Disease and climate data fusion for modeling",
    "section": "Temperature",
    "text": "Temperature"
  },
  {
    "objectID": "talks/zenith_seminar_2024.html#precipitation",
    "href": "talks/zenith_seminar_2024.html#precipitation",
    "title": "Disease and climate data fusion for modeling",
    "section": "Precipitation",
    "text": "Precipitation\n\n\n\n\n\n\n\n\n\n\n\n\nRio de Janeiro municipalities. January 1, 2010."
  },
  {
    "objectID": "talks/zenith_seminar_2024.html#angra-dos-reis",
    "href": "talks/zenith_seminar_2024.html#angra-dos-reis",
    "title": "Disease and climate data fusion for modeling",
    "section": "Angra dos Reis",
    "text": "Angra dos Reis"
  },
  {
    "objectID": "talks/zenith_seminar_2024.html#resolution-and-spatial-variability",
    "href": "talks/zenith_seminar_2024.html#resolution-and-spatial-variability",
    "title": "Disease and climate data fusion for modeling",
    "section": "Resolution and spatial variability",
    "text": "Resolution and spatial variability\n\n\n\nBrazilian municipalities size variation\n\nAltamira (PA): 159,533 km2\nSanta Cruz de Minas (MG): 3 km2\n\n\nERA5-Land cell: ~ 100 km2"
  },
  {
    "objectID": "talks/zenith_seminar_2024.html#published-paper",
    "href": "talks/zenith_seminar_2024.html#published-paper",
    "title": "Disease and climate data fusion for modeling",
    "section": "Published paper",
    "text": "Published paper\n\n\n\nEnvironmental Data Science journal\nPublished on February 8, 2024\nJournal’s most read paper of the month\nMore than 9,000 datasets downloads on Zenodo so far\nInria has an agreement for APC fees with the Cambridge University Press\nSwift submission process"
  },
  {
    "objectID": "talks/zenith_seminar_2024.html#brclimr-r-package",
    "href": "talks/zenith_seminar_2024.html#brclimr-r-package",
    "title": "Disease and climate data fusion for modeling",
    "section": "brclimr R package",
    "text": "brclimr R package\n\n\n\n\nPackage to retrieve climate data of Brazilian municipalities\nQuery remote parquet files stored on a S3 system\nAvoid huge dataset downloads when user wants only a subset of the data\nAvailable on CRAN. More than 3,000 downloads\n\n\nbrclimr::fetch_data(\n    code_muni = 3304557,\n    product = \"brdwgd\",\n    indicator = \"rh\",\n    statistics = \"mean\",\n    date_start = as.Date(\"2010-10-15\"),\n    date_end = as.Date(\"2010-10-20\")\n  )"
  },
  {
    "objectID": "talks/zenith_seminar_2024.html#usage",
    "href": "talks/zenith_seminar_2024.html#usage",
    "title": "Disease and climate data fusion for modeling",
    "section": "Usage",
    "text": "Usage\n\n\n\nTraining multivariate machine learning models to forecast dengue incidence in Brazil with a subsets strategy\n\n\n\\[\n{\\small\n\\begin{aligned}        D_t = \\mu + & \\theta_1 D_{t-1} + \\cdots + \\theta_p D_{t-p} + \\\\        & \\lambda_1 C_{t-1} + \\cdots + \\lambda_p C_{t-p} + \\\\         & \\varepsilon_1 e_{t-1} + \\cdots + \\varepsilon_p e_{t-p}    \\end{aligned}}\n\\]\n\nCluster municipalities based on dengue spread and climate regimes\nTrain different models for each partition\n\n\n\nAccepted paper on ICDE2024, Multivariate Time Series Analytics workshop"
  },
  {
    "objectID": "talks/zenith_seminar_2024.html#next-steps",
    "href": "talks/zenith_seminar_2024.html#next-steps",
    "title": "Disease and climate data fusion for modeling",
    "section": "Next steps…",
    "text": "Next steps…\n\n\n\nContinuous update\nHuman settlements, population-weighted zonal statistics\nCompute climate time-series features: heat waves, persistent rains, etc.\nAdopt climate products with finer resolutions when possible (CHIRPS)\nExpand results to other countries"
  },
  {
    "objectID": "talks/zenith_seminar_2024.html#section-1",
    "href": "talks/zenith_seminar_2024.html#section-1",
    "title": "Disease and climate data fusion for modeling",
    "section": "",
    "text": "Thanks"
  },
  {
    "objectID": "talks/zenith_seminar_2024.html#backup-slides",
    "href": "talks/zenith_seminar_2024.html#backup-slides",
    "title": "Disease and climate data fusion for modeling",
    "section": "Backup slides",
    "text": "Backup slides"
  },
  {
    "objectID": "talks/zenith_seminar_2024.html#climate-reanalysis",
    "href": "talks/zenith_seminar_2024.html#climate-reanalysis",
    "title": "Disease and climate data fusion for modeling",
    "section": "Climate reanalysis",
    "text": "Climate reanalysis"
  },
  {
    "objectID": "talks/zenith_seminar_2024.html#era5-land-hourly-to-daily-aggregation",
    "href": "talks/zenith_seminar_2024.html#era5-land-hourly-to-daily-aggregation",
    "title": "Disease and climate data fusion for modeling",
    "section": "ERA5-Land hourly to daily aggregation",
    "text": "ERA5-Land hourly to daily aggregation\n\nUsage of {KrigR} package to access the Copernicus Climate Data Store API, crop data at server side, download and perform the time aggregation.\n\n\ndownload_ERA(Variable = \"2m_temperature\", DataSet = \"era5-land\", \n             DateStart = \"2022-12-01\", DateStop = \"2022-12-31\",\n             TResolution = \"day\", TStep = 1,\n             FUN = \"max\",\n             Extent = extent(c(-118.47,-34.1,-56.65, 33.28)), \n             Dir = \"dir_name\", FileName = \"file_name.nc\", \n             API_User = \"api_user\", API_Key = \"api_key\")\n\n\nTook ~15 days to download and process the data from 8 climate indicators covering the Latin America region"
  },
  {
    "objectID": "talks/zenith_seminar_2024.html#computation",
    "href": "talks/zenith_seminar_2024.html#computation",
    "title": "Disease and climate data fusion for modeling",
    "section": "Computation",
    "text": "Computation\n\nZonal statistics weighted by the fraction of the cell that is covered, with the {exactextractr} package\n\n\nexact_extract(\n  x = rst,\n  y = pol,\n  fun = \"mean\"\n)"
  },
  {
    "objectID": "talks/zenith_seminar_2024.html#results",
    "href": "talks/zenith_seminar_2024.html#results",
    "title": "Disease and climate data fusion for modeling",
    "section": "Results",
    "text": "Results\n\n\n\n\n\n\n\nERA5-Land indicators\nDaily time-aggregating functions\nSpatial zonal statistics\n\n\n\nTemperature (2m)\nmean, max, min\nmax, min, stdev, count\n\n\nDewpoint temp. (2m)\nmean\nmax, min, stdev, count\n\n\n\n\\(u\\) component of wind\nmean\nmax, min, stdev, count\n\n\n\n\\(v\\) component of wind\nmean\nmax, min, stdev, count\n\n\nSurface pressure\nmean\nmax, min, stdev, count\n\n\nTotal precipitation\nsum\nmax, min, stdev, count, sum"
  },
  {
    "objectID": "talks/isntd_conference_2023.html#introduction",
    "href": "talks/isntd_conference_2023.html#introduction",
    "title": "Disease and climate data fusion for modelling",
    "section": "Introduction",
    "text": "Introduction\n\nPostdoc researcher at Inria, a French research institute for digital science and technology\nBSC, Global Health Resilience collaborator\nFiocruz, Climate and Health Observatory"
  },
  {
    "objectID": "talks/isntd_conference_2023.html#climate-sensitive-diseases",
    "href": "talks/isntd_conference_2023.html#climate-sensitive-diseases",
    "title": "Disease and climate data fusion for modelling",
    "section": "Climate sensitive diseases",
    "text": "Climate sensitive diseases\n\nDirect relationship: floods, droughts, heat waves…\nIndirect relationship\n\n\n\n\n\n\n%%{\n  init: {\n    'theme': 'base',\n    'themeVariables': {\n      'fontSize': '30px'\n    }\n  }\n}%%\nflowchart LR\n\nclimate(Climate) --&gt; vector(Disease vectors) --&gt; health(Human health)\nclimate --&gt; health\nclimate --&gt; social(Social & economic \\n determinants) --&gt; health\n\n\n\n\n\n\n\nClimate necessary conditions to vector viability, reproduction and disease transmission efficiency\nClimate indicators may act as proxy variables to vector distribution on statistical models"
  },
  {
    "objectID": "talks/isntd_conference_2023.html#a-time-lagged-relationship",
    "href": "talks/isntd_conference_2023.html#a-time-lagged-relationship",
    "title": "Disease and climate data fusion for modelling",
    "section": "A time-lagged relationship",
    "text": "A time-lagged relationship\n\n\n\nVector life cycle in a time perspective\nClimate conditions from the past leads to the disease incidence of today"
  },
  {
    "objectID": "talks/isntd_conference_2023.html#climate-data",
    "href": "talks/isntd_conference_2023.html#climate-data",
    "title": "Disease and climate data fusion for modelling",
    "section": "Climate data",
    "text": "Climate data\n\nData sources\n\nWeather stations, rain gauges\nSatellites\n\nData products\n\nStatistical surface interpolations\nModel reanalysis"
  },
  {
    "objectID": "talks/isntd_conference_2023.html#era5-land-reanalysis",
    "href": "talks/isntd_conference_2023.html#era5-land-reanalysis",
    "title": "Disease and climate data fusion for modelling",
    "section": "ERA5-Land reanalysis",
    "text": "ERA5-Land reanalysis\n\n\n\nCopernicus, ECMWF\nGlobal coverage\nHourly data\n1950 to the present (one week lag)\nSpatial resolution ~9km\nSeveral climate indicators"
  },
  {
    "objectID": "talks/isntd_conference_2023.html#data-structures",
    "href": "talks/isntd_conference_2023.html#data-structures",
    "title": "Disease and climate data fusion for modelling",
    "section": "Data structures",
    "text": "Data structures\n\nClimate indicators: grid data\nDisease incidence: tabular, individual cases aggregated by spatial regions and time spans"
  },
  {
    "objectID": "talks/isntd_conference_2023.html#fusioning-data",
    "href": "talks/isntd_conference_2023.html#fusioning-data",
    "title": "Disease and climate data fusion for modelling",
    "section": "Fusioning data",
    "text": "Fusioning data"
  },
  {
    "objectID": "talks/isntd_conference_2023.html#case-example",
    "href": "talks/isntd_conference_2023.html#case-example",
    "title": "Disease and climate data fusion for modelling",
    "section": "Case example",
    "text": "Case example\nZonal Statistics of Climate Indicators from ERA5-Land for Brazilian Municipalities, 1950-2022\n\nERA5-Land hourly data to daily aggregates\nAverage, maximum and minimum temperature, total precipitation\nSurface pressure, dewpoint, \\(u\\) and \\(v\\) components of wind\nZonal statistics computation for the 5,570 Brazilian municipalities\n\nMinimum, maximum, average, sum, standard deviation, cell count"
  },
  {
    "objectID": "talks/isntd_conference_2023.html#workflow",
    "href": "talks/isntd_conference_2023.html#workflow",
    "title": "Disease and climate data fusion for modelling",
    "section": "Workflow",
    "text": "Workflow\n\n\n\n\n\n%%{\n  init: {\n    'theme': 'base',\n    'themeVariables': {\n      'fontSize': '30px'\n    }\n  }\n}%%\nflowchart TD\n\nera5(ERA5-Land \\n indicators) --&gt; hdata(Hourly data)\nbb(Latin America \\n bounding box) --&gt; hdata\n\nhdata --&gt; agg(Daily aggregated \\n data)\n\nagg --&gt; mun(Municipal boundaries)\nmun --&gt; zs(Zonal statistics)"
  },
  {
    "objectID": "talks/isntd_conference_2023.html#results",
    "href": "talks/isntd_conference_2023.html#results",
    "title": "Disease and climate data fusion for modelling",
    "section": "Results",
    "text": "Results\n\n\n\n\n\n\n\n\nERA5-Land indicators\nDaily time-aggregating functions\nSpatial zonal statistics\n\n\n\n\nTemperature (2m)\nmean, max, min\nmax, min, stdev, count\n\n\nDewpoint temp. (2m)\nmean\nmax, min, stdev, count\n\n\n\\(u\\) component of wind\nmean\nmax, min, stdev, count\n\n\n\\(v\\) component of wind\nmean\nmax, min, stdev, count\n\n\nSurface pressure\nmean\nmax, min, stdev, count\n\n\nTotal precipitation\nsum\nmax, min, stdev, count, sum"
  },
  {
    "objectID": "talks/isntd_conference_2023.html#average-temperature",
    "href": "talks/isntd_conference_2023.html#average-temperature",
    "title": "Disease and climate data fusion for modelling",
    "section": "Average temperature",
    "text": "Average temperature"
  },
  {
    "objectID": "talks/isntd_conference_2023.html#next-steps",
    "href": "talks/isntd_conference_2023.html#next-steps",
    "title": "Disease and climate data fusion for modelling",
    "section": "Next steps…",
    "text": "Next steps…\n\nContinuous update\nHuman settlements, population-weighted zonal statistics\nCompute climate time-series features: heat waves, persistent rains, etc.\nExpand methodology to other countries"
  },
  {
    "objectID": "talks/isntd_conference_2023.html#thanks",
    "href": "talks/isntd_conference_2023.html#thanks",
    "title": "Disease and climate data fusion for modelling",
    "section": "Thanks!",
    "text": "Thanks!\nContact, data links, R packages and short tutorials available at rfsaldanha.github.io"
  },
  {
    "objectID": "talks/dengue_class.html#the-case-itinerary",
    "href": "talks/dengue_class.html#the-case-itinerary",
    "title": "Dengue cases screening",
    "section": "The case itinerary",
    "text": "The case itinerary\n\nAfter a clinical assessement, a patient can be pointed as a suspected dengue/chikungunya case, based on its symptoms, clinical conditions and history (anamnesis)\n\nFever, arthralgia, myalgia, headache, and retroorbital pain\n\nThe suspected case is immediately notified to the health authorities (SINAN dataset)\nThe diagnosis must be verified to adequate the health treatment"
  },
  {
    "objectID": "talks/dengue_class.html#possible-case-classifications",
    "href": "talks/dengue_class.html#possible-case-classifications",
    "title": "Dengue cases screening",
    "section": "Possible case classifications",
    "text": "Possible case classifications\n\nSuspected case\nConfirmed case: dengue or chikungunya\nDiscarded case"
  },
  {
    "objectID": "talks/dengue_class.html#how-a-case-is-confirmed-or-discarded",
    "href": "talks/dengue_class.html#how-a-case-is-confirmed-or-discarded",
    "title": "Dengue cases screening",
    "section": "How a case is confirmed or discarded?",
    "text": "How a case is confirmed or discarded?\n\nOn normal conditions\n\nPrimarly by laboratory exams (blood tests)\nNucleic acid amplification tests, serologic tests, and IgG antibody tests.\n\nOn outbreaks\n\nBy laboratorial test, if available\nOr by the doctor’s judgment, based on symptoms, clinical conditions and the patient history"
  },
  {
    "objectID": "talks/dengue_class.html#dengue-cases-diagnosis-and-criteria",
    "href": "talks/dengue_class.html#dengue-cases-diagnosis-and-criteria",
    "title": "Dengue cases screening",
    "section": "Dengue cases diagnosis and criteria",
    "text": "Dengue cases diagnosis and criteria"
  },
  {
    "objectID": "talks/dengue_class.html#dengue-cases-diagnosis-and-criteria-1",
    "href": "talks/dengue_class.html#dengue-cases-diagnosis-and-criteria-1",
    "title": "Dengue cases screening",
    "section": "Dengue cases diagnosis and criteria",
    "text": "Dengue cases diagnosis and criteria\n\n\n\nDiagnosis\nLaboratory\nClinical\n\n\n\n\nPositive Dengue\n3,624,320\n6,130,030\n\n\nPositive Chikungunya\n300,429\n502,977\n\n\nDiscarded\n1,304,000\n1,052,512"
  },
  {
    "objectID": "talks/dengue_class.html#research-objectives",
    "href": "talks/dengue_class.html#research-objectives",
    "title": "Dengue cases screening",
    "section": "Research objectives",
    "text": "Research objectives\n\nTrain a ML model to predict a patient’s diagnosis based on its symptoms and clinical conditions\nEvaluate the performance of the model for cases screening during dengue outbreaks\nAssess the human performance on diagnosis based on the trained model"
  },
  {
    "objectID": "talks/dengue_class.html#previous-works",
    "href": "talks/dengue_class.html#previous-works",
    "title": "Dengue cases screening",
    "section": "Previous works",
    "text": "Previous works\n\nNeto (2022) present a systematic literature review on ML and DL techniques to support clinical diagnoses of arboviral diseases.\n\nCurrent research is focused on binary classification of Dengue diagnosis.\nNone of the reviewed papers were trained to predict Dengue and or Chikungunya diagnosis with Brazilian data\nMaximum number of records of 14,019 was used for training and testing ML models"
  },
  {
    "objectID": "talks/dengue_class.html#previous-works-1",
    "href": "talks/dengue_class.html#previous-works-1",
    "title": "Dengue cases screening",
    "section": "Previous works",
    "text": "Previous works\n\nTabosa (2022) present a comparative study of ML techniques for multi-class classification of arboviral diseases\n\nData restricted to Amazonas and the city of Recife, Pernambuco, from 2015 to 2020.\nGradient-boosting learners offered the best performance\nNo learner achieved an accuracy above 70%"
  },
  {
    "objectID": "talks/dengue_class.html#previous-works-2",
    "href": "talks/dengue_class.html#previous-works-2",
    "title": "Dengue cases screening",
    "section": "Previous works",
    "text": "Previous works\n\nBohm (2024) present a model for binary classification with 98% accuracy\n\nData restricted to Rio de Janeiro and Minas Gerais states\nSample of 20,000 records\nPossible discrepancies between paper methodology and available data from author"
  },
  {
    "objectID": "talks/dengue_class.html#study-design",
    "href": "talks/dengue_class.html#study-design",
    "title": "Dengue cases screening",
    "section": "Study design",
    "text": "Study design\n\nCreate two datasets: \\(D_a\\) with cases that received diagnosis from laboratorial exams, and \\(D_b\\) containing only cases diagnosed solely on symptoms and clinical conditions\nTrain different ML learners on \\(D_a\\) based on the patient’s symptoms and clinical conditions\nModel evaluation\nApply the selected model on \\(D_b\\)\nCompare the results"
  },
  {
    "objectID": "talks/dengue_class.html#preliminary-results",
    "href": "talks/dengue_class.html#preliminary-results",
    "title": "Dengue cases screening",
    "section": "Preliminary results",
    "text": "Preliminary results\n\n\\(D_a\\) with 50,000 ramdom samples for each class: positive Dengue, positive Chikungunya, and discarded cases\nTraining dataset with 3/4 proportion, 5 folds\nLearners: XG-Boosting, Random Forest, KNN, MLP, Logistic Regression, and Regularized Discriminant Analysis (RDA) model\nThree cumulative model especifications: symptoms and clinical conditions, weather season and geographical region (state)"
  },
  {
    "objectID": "talks/dengue_class.html#best-model-so-far",
    "href": "talks/dengue_class.html#best-model-so-far",
    "title": "Dengue cases screening",
    "section": "Best model, so far",
    "text": "Best model, so far\n\nComplete model especification\nXG-Boost learner\n64,28% accurracy\n64,28% sensibility\n82,14% specificity\n81,73% ROC AUC"
  },
  {
    "objectID": "talks/dengue_class.html#confusion-matrix",
    "href": "talks/dengue_class.html#confusion-matrix",
    "title": "Dengue cases screening",
    "section": "Confusion matrix",
    "text": "Confusion matrix\n                      Truth\nPrediction             Discarded Positive Chikungunya Positive Dengue\n  Discarded                 5440                 1177            2415\n  Positive Chikungunya      2788                 9942            1362\n  Positive Dengue           4272                 1381            8723"
  },
  {
    "objectID": "talks/dengue_class.html#human-vs-machine",
    "href": "talks/dengue_class.html#human-vs-machine",
    "title": "Dengue cases screening",
    "section": "Human vs machine",
    "text": "Human vs machine"
  },
  {
    "objectID": "talks/dengue_class.html#thoughts",
    "href": "talks/dengue_class.html#thoughts",
    "title": "Dengue cases screening",
    "section": "Thoughts",
    "text": "Thoughts\n\nSensitivity: ability to designate an individual with disease as positive\nSpecificity: ability to designate an individual who does not have a disease as negative\nFor cases screening, a high specificity is desired: ruled-in cases can be directed to laboratory diagnosis for confirmation"
  },
  {
    "objectID": "talks/dengue_class.html#strenghts-of-the-paper-so-far",
    "href": "talks/dengue_class.html#strenghts-of-the-paper-so-far",
    "title": "Dengue cases screening",
    "section": "Strenghts of the paper, so far",
    "text": "Strenghts of the paper, so far\n\nCountry-wide representative sample\nRelatively high specificity model\nMore realistic discussion on ML models for cases screening\nNew discussion on human performance"
  },
  {
    "objectID": "talks/climate_br.html#overview",
    "href": "talks/climate_br.html#overview",
    "title": "Climate data for Brazilian municipalities",
    "section": "Overview",
    "text": "Overview\n\nClimate raw data sources: ground level and reanalysis\nData on Braziliam municipalities spatial unit\nClimatological normals and indicators"
  },
  {
    "objectID": "talks/climate_br.html#ground-level-climate-data",
    "href": "talks/climate_br.html#ground-level-climate-data",
    "title": "Climate data for Brazilian municipalities",
    "section": "Ground-level climate data",
    "text": "Ground-level climate data\n\n\n\nWeather stations on Cametá and Mocajuba (north region)\nData every 5 minutes, sent to Fiocruz Postgres server\n\n{plugfieldapi} package to retrieve data from stations with daily e-mail reports\nError detection and notification for problems\n\n\nVideo"
  },
  {
    "objectID": "talks/climate_br.html#reanalysis-climate-data",
    "href": "talks/climate_br.html#reanalysis-climate-data",
    "title": "Climate data for Brazilian municipalities",
    "section": "ReAnalysis climate data",
    "text": "ReAnalysis climate data\n\n\n\n\nERA5-Land: global coverage, hourly and daily data, regular updates\n\nBR-DWGD: Brazil coverage, daily data, sensible to extreme events, sporadic updates\n\nTerraClimate: global coverage, monthly data, higher resolution (~4km), regular updates\n\n\n\nMore details here"
  },
  {
    "objectID": "talks/climate_br.html#climate-data-for-brazilian-municipalities",
    "href": "talks/climate_br.html#climate-data-for-brazilian-municipalities",
    "title": "Climate data for Brazilian municipalities",
    "section": "Climate data for Brazilian municipalities",
    "text": "Climate data for Brazilian municipalities\n\nZonal statistics computation\nAdoption of exactextractr package for cell’s coverage weighted computations\nCreation of package {zonalclim} with helper functions to compute scalable zonal statistics with chunks strategy\nDAG system using the targets package to compute climate zonal statistics for Brazilian municipalities\nPublication on Environmental Data Science journal\n\nMore details here"
  },
  {
    "objectID": "talks/climate_br.html#example",
    "href": "talks/climate_br.html#example",
    "title": "Climate data for Brazilian municipalities",
    "section": "Example",
    "text": "Example"
  },
  {
    "objectID": "talks/climate_br.html#climatological-normals",
    "href": "talks/climate_br.html#climatological-normals",
    "title": "Climate data for Brazilian municipalities",
    "section": "Climatological normals",
    "text": "Climatological normals\n\nClimatological normals are computed only for weather stations and municipal references are needed for better climate change understanding\nNormals computed for each Brazilian municipality using the Zonal BR-DWGD from 1961 to 1990\n\nMean, 10th and 90th percentile\nTemperature (max, min), precipitation, relative humidity, solar radiation, wind speed, and evapotranspiration"
  },
  {
    "objectID": "talks/climate_br.html#time-aggregated-indicators",
    "href": "talks/climate_br.html#time-aggregated-indicators",
    "title": "Climate data for Brazilian municipalities",
    "section": "Time-aggregated indicators",
    "text": "Time-aggregated indicators\n\nMunicipal daily data time series to monthly indicators\nMonthly statistics\n\nAverage, median, standard deviation, standard error, maximum and minimum values, and percentiles\n\n\nOccurence of events\n\n22 indicators, in reference to normals or count sequence. Creation of nseq and climindi packages\nHeat waves, cold spells, count of warm days, count of dry and wet days, and others\n\n\n\nMore details here."
  },
  {
    "objectID": "talks/climate_br.html#future-work",
    "href": "talks/climate_br.html#future-work",
    "title": "Climate data for Brazilian municipalities",
    "section": "Future work",
    "text": "Future work\n\nDashboards for weather stations\nCompute zonal statistics with populated areas weights\nCreate other time-aggregated indicators\nTest the methodologies on other countries\nCompare ground-level data from weather stations with climate reanalysis datasets"
  },
  {
    "objectID": "talks/bda_subsets_2024.html#introduction",
    "href": "talks/bda_subsets_2024.html#introduction",
    "title": "Subset Models for Multivariate Time Series Forecast",
    "section": "Introduction",
    "text": "Introduction\n\nPreviously presented at MulTiSA workshop (ICDE 2024)\nAbundant multivariate time series, good opportunity for forecasting machine learning methods\nData may present intrinsic diversity of samples, affecting model’s performance on different parts of the input\nGlobal models: use all available time series\nLocal models: use only time series pertaining to each sample\nData subsets models: our proposal"
  },
  {
    "objectID": "talks/bda_subsets_2024.html#case-example",
    "href": "talks/bda_subsets_2024.html#case-example",
    "title": "Subset Models for Multivariate Time Series Forecast",
    "section": "Case example",
    "text": "Case example\n\nDengue disease is transmitted by mosquitoes and is a Public Health concern. Record number cases on 2024 in Brazil, tendency to increase with climate change\nA typical forecasting model is targeted to predict number of cases based on climate indicators (rain and temperature)\nA global model would use data from all municipalities, facing difficulties related to distinct temporal and spatial disease transmission patterns"
  },
  {
    "objectID": "talks/bda_subsets_2024.html#objective",
    "href": "talks/bda_subsets_2024.html#objective",
    "title": "Subset Models for Multivariate Time Series Forecast",
    "section": "Objective",
    "text": "Objective\n\nPropose a subset modeling framework\nAccommodate regional variations across diverse units (e.g. municipalities)\nCost-effective training with robust prediction capabilities in comparison with global models"
  },
  {
    "objectID": "talks/bda_subsets_2024.html#framework-proposal",
    "href": "talks/bda_subsets_2024.html#framework-proposal",
    "title": "Subset Models for Multivariate Time Series Forecast",
    "section": "Framework proposal",
    "text": "Framework proposal\n\n\n\nIdentify subsets within the dataset with similar patterns\nTrain models for each subset\nUse the model trained on the subset data for prediction"
  },
  {
    "objectID": "talks/bda_subsets_2024.html#datasets",
    "href": "talks/bda_subsets_2024.html#datasets",
    "title": "Subset Models for Multivariate Time Series Forecast",
    "section": "Datasets",
    "text": "Datasets\n\nDengue dataset. Weekly cases count, from 2011 to 2020, for 333 municipalities.\nClimate dataset. Average maximum and minimum temperature, total precipitation. Same time and spatial units and coverage, derived from Copernicus ERA5-Land.\nAll indicators were standardized (with zero mean and one SD)"
  },
  {
    "objectID": "talks/bda_subsets_2024.html#experimental-setup",
    "href": "talks/bda_subsets_2024.html#experimental-setup",
    "title": "Subset Models for Multivariate Time Series Forecast",
    "section": "Experimental setup",
    "text": "Experimental setup\n\nIdentify data subsets considering dengue cases and covariates patterns across municipalities with DTW distance.\n\nSelect the optimum number of subsets (\\(k\\)) considering silhouette score\n\nTrain random forest Global Model with and without the subset id feature information\nTrain random forest Subsets Models\nEvaluate forecasting model’s performance on test data"
  },
  {
    "objectID": "talks/bda_subsets_2024.html#clustering-results",
    "href": "talks/bda_subsets_2024.html#clustering-results",
    "title": "Subset Models for Multivariate Time Series Forecast",
    "section": "Clustering results",
    "text": "Clustering results\n\n\n\n\\(k = 5\\) returned the highest silhouette score\nPartition sizes: \\(g_1 = 69\\), \\(g_2 = 62\\), \\(g_3 = 82\\), \\(g_4 = 102\\), \\(g_5 = 18\\)"
  },
  {
    "objectID": "talks/bda_subsets_2024.html#model-results",
    "href": "talks/bda_subsets_2024.html#model-results",
    "title": "Subset Models for Multivariate Time Series Forecast",
    "section": "Model results",
    "text": "Model results"
  },
  {
    "objectID": "talks/bda_subsets_2024.html#conclusions-and-next-steps",
    "href": "talks/bda_subsets_2024.html#conclusions-and-next-steps",
    "title": "Subset Models for Multivariate Time Series Forecast",
    "section": "Conclusions and next steps",
    "text": "Conclusions and next steps\n\nSubsets models rendered better performance than global models on 116 municipalities from 333 (34.83%)\nSubsets models overall performance is related to the partition’s size. Bigger partitions (more municipalities) have more training data.\nWe are currently testing different clustering strategies (including constraints in partition size and featured-based approaches) and investigating different learners performance on model training\nLooking for datasets of different domains. Suggestions?"
  },
  {
    "objectID": "talks/bda_subsets_2024.html#thanks",
    "href": "talks/bda_subsets_2024.html#thanks",
    "title": "Subset Models for Multivariate Time Series Forecast",
    "section": "Thanks!",
    "text": "Thanks!\nContact and more info at\nrfsaldanha.github.io"
  },
  {
    "objectID": "publications/xavierRegioesSaudeNo2019.html#reference",
    "href": "publications/xavierRegioesSaudeNo2019.html#reference",
    "title": "As Regiões de Saúde no Brasil segundo internações: método para apoio na regionalização de saúde",
    "section": "Reference",
    "text": "Reference\n\n\nXAVIER, D. R. et al. As Regiões de Saúde no Brasil segundo internações: método para apoio na regionalização de saúde. Cadernos de Saúde Pública, v. 35, p. e00076118, Jun. 2019."
  },
  {
    "objectID": "publications/xavierRegioesSaudeNo2019.html#abstract",
    "href": "publications/xavierRegioesSaudeNo2019.html#abstract",
    "title": "As Regiões de Saúde no Brasil segundo internações: método para apoio na regionalização de saúde",
    "section": "Abstract",
    "text": "Abstract\nThis study addressed health regionalization on various spatial scales based on patient flow. The article analyzed data through data linkage on the origin and destination of admissions at the municipal level in Brazil in 2016. The analysis is based on graph theory and uses a modularity algorithm that seeks to group municipalities in communities with a large number of interlinks. The algorithm optimizes the number of hospital admissions and discharges, taking patient flow into account. The results are shown, considering different political and administrative spatial structures. Considering patient flow without spatial restrictions, 29 communities were created in the country, compared to 64 communities when the boundaries of the major geographic regions were respected, and 164 when considering only patient flows within the respective states. The results show the importance of historically constituted regions, ignoring formal administrative boundaries, in order to implement access to health services. They also reveal adherence to administrative boundaries in many states of Brazil, demonstrating this spatial scale’s importance in the context of access to hospital admissions. The methodology makes relevant contributions to regional health planning."
  },
  {
    "objectID": "publications/szwarcwaldAdesaoMedidasRestricao2020a.html#reference",
    "href": "publications/szwarcwaldAdesaoMedidasRestricao2020a.html#reference",
    "title": "Adesão às medidas de restrição de contato físico e disseminação da COVID-19 no Brasil",
    "section": "Reference",
    "text": "Reference\n\n\nSZWARCWALD, C. L. et al. Adesão às medidas de restrição de contato físico e disseminação da COVID-19 no Brasil. Epidemiologia e Serviços de Saúde, v. 29, p. e2020432, Nov. 2020."
  },
  {
    "objectID": "publications/szwarcwaldAdesaoMedidasRestricao2020a.html#abstract",
    "href": "publications/szwarcwaldAdesaoMedidasRestricao2020a.html#abstract",
    "title": "Adesão às medidas de restrição de contato físico e disseminação da COVID-19 no Brasil",
    "section": "Abstract",
    "text": "Abstract\nObjetivo: Analisar a adesão da população às medidas de restrição de contato físico e disseminação da COVID-19 no Brasil. Métodos: Inquérito de saúde, realizado pela internet, com amostragem em cadeia, no período de 24 de abril a 24 de maio de 2020. A intensidade da adesão à restrição de contato físico foi analisada segundo características sociodemográficas, utilizando-se modelos de regressão logística para investigar associações com ‘Nenhuma/pouca adesão’. Resultados: Dos 45.161 participantes, 74,2% (73,8-74,6%) relataram intensa adesão às medidas. O grupo que não aderiu às medidas foi composto homens (31,7%), com idade de 30 a 49 anos (36,4%), baixa escolaridade (33,0%), trabalhando durante a pandemia (81,3%), residentes nas regiões Norte (28,1%) e Centro-Oeste (28,5%) do país. Houve importante redução das taxas de crescimento diário, de 45,4 para 5,0%. Conclusão: Grande parte da população brasileira aderiu às medidas de restrição de contato físico, o que, possivelmente, contribuiu para reduzir a disseminação da COVID-19."
  },
  {
    "objectID": "publications/soutoDatasetInfantMortality2023.html#reference",
    "href": "publications/soutoDatasetInfantMortality2023.html#reference",
    "title": "Dataset on infant mortality rates in Brazil",
    "section": "Reference",
    "text": "Reference\n\n\nSOUTO, G. et al. Dataset on infant mortality rates in Brazil. BMC Research Notes, v. 16, n. 1, p. 149, Jul. 2023."
  },
  {
    "objectID": "publications/soutoDatasetInfantMortality2023.html#abstract",
    "href": "publications/soutoDatasetInfantMortality2023.html#abstract",
    "title": "Dataset on infant mortality rates in Brazil",
    "section": "Abstract",
    "text": "Abstract\n\nObjectives\nSurveillance of infant and fetal deaths is of paramount importance in thinking about government strategies to reduce these rates, provide greater visibility of these mortality figures in the country, enable the adoption of prevention measures, as well as contribute to a better record of deaths.\n\n\nData description\nThe dataset comprises fetal, neonatal, early neonatal, late neonatal, and perinatal Mortality Rates of Brazilian municipalities with their respective information, between 2010 to 2020, aggregated by epidemiological week."
  },
  {
    "objectID": "publications/sallesComprehensiveIntegratedDataset2023a.html#reference",
    "href": "publications/sallesComprehensiveIntegratedDataset2023a.html#reference",
    "title": "A comprehensive integrated dataset on Brazilian health facilities: from 2005 to 2021",
    "section": "Reference",
    "text": "Reference\n\n\nSALLES, R. et al. A comprehensive integrated dataset on Brazilian health facilities: From 2005 to 2021. BMC Research Notes, v. 16, n. 1, p. 151, Jul. 2023."
  },
  {
    "objectID": "publications/sallesComprehensiveIntegratedDataset2023a.html#abstract",
    "href": "publications/sallesComprehensiveIntegratedDataset2023a.html#abstract",
    "title": "A comprehensive integrated dataset on Brazilian health facilities: from 2005 to 2021",
    "section": "Abstract",
    "text": "Abstract\n\nObjectives\nThe National Registry of Healthcare Facilities is a system with the registry of every healthcare facility in Brazil with information on the capacity building and healthcare workforce regarding its public or private nature. Despite being publicly available, it can only be accessed in separated disjoint tables, with different primary units of analysis. The objective is to offer an interoperable dataset containing monthly data from 2005 to 2021 with information on healthcare facilities, including their physical and human resources, services and teams, enriched with municipal information.\n\n\nData description\nDatabase with historical data and geographic information for each health facility in Brazil. It is composed by 5 distinct tables, organized according to combinations of time, space, and types of resources, services and teams. This database opens up a range of possibilities for research topics, from case studies in a single health facility and period, analysis of a group of health facilities with characteristics of interest, to a broader study using the entire dataset and aggregated data by municipality. Furthermore, the fact that there is a row for each health facility/month/ year facilitates the integration with other datasets from the Brazilian healthcare system. In addition to being a potential object of study in the health area, the dataset is also convenient in data science, especially for studies focused on time series."
  },
  {
    "objectID": "publications/saldanhaSubsetModelsMultivariate2024a.html#reference",
    "href": "publications/saldanhaSubsetModelsMultivariate2024a.html#reference",
    "title": "Subset Models for Multivariate Time Series Forecast",
    "section": "Reference",
    "text": "Reference\n\n\nSALDANHA, R. et al. Subset Models for Multivariate Time Series Forecast. 2024 IEEE 40th International Conference on Data Engineering Workshops. Anais...Netherlands: 2024."
  },
  {
    "objectID": "publications/saldanhaSubsetModelsMultivariate2024a.html#abstract",
    "href": "publications/saldanhaSubsetModelsMultivariate2024a.html#abstract",
    "title": "Subset Models for Multivariate Time Series Forecast",
    "section": "Abstract",
    "text": "Abstract\nMultivariate time series ﬁnd extensive applications in conjunction with machine learning methodologies for scenario forecasting across various domains. Nevertheless, certain domains exhibit inherent complexities and diversities, which detrimentally impact the predictive efﬁcacy of global models. This ongoing study introduces a Subset Modeling Framework designed to acknowledge the inherent diversity within a domain’s multivariate space. Comparative assessments between subset models and global models are conducted in terms of performance, revealing compelling ﬁndings and suggesting the potential for further exploration and reﬁnement of this novel framework."
  },
  {
    "objectID": "publications/saldanhaPesquisaCientificaNa2022.html#reference",
    "href": "publications/saldanhaPesquisaCientificaNa2022.html#reference",
    "title": "Book review: A pesquisa científica na era do Big data: cinco maneiras que mostram como o Big data prejudica a ciência, e como podemos salvá-la",
    "section": "Reference",
    "text": "Reference\n\n\nSALDANHA, R. D. F. A pesquisa científica na era do Big data: cinco maneiras que mostram como o Big data prejudica a ciência, e como podemos salvá-la. Revista Eletrônica de Comunicação, Informação & Inovação em Saúde, v. 16, n. 3, p. 742–745, Sep. 2022."
  },
  {
    "objectID": "publications/saldanhaPesquisaCientificaNa2022.html#abstract",
    "href": "publications/saldanhaPesquisaCientificaNa2022.html#abstract",
    "title": "Book review: A pesquisa científica na era do Big data: cinco maneiras que mostram como o Big data prejudica a ciência, e como podemos salvá-la",
    "section": "Abstract",
    "text": "Abstract\nThe book titled A pesquisa científica na era do Big Data: cinco maneiras que mostram como o Big Data prejudica a ciência, e como podemos salvá-la [The scientific research in the age of Big Data: five ways that show how the Big Data harms the science, and how we can save it], by Sabina Leonelli, published in 2002, by Editora Fiocruz, explores in its chapters the definitions of Big Data and its negative impacts on scientific research. Then, the author reveals a new epistemological approach to Big data and finally she presents a set of proposals for developing a good scientific research. The literature review and updating of definitions as well as the important reflections and questions for a conscious use of Big data in scientific research make the work an important contribution to the researcher’s library of the information and communication about health."
  },
  {
    "objectID": "publications/saldanhaMicrodatasus2023.html#reference",
    "href": "publications/saldanhaMicrodatasus2023.html#reference",
    "title": "Acesso aos dados agregados e microdados do SUS",
    "section": "Reference",
    "text": "Reference\n\n\nSALDANHA, R.; PEDROSO, M.; MAGALHÃES, M. DE A. F. M. Acesso aos dados agregados e microdados do SUS. In: Avaliação de impacto das políticas públicas de saúde: Um guia para o SUS. 1. ed. Brasília: Ministério da Saúde, 2023. p. 418–434."
  },
  {
    "objectID": "publications/saldanhaMicrodatasus2023.html#abstract",
    "href": "publications/saldanhaMicrodatasus2023.html#abstract",
    "title": "Acesso aos dados agregados e microdados do SUS",
    "section": "Abstract",
    "text": "Abstract\nThis chapter presents a historical overview about health data dissemination in Brazil and the creation of the DataSUS. Some of the main Brazilian Health Information Systems characteristics and usage are described. Access tools like TabWin and TabNe are presented, and also an R package called “microdatasus”, for data download and pre-processing."
  },
  {
    "objectID": "publications/saldanhaContributingEliminationCrossBorder2020.html#reference",
    "href": "publications/saldanhaContributingEliminationCrossBorder2020.html#reference",
    "title": "Contributing to Elimination of Cross-Border Malaria Through a Standardized Solution for Case Surveillance, Data Sharing, and Data Interpretation: Development of a Cross-Border Monitoring System",
    "section": "Reference",
    "text": "Reference\n\n\nSALDANHA, R. et al. Contributing to Elimination of Cross-Border Malaria Through a Standardized Solution for Case Surveillance, Data Sharing, and Data Interpretation: Development of a Cross-Border Monitoring System. JMIR Public Health and Surveillance, v. 6, n. 3, p. e15409, Sep. 2020."
  },
  {
    "objectID": "publications/saldanhaContributingEliminationCrossBorder2020.html#abstract",
    "href": "publications/saldanhaContributingEliminationCrossBorder2020.html#abstract",
    "title": "Contributing to Elimination of Cross-Border Malaria Through a Standardized Solution for Case Surveillance, Data Sharing, and Data Interpretation: Development of a Cross-Border Monitoring System",
    "section": "Abstract",
    "text": "Abstract\n\nBackground\nCross-border malaria is a significant obstacle to achieving malaria control and elimination worldwide. Objective: This study aimed to build a cross-border surveillance system that can make comparable and qualified data available to all parties involved in malaria control between French Guiana and Brazil.\n\n\nMethods\nData reconciliation rules based on expert knowledge were defined and applied to the heterogeneous data provided by the existing malaria surveillance systems of both countries. Visualization dashboards were designed to facilitate progressive data exploration, analysis, and interpretation. Dedicated advanced open source and robust software solutions were chosen to facilitate solution sharing and reuse.\n\n\nResults\nA database gathering the harmonized data on cross-border malaria epidemiology is updated monthly with new individual malaria cases from both countries. Online dashboards permit a progressi ve and user-friendly visualization of raw data and epidemiological indicators, in the form of time series, maps, and data quality indexes. The monitoring system was shown to be able to identify changes in time series that are related to control actions, as well as differentiated changes according to space and to population subgroups.\n\n\nConclusions\nThis cross-border monitoring tool could help produce new scientific evidence on cross-border malaria dynamics, implementing cross-border cooperation for malaria control and elimination, and can be quickly adapted to other cross-border contexts."
  },
  {
    "objectID": "publications/pedrosoDataSciencePlatform2023.html#reference",
    "href": "publications/pedrosoDataSciencePlatform2023.html#reference",
    "title": "Data Science Platform Applied to Health in Contribution to the Brazilian Unified Health System",
    "section": "Reference",
    "text": "Reference\n\n\nPEDROSO, M. et al. Data Science Platform Applied to Health in Contribution to the Brazilian Unified Health System. 49ª International Conference on Very Large Data Bases, 2023.\n\n\nThe Data Science Platform Applied to Health (PCDaS) is a research and technological development project that aims to develop and apply novel data analysis methods to public health data. It fills a technological gap between the variety of data sources available in legacy and unstandardized formats and the current needs and possibilities of Data Science applications to consume and explore data for the benefit of the Brazilian Health System. PCDaS provides democratic access to health-related datasets and information by requiring fewer technological abilities from its users while maintaining a continuously updated stack of technologies. As a data ecosystem, our primary goal is to provide secure and remote access to health data, technological tools, and a robust infrastructure provided by our platform to process and analyze a large amount of data that generally demand computational power often unavailable to researchers. The infrastructure consists of multi-region on-premise and cloud servers prepared to deal with the heavy analysis of Big Data from anywhere from multiple users simultaneously. Providing secure and remote access to health databases, whether in their original form or processed, is a daily breakthrough for a public health researcher. Knowing that there is a place where they can access integrated data in a standard format makes the research process much more manageable. To ensure quality, our data engineering and governance teams process these data sources following a gold standard based on cross-tables provided by the Health Ministry (the TabNET system) and decoding the original variables into meaningful names provided by the sources. It is very relevant to emphasize the comprehensive documentation of metadata, attributes, and the ETL (Extract, Transform, Load) process for databases. Every part of these steps is described in detail on the PCDaS website, ensuring the comprehension and reproducibility of the process. These features ensure that PCDaS users can effectively leverage the platform’s resources and capabilities, enabling them to conduct research, perform data analysis, and collaborate within a secure and supportive environment to contribute to the Brazilian Health System."
  },
  {
    "objectID": "publications/mosnierResurgenceRiskMalaria2020a.html#reference",
    "href": "publications/mosnierResurgenceRiskMalaria2020a.html#reference",
    "title": "Resurgence risk for malaria, and the characterization of a recent outbreak in an Amazonian border area between French Guiana and Brazil",
    "section": "Reference",
    "text": "Reference\n\n\nMOSNIER, E. et al. Resurgence risk for malaria, and the characterization of a recent outbreak in an Amazonian border area between French Guiana and Brazil. BMC Infectious Diseases, v. 20, n. 1, p. 373, May 2020."
  },
  {
    "objectID": "publications/mosnierResurgenceRiskMalaria2020a.html#abstract",
    "href": "publications/mosnierResurgenceRiskMalaria2020a.html#abstract",
    "title": "Resurgence risk for malaria, and the characterization of a recent outbreak in an Amazonian border area between French Guiana and Brazil",
    "section": "Abstract",
    "text": "Abstract\n\nBackground\nIn 2017, inhabitants along the border between French Guiana and Brazil were affected by a malaria outbreak primarily due to Plasmodium vivax (Pv). While malaria cases have steadily declined between 2005 and 2016 in this Amazonian region, a resurgence was observed in 2017.\n\n\nMethods\nTwo investigations were performed according to different spatial scales and information details: (1) a local study on the French Guiana border, which enabled a thorough investigation of malaria cases treated at a local village health center and the entomological circumstances in the most affected neighborhood, and (2) a regional and cross-border study, which enabled exploration of the regional spatiotemporal epidemic dynamic. Number and location of malaria cases were estimated using French and Brazilian surveillance systems.\n\n\nResults\nOn the French Guianese side of the border in Saint-Georges de l’Oyapock, the attack rate was 5.5% (n = 4000), reaching 51.4% (n = 175) in one Indigenous neighborhood. Entomological findings suggest a peak of Anopheles darlingi density in August and September. Two female An. darlingi (n = 1104, 0.18%) were found to be Pvpositive during this peak. During the same period, aggregated data from passive surveillance conducted by Brazilian and French Guianese border health centers identified 1566 cases of Pv infection. Temporal distribution during the 2007–2018 period displayed seasonal patterns with a peak in November 2017. Four clusters were identified among epidemic profiles of cross-border area localities. All localities of the first two clusters were Brazilian. The localization of the first cluster suggests an onset of the outbreak in an Indigenous reservation, subsequently expanding to French Indigenous neighborhoods and non-Native communities.\n\n\nConclusions\nThe current findings demonstrate a potential increase in malaria cases in an area with otherwise declining numbers. This is a transborder region where human mobility and remote populations challenge malaria control programs. This investigation illustrates the importance of international border surveillance and collaboration for malaria control, particularly in Indigenous villages and mobile populations."
  },
  {
    "objectID": "publications/guimaraesItTimeTalk2020b.html#reference",
    "href": "publications/guimaraesItTimeTalk2020b.html#reference",
    "title": "Is it time to talk about the end of social distancing? A joinpoint analysis of COVID-19 time series in Brazilian capitals",
    "section": "Reference",
    "text": "Reference\n\n\nGUIMARÃES, R. M. et al. Is it time to talk about the end of social distancing? A joinpoint analysis of COVID-19 time series in Brazilian capitals. Revista da Sociedade Brasileira de Medicina Tropical, v. 53, p. e20200469, Sep. 2020."
  },
  {
    "objectID": "publications/guimaraesItTimeTalk2020b.html#abstract",
    "href": "publications/guimaraesItTimeTalk2020b.html#abstract",
    "title": "Is it time to talk about the end of social distancing? A joinpoint analysis of COVID-19 time series in Brazilian capitals",
    "section": "Abstract",
    "text": "Abstract\n\nIntroduction\nMonitoring coronavirus disease (COVID-19)-related infections and deaths in Brazil is controversial, with increasing pressure to ease social distance measures. However, no evidence of a sustained, widespread fall in cases exists.\n\n\nMethods\nWe used segmented (joinpoint) regression analysis to describe the behavior of COVID-19 infections in Brazilian capital cities.\n\n\nResults\nAll capitals showed an exponential or a near-exponential increase in cases through May. A decline in reported cases was subsequently noted in 20 cities but was only significant for 8 (29.6%) and was followed in two by a renewed increase.\n\n\nConclusions\nCaution is warranted when considering the relaxation of restrictions."
  },
  {
    "objectID": "publications/guimaraesHowOvercomeStagnation2022a.html#reference",
    "href": "publications/guimaraesHowOvercomeStagnation2022a.html#reference",
    "title": "How to overcome the stagnation of the first dose vaccine coverage curve against coronavirus disease 2019 in Brazil?",
    "section": "Reference",
    "text": "Reference\n\n\nGUIMARÃES, R. M. et al. How to overcome the stagnation of the first dose vaccine coverage curve against coronavirus disease 2019 in Brazil? Revista da Sociedade Brasileira de Medicina Tropical, v. 55, p. e0722, Jun. 2022."
  },
  {
    "objectID": "publications/guimaraesHowOvercomeStagnation2022a.html#abstract",
    "href": "publications/guimaraesHowOvercomeStagnation2022a.html#abstract",
    "title": "How to overcome the stagnation of the first dose vaccine coverage curve against coronavirus disease 2019 in Brazil?",
    "section": "Abstract",
    "text": "Abstract\nBackground: A large percentage of the population has not yet started vaccination, for which the increase in coverage is almost null. Methods: We used segmented regression analysis to estimate trends in the first dose coverage curve. Results: There has been a slowdown in the application of the first doses in Brazil since epidemiological week 36 (average percent change [APC] 0.83%, 95% confidence interval [CI] 0.75-0.91%), with a trend close to stagnation. Conclusions: It is important to develop strategies to increase access to vaccination posts. Furthermore, it is recommended to expand vaccination to children, thereby increasing the eligible population."
  },
  {
    "objectID": "publications/fonsecaGeographicAccessibilityCancer2022a.html#reference",
    "href": "publications/fonsecaGeographicAccessibilityCancer2022a.html#reference",
    "title": "Geographic accessibility to cancer treatment in Brazil: A network analysis",
    "section": "Reference",
    "text": "Reference\n\n\nFONSECA, B. DE P. et al. Geographic accessibility to cancer treatment in Brazil: A network analysis. The Lancet Regional Health - Americas, v. 7, p. 100153, Mar. 2022."
  },
  {
    "objectID": "publications/fonsecaGeographicAccessibilityCancer2022a.html#abstract",
    "href": "publications/fonsecaGeographicAccessibilityCancer2022a.html#abstract",
    "title": "Geographic accessibility to cancer treatment in Brazil: A network analysis",
    "section": "Abstract",
    "text": "Abstract\n\nBackground\nGeographic accessibility to healthcare services is a fundamental component in achieving universal health coverage, the central commitment of the Brazilian Unified Health System (SUS). For cancer patients, poor accessibility has been associated with inadequate treatment, worse prognosis, and poorer quality of life.\n\n\nMethods\nWe explored nationwide healthcare data from the SUS health information systems, and mapped the geographic accessibility to cancer treatment in two time-frames: 2009–2010 and 2017–2018. We applied social network analysis (SNA) to estimate the commuting route, flow, and distances travelled by cancer patients to undergo surgical, radiotherapy, and chemotherapy treatment.\n\n\nFindings\nA total of 12,751,728 treatment procedures were analyzed. Overall, more than half of the patients (49·2 to 60·7%) needed to travel beyond their municipality of residence for treatment, a fact that did not change over time. Marked regional differences were observed, as patients living in the northern and midwestern regions of the country had to travel longer distances (weighted average of 296 to 870 km). Cancer care hubs and attraction poles were mostly identified in the southeast and northeast regions, with Barretos being the main hub for all types of treatment throughout time.\n\n\nInterpretation\nImportant regional disparities in the accessibility to cancer treatment in Brazil were revealed, suggesting the need to review the distribution of specialized care in the country. The data presented here contribute to ongoing research on improving access to cancer care and can provide reference to other countries, offering relevant data for oncological and healthcare service evaluation, monitoring, and strategic planning.\n\n\nFunding\nThis work was funded by the Oswaldo Cruz Foundation - Fiocruz (Inova - no. 8451635123 to BPF) and the National Council for Scientific and Technological Development - CNPq (no. 407060/2018–9 to BPF); Coordination for the Improvement of Higher Education Personnel – CAPES (scholarship to PCA, Finance Code 001); and Instituto Nacional de Ciência e Tecnologia de Inovação em Doenças de Populações Negligenciadas (INCT-IDPN)."
  },
  {
    "objectID": "publications/baroniNeonatalMortalityRates2021b.html#reference",
    "href": "publications/baroniNeonatalMortalityRates2021b.html#reference",
    "title": "Neonatal mortality rates in Brazilian municipalities: from 1996 to 2017",
    "section": "Reference",
    "text": "Reference\n\n\nBARONI, L. et al. Neonatal mortality rates in Brazilian municipalities: From 1996 to 2017. BMC Research Notes, v. 14, n. 1, p. 55, Feb. 2021."
  },
  {
    "objectID": "publications/baroniNeonatalMortalityRates2021b.html#abstract",
    "href": "publications/baroniNeonatalMortalityRates2021b.html#abstract",
    "title": "Neonatal mortality rates in Brazilian municipalities: from 1996 to 2017",
    "section": "Abstract",
    "text": "Abstract\nNeonatal mortality is a global public health problem, and the efforts to reduce child mortality is one of the goals of the 2030 Agenda for Sustainable Development, launched in 2015 by the United Nations. The availability of historical neonatal mortality rates (NMR) data in Brazilian municipalities is crucial to evaluate trends at local, regional and national level, identifying gaps and vulnerable territories. Therefore, the objective of this article is to offer an integrated dataset containing monthly data in a historical series from 1996 to 2017 with information on all births, neonatal deaths, and NMR (total, early and late components) enriched with information related to the municipality."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nSubset Models for Multivariate Time Series Forecast\n\n\n\n\n\n\n\n\n\n\n\nMay 13, 2024\n\n\nRaphael Saldanha, Victor Ribeiro, Eduardo Pena, Marcel Pedroso, Reza Akbarinia, Patrick Valduriez, Fabio Porto\n\n\n\n\n\n\n\n\n\n\n\n\nZonal statistics datasets of climate indicators for Brazilian municipalities\n\n\n\n\n\n\n\n\n\n\n\nFeb 12, 2024\n\n\nRaphael Saldanha, Reza Akbarinia, Marcel Pedroso, Victor Ribeiro, Carlos Cardoso, Eduardo H. M. Pena, Patrick Valduriez, Fabio Porto\n\n\n\n\n\n\n\n\n\n\n\n\nAcesso aos dados agregados e microdados do SUS\n\n\n\n\n\n\n\n\n\n\n\nDec 11, 2023\n\n\nRaphael Saldanha, Marcelo Pedroso, Monica Magalhães\n\n\n\n\n\n\n\n\n\n\n\n\nSubset modelling: A domain partitioning strategy for data-efficient machine-learning\n\n\n\n\n\n\n\n\n\n\n\nSep 25, 2023\n\n\nVitor Ribeiro, Eduardo Pena, Raphael Saldanha, Reza Akbarinia, Patrick Valduriez, Falaah Khan, Julia Stoyanovich, Fabio Porto\n\n\n\n\n\n\n\n\n\n\n\n\nData Science Platform Applied to Health in Contribution to the Brazilian Unified Health System\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2023\n\n\nMarcel Pedroso, Rebecca Salles, Raphael Saldanha, Vinicius Kreischer de Almeida, Gabriel Souto, Balthazar Paixão, Victor Ribeiro, Raquel Gritz, Carmen Bonifácio, Matheus Miloski, Carlos Augusto de Sousa, Gizelton Pereira Alencar, Ariane Alves, Nelson Niero Neto, Letícia Sabbadini, Eduardo Ogasawara, Christovam Barcellos, Fabio Porto, Lucas Zinato Carraro, Jefferson Lima\n\n\n\n\n\n\n\n\n\n\n\n\nA comprehensive integrated dataset on Brazilian health facilities: from 2005 to 2021\n\n\n\n\n\n\n\n\n\n\n\nJul 20, 2023\n\n\nRebecca Salles, Victor Dornella, Raquel Gritz, Carlos Cardoso, Sérgio Cruz, Vinicius Kreischer, Carmen Bonifácio, Raphael Saldanha, Gabriel Souto, Matheus Miloski, Leandro Zirondi, Gizelton Alencar, Ariane Alves, Nelson Nieiro Neto, Jefferson Lima, Marcel Pedroso, Ronaldo Alves, Cristiano Boccolini\n\n\n\n\n\n\n\n\n\n\n\n\nDataset on infant mortality rates in Brazil\n\n\n\n\n\n\n\n\n\n\n\nJul 17, 2023\n\n\nGabriel Souto, Matheus Miloski, Carlos Cardoso, Vinicius Kreischer, Balthazar Paixão, Victor Ribeiro, Raquel Gritz, Lucas Carraro, Sérgio Cruz, Carmen Bonifácio, Raphael Saldanha, Leandro Zirondi, Gizelton Alencar, Ariane Alves, Nelson Nieiro Neto, Rebecca Salles, Jefferson Lima, Marcel Pedroso\n\n\n\n\n\n\n\n\n\n\n\n\nBook review: A pesquisa científica na era do Big data: cinco maneiras que mostram como o Big data prejudica a ciência, e como podemos salvá-la\n\n\n\n\n\n\n\n\n\n\n\nSep 30, 2022\n\n\nRaphael Saldanha\n\n\n\n\n\n\n\n\n\n\n\n\nTemporal and spatial distribution trends of polio vaccine coverage in children in Brazil, 2011-2021\n\n\n\n\n\n\n\n\n\n\n\nAug 29, 2022\n\n\nTércia Silva, Ana Carolina de Sá, Elton Prates, Raphael Saldanha, Thales Silva, Antonia Teixeira, Mark Beinner, Suelen Oliveira, Anonio de Sá, Fernanda Matozinhos, Ed Vieira\n\n\n\n\n\n\n\n\n\n\n\n\nComparison of sampling designs from the two editions of the Brazilian National Health Survey, 2013 and 2019\n\n\n\n\n\n\n\n\n\n\n\nJul 13, 2022\n\n\nPaulo Souza Júnior, Celia Szwarcwald, Wanessa Almeida, Giseli Damacena, Marcel Pedroso, Carlos Sousa, Igor Morais, Raphael Saldanha, Jefferson Lima, Sheila Stopa\n\n\n\n\n\n\n\n\n\n\n\n\nHow to overcome the stagnation of the first dose vaccine coverage curve against coronavirus disease 2019 in Brazil?\n\n\n\n\n\n\n\n\n\n\n\nJun 6, 2022\n\n\nRaphael Guimarães, Diego Xavier, Raphael Saldanha, Monica Magalhães\n\n\n\n\n\n\n\n\n\n\n\n\nGeographic accessibility to cancer treatment in Brazil: A network analysis\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2022\n\n\nBruna Fonseca, Priscila Albuquerque, Raphael Saldanha, Fabio Zicker\n\n\n\n\n\n\n\n\n\n\n\n\nCiência de dados e big data: o que isso significa para estudos populacionais e da saúde?\n\n\n\n\n\n\n\n\n\n\n\nNov 26, 2021\n\n\nRaphael Saldanha, Christovam Barcellos, Marcel Pedroso\n\n\n\n\n\n\n\n\n\n\n\n\nEstimation of COVID-19 Under-Reporting in the Brazilian States Through SARI\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2021\n\n\nBalthazar Paixão, Lais Baroni, Marcel Pedroso, Rebecca Sales, Luciana Escobar, Carlos Sousa, Raphael Saldanha, Jorge Soares, Rafaelli Coutinho, Fabio Porto, Eduardo Ogasawara\n\n\n\n\n\n\n\n\n\n\n\n\nNeonatal mortality rates in Brazilian municipalities: from 1996 to 2017\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2021\n\n\nLais Baroni, Rebecca Sales, Samella Sales, Marcel Pedroso, Jefferson Lima, Igor Morais, Lucas Carraro, Raphael Saldanha, Carlos Sousa, Carlos Cardoso, Balthazar Paixão, Sérgio Cruz, Eduardo Ogasawara, Patricia Boccolini, Cristiano Boccolini\n\n\n\n\n\n\n\n\n\n\n\n\nIncreasing impact of COVID-19 on young adults: evidence from hospitalisations in Brazil\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2021\n\n\nRaphael Guimarães, Daniel Vilela, Diego Xavier, Raphael Saldanha, Christovam Barcellos, Carlos Freitas, Margareth Portela\n\n\n\n\n\n\n\n\n\n\n\n\nAdesão às medidas de restrição de contato físico e disseminação da COVID-19 no Brasil\n\n\n\n\n\n\n\n\n\n\n\nNov 6, 2020\n\n\nCelia Szwarcwald, Paulo Souza Junior, Deborah Malta, Marilisa Barros, Raphael Saldanha, Giseli Damascena, Luiz Azevedo, Margareth Lima, Dalia Romero, Ísis Machado, Crizian Gomes, André Werneck, Danilo Silva, Renata Gracie, Maria Pina\n\n\n\n\n\n\n\n\n\n\n\n\nIs it time to talk about the end of social distancing? A joinpoint analysis of COVID-19 time series in Brazilian capitals\n\n\n\n\n\n\n\n\n\n\n\nSep 21, 2020\n\n\nRaphael Guimarães, Monica Magalhães, Diego Xavier, Raphael Saldanha, Rafael Catão\n\n\n\n\n\n\n\n\n\n\n\n\nResurgence risk for malaria, and the characterization of a recent outbreak in an Amazonian border area between French Guiana and Brazil\n\n\n\n\n\n\n\n\n\n\n\nMay 26, 2020\n\n\nEmilie Mosnier, Isabelle Dusfour, Guillaume Lacour, Raphael Saldanha, Amandine Guidez, Margaret Gomes, Alice Sanna, Yanouk Epelboin, Johana Restrepo, Damien Davy, Magalie Demar, Félix Djossou, Maylis Douine, Vanessa Ardillon, Mathieu Nacher, Lise Musset, Emmanuel Roux\n\n\n\n\n\n\n\n\n\n\n\n\nFatores associados ao letramento funcional em saúde de mulheres atendidas pela Estratégia de Saúde da Família\n\n\n\n\n\n\n\n\n\n\n\nApr 9, 2020\n\n\nAngélica Campos, Felipe Neves, Raphael Saldanha, Kristiane Duque, Maximiliano Guerra, Isabel Leite, Maria Teixeira\n\n\n\n\n\n\n\n\n\n\n\n\nMicrodatasus: pacote para download e pré-processamento de microdados do Departamento de Informática do SUS (DATASUS)\n\n\n\n\n\n\n\n\n\n\n\nSep 16, 2019\n\n\nRaphael Saldanha, Ronaldo Bastos, Christovam Barcellos\n\n\n\n\n\n\n\n\n\n\n\n\nEstudo de análise de rede do fluxo de pacientes de câncer de mama no Brasil entre 2014 e 2016\n\n\n\n\n\n\n\n\n\n\n\nJul 22, 2019\n\n\nRaphael Saldanha, Diego Xavier, Keila Carnavalli, Kátia Lerner, Christovam Barcellos\n\n\n\n\n\n\n\n\n\n\n\n\nCrescimento do emprego industrial local no Brasil: o grau de especialização por intensidade tecnológica importa?\n\n\n\n\n\n\n\n\n\n\n\nJun 13, 2019\n\n\nEduardo Gonçalves, Raphael Saldanha, Eduardo Almeida, André Silva\n\n\n\n\n\n\n\n\n\n\n\n\nContributing to Elimination of Cross-Border Malaria Through a Standardized Solution for Case Surveillance, Data Sharing, and Data Interpretation: Development of a Cross-Border Monitoring System\n\n\n\n\n\n\n\n\n\n\n\nJun 13, 2019\n\n\nRaphael Saldanha, Émilie Mosnier, Christovam Barcellos, Aurel Carbunar, Christophe Charron, Jean-Christophe Desconnets, Basma Guarmit, Margarete Gomes, Théophile Mandon, Anapaula Mendes, Paulo Peiter, Lise Musset, Alice Sanna, Benoît Van Gastel, Emmanuel Roux\n\n\n\n\n\n\n\n\n\n\n\n\nAs Regiões de Saúde no Brasil segundo internações: método para apoio na regionalização de saúde\n\n\n\n\n\n\n\n\n\n\n\nJun 13, 2019\n\n\nDiego Xavier, Ricardo Oliveira, Christovam Barcellos, Raphael Saldanha, Walter Ramalho, Josué Laguardia, Francisco Viacava\n\n\n\n\n\n\n\n\n\n\n\n\nWhat have we learned from Mariana? The importance of names, places and affections\n\n\n\n\n\n\n\n\n\n\n\nMay 16, 2019\n\n\nNadja Araújo, Keila Carnavalli, Nathalia Barbosa, Patricia Barcelos, Raphael Saldanha, Teresa Neves, Vinicius Klein, Maria Guimarães\n\n\n\n\n\n\n\n\n\n\n\n\nBurden of disease in Brazil, 1990–2016: a systematic subnational analysis for the Global Burden of Disease Study 2016\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2018\n\n\nFatima Marinho et al.\n\n\n\n\n\n\n\n\n\n\n\n\nProposta de um observatório epidemiológico do Sistema Único de Saúde\n\n\n\n\n\n\n\n\n\n\n\nJan 23, 2017\n\n\nRaphael Saldanha, Ronaldo Bastos, Maria Bustamante-Teixeira, Isabel Leite, Estela Campos\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n Back to top"
  },
  {
    "objectID": "projects/redes-de-cuidado.html",
    "href": "projects/redes-de-cuidado.html",
    "title": "Redes de Cuidado",
    "section": "",
    "text": "Commuting networks of patients that received cancer treatment within SUS, from article http://dx.doi.org/10.1016/j.lana.2021.100153.\n\n\nThis project was a partnership with PCDaS and CDTS. The project studied the flow of cancer patients receiving treatment in Brazil between municipalities.\nI was in charge to handle the large datasets about hospitalizations and treatments and perform some network analysis.\nThis work was supported by Inova Fiocruz and resulted in a published paper: http://dx.doi.org/10.1016/j.lana.2021.100153.\n\n\n\n Back to top"
  },
  {
    "objectID": "projects/pmm.html",
    "href": "projects/pmm.html",
    "title": "PMM",
    "section": "",
    "text": "PMM main webpage.\n\n\n“Projeto Mais Médicos” was the first project that I got involved with at Fiocruz, through the PCDaS initiative. I was in charge of developing a data dashboard and to coordinate a team with other research assistants.\nThe project aimed to present data about a federal project called “Mais Médicos”, where a public health policy encouraged MDs to work in less developed regions in Brazil and also contracted foreign MDs from Cuba and other countries to work on those regions.\nThe project has been discontinued.\n\n\n\n Back to top"
  },
  {
    "objectID": "projects/onde-estao-nossas-doencas.html",
    "href": "projects/onde-estao-nossas-doencas.html",
    "title": "Onde estão nossas doenças?",
    "section": "",
    "text": "Exhibition visitors using the interactive map.\n\n\nThis was a very special project, where we were invited to develop a museum exhibition at Fiocruz. On this set, we present to the visitors the relationship between health and space by using interactive maps in a wide display and also with printed and tactile special maps for visually impaired visitors.\nThe exhibition is permanent and located at the Fiocruz Maré Campus.\nThe interactive map can also be accessed here: https://rfsaldanha.github.io/app_cavalarica/\n\n\n\nTactile map for visually impaired visitors.\n\n\n\n\n\nFiocruz president, Dra. Nisia Trindade, contemplating the exhibition.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "projects/mauro.html",
    "href": "projects/mauro.html",
    "title": "Mauro",
    "section": "",
    "text": "Mauro main webpage.\n\n\n“Mauro” was a research project that I initiated during my Ph.D. at Fiocruz, when I was taking some courses about technological information on health.\nThe objectives of the project were (1) to download and organize bibliometric information available at “Scielo” from the Public Health, and Brazil collections. (2) Provide a simple and intuitive way for researchers to access its data through filters and visualize information.\nThe ETL process and dashboard were developed with R and Shiny. The project is now deactivated.\n\n\n\n Back to top"
  },
  {
    "objectID": "projects/cross-border-malaria.html",
    "href": "projects/cross-border-malaria.html",
    "title": "Cross-border malaria",
    "section": "",
    "text": "Cross-border malaria main webpage.\n\n\nThis was the second project that I got involved at Fiocruz with the Climate and Health Observatory. It was a project in partnership with the French IRD (Institute de Recherche pour le Développement).\nI was in charge to provide knowledge about the malaria databases in Brazil (SIVEP-Malária) and developing a data dashboard showing cross-border malaria transmission on the French Guiana and Brazil (Amapá) frontier. This was an especially difficult task considering that the dashboard needed to be present in English, French, and Portuguese. The dashboard was developed with R and Shiny technology.\nThis project involved a whole month trip to Montpellier, France to engage with other team members and also a trip to Cayenne, the French Guiana capital to learn firsthand how data was collected and handled.\nThis project resulted in two published papers: http://dx.doi.org/10.1186/s12879-020-05086-4 and http://dx.doi.org/10.2196/15409.\nThis work was supported by a grant from Bill & Mellinda Gates Foundation.\n\n\n\nIRD\n\n\n\n\n\nMaison de la Télédétection\n\n\n\n\n\nProject first sketches on board (Montpellier, France).\n\n\n\n\n\nProject presentation at Cayenne by Dr. Emmanuel Roux, coordinator.\n\n\n\n\n\nSome leisure time at Cayenne.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "projects/arboalvo.html",
    "href": "projects/arboalvo.html",
    "title": "ArboAlvo",
    "section": "",
    "text": "ArboAlvo dashboard.\n\n\nConducted at the Instituto Oswaldo Cruz (IOC), the project ArboAlvo’s objective is to stratify arboviruses risk into national and intra-municipal levels, considering epidemiolocal, entomological, and socio-demographic dimensions.\nI participated in this project by creating, reviewing, and optimizing code in the project data analysis, and developing data dashboards.\n\n\n\nArboAlvo dashboard running on Natal local server.\n\n\n\n\n\nWith the Natal IT team, August 8, 2022.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Saúde RS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHarmonize\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonitoraCovid-19\n\n\nCovid-19 monitoring.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOuvSUS\n\n\nSUS Ombudsman.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPNS\n\n\nPesquisa Nacional de Saúde.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCross-border malaria\n\n\nIn partnership with IRD.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRedes de Cuidado\n\n\nCancer patients displacement and network analysis.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArboAlvo\n\n\nStratifying arboviruses risk.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOnde estão nossas doenças?\n\n\nMuseum exhibition.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPMM\n\n\nProjeto Mais Médicos.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nÁgua & Saúde\n\n\nIn partneship with ANA.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMauro\n\n\na Metadata Automatic Retrieval system for Updated References and cited Objects from Scielo.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCidade & Saúde\n\n\nAccessible health information about Brazilian municipalities.\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "pres/bsc2024.html#main-fields-of-work-and-projects",
    "href": "pres/bsc2024.html#main-fields-of-work-and-projects",
    "title": "Research Experience & Plans",
    "section": "Main fields of work and projects",
    "text": "Main fields of work and projects\n\nData handling tools: author of ten R packages to handle health and climate datasets and indicators.\n\n{microdatasus}, {brpop}, {tidyrates}, {zendown}, {zenstats}, {nseq}, {rspell}, {brclimr}, {zonalclim}, {opendenguedata}\n\nVisualization: malaria and COVID-19 interactive visualization dashboards (IRD-FR and Fiocruz-BR)\nAnalysis and modeling of climate-sensitive diseases (Fiocruz-BR, LNCC-BR, BSC, Inria-FR, IRD-FR)"
  },
  {
    "objectID": "pres/bsc2024.html#health-and-climate-indicators",
    "href": "pres/bsc2024.html#health-and-climate-indicators",
    "title": "Research Experience & Plans",
    "section": "Health and Climate indicators",
    "text": "Health and Climate indicators\n\n\n\nStandardized methodology to harmonize climate indicators with health data\nSame time and spatial units (administrative regions)\nMethod of Zonal Statistics\nPublication at Environmental Data Science journal (2024)"
  },
  {
    "objectID": "pres/bsc2024.html#health-and-climate-indicators-1",
    "href": "pres/bsc2024.html#health-and-climate-indicators-1",
    "title": "Research Experience & Plans",
    "section": "Health and Climate indicators",
    "text": "Health and Climate indicators\n\nStreamline methods, expansion to other countries and continents\nProvide regularly updated datasets of health and climate indicators at standardized administrative geographic boundaries (level 2, level 3)\nUsage of population estimates as weighting factor\nBuild synthetic indicators for water cycle and droughts, warm and cold spells, and extreme events"
  },
  {
    "objectID": "pres/bsc2024.html#climate-sensitive-diseases-modeling",
    "href": "pres/bsc2024.html#climate-sensitive-diseases-modeling",
    "title": "Research Experience & Plans",
    "section": "Climate-Sensitive Diseases modeling",
    "text": "Climate-Sensitive Diseases modeling\n\nDengue inflicts an important health burden in Brazil\n\n1 million new cases reported just in 2024, 214 deaths, official emergency status declaration\n\nProject for dengue incidence forecast on Brazil with machine learning methods\nNovel ML subsets approach, with cluster based models"
  },
  {
    "objectID": "pres/bsc2024.html#early-warning-systems-for-csd",
    "href": "pres/bsc2024.html#early-warning-systems-for-csd",
    "title": "Research Experience & Plans",
    "section": "Early Warning Systems for CSD",
    "text": "Early Warning Systems for CSD\n\nFramework proposal to prevent and early detect CSD outbreaks adaptive to regions’ contexts and priorities\nAligned with WHO guidelines\nClose work with health managers and civil society representatives for project usage and tools appropriation\nTraditional and machine learning multivariate models to forecast incidence for short and medium-time horizons (ARIMA, random forest, XGBoost, LSTM…)\nContinuous model monitoring and update (MLOps)"
  },
  {
    "objectID": "pres/bsc2024.html#integration-with-the-global-health-resilience-group-and-projects",
    "href": "pres/bsc2024.html#integration-with-the-global-health-resilience-group-and-projects",
    "title": "Research Experience & Plans",
    "section": "Integration with the Global Health Resilience group and projects",
    "text": "Integration with the Global Health Resilience group and projects\n\nToolkits development, maintenance and training. DAGs (directed acyclic graph) to integrate and monitor tools execution and results lifecycle\nClimate and health data methods and products, basis of different projects\nML methods and models for EWS\nProvide interoperable data, methods and models to different projects, as HARMONIZE, IDAlert, IDExtremes, and E4Warning"
  },
  {
    "objectID": "posts/std_br_covid_rates.html",
    "href": "posts/std_br_covid_rates.html",
    "title": "Age-adjusted COVID-19 mortality rates for Brazilian municipalities",
    "section": "",
    "text": "On this post, we will compute crude and age-adjusted COVID-19 mortality rates for Brazilian municipalities, from 2020 to 2022 per epidemiological weeks.\nWe will use the {tidyverse} and other packages that I will call later.\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/std_br_covid_rates.html#covid-19-mortality-data",
    "href": "posts/std_br_covid_rates.html#covid-19-mortality-data",
    "title": "Age-adjusted COVID-19 mortality rates for Brazilian municipalities",
    "section": "COVID-19 mortality data",
    "text": "COVID-19 mortality data\nThe Brazilian official source of mortality data is a health information system called “Sistema de Informações de Mortalidade – SIM”. This system receives mortality data from the municipalities and states and consolidates it.\nThe SIM dataset is published yearly, with a year lag. This time is needed to consolidate all the data and perform checks. The SIM datasets of 2019, 2020, and 2021 are officially published and the 2022 dataset is published as “preliminary”, which means that modifications and updates are possible.\nThe datasets are publicly available on the OpenDataSUS website as CSV files. I downloaded the datasets from 2019 to 2023, as the 2023 may have some death records that occurred in 2022.\nThe code chunk below has the CSV column types specification to read these files with the {readr} package. The chunk is folded to save screen space ;-)\n\n\nCode\ndate_format &lt;- \"%d%m%Y\"\n\ncols_spec &lt;- cols(\n  ORIGEM = col_double(),\n  TIPOBITO = col_double(),\n  DTOBITO = col_date(format = date_format),\n  HORAOBITO = col_character(),\n  NATURAL = col_character(),\n  CODMUNNATU = col_double(),\n  DTNASC = col_date(format = date_format),\n  IDADE = col_double(),\n  SEXO = col_double(),\n  RACACOR = col_double(),\n  ESTCIV = col_double(),\n  ESC = col_double(),\n  ESC2010 = col_double(),\n  SERIESCFAL = col_double(),\n  OCUP = col_character(),\n  CODMUNRES = col_double(),\n  LOCOCOR = col_double(),\n  CODESTAB = col_character(),\n  ESTABDESCR = col_character(),\n  CODMUNOCOR = col_double(),\n  IDADEMAE = col_double(),\n  ESCMAE = col_double(),\n  ESCMAE2010 = col_double(),\n  SERIESCMAE = col_double(),\n  OCUPMAE = col_double(),\n  QTDFILVIVO = col_character(),\n  QTDFILMORT = col_character(),\n  GRAVIDEZ = col_double(),\n  SEMAGESTAC = col_double(),\n  GESTACAO = col_double(),\n  PARTO = col_double(),\n  OBITOPARTO = col_double(),\n  PESO = col_character(),\n  TPMORTEOCO = col_double(),\n  OBITOGRAV = col_double(),\n  OBITOPUERP = col_double(),\n  ASSISTMED = col_double(),\n  EXAME = col_character(),\n  CIRURGIA = col_character(),\n  NECROPSIA = col_double(),\n  LINHAA = col_character(),\n  LINHAB = col_character(),\n  LINHAC = col_character(),\n  LINHAD = col_character(),\n  LINHAII = col_character(),\n  CAUSABAS = col_character(),\n  CB_PRE = col_character(),\n  COMUNSVOIM = col_character(),\n  DTATESTADO = col_date(format = date_format),\n  CIRCOBITO = col_double(),\n  ACIDTRAB = col_double(),\n  FONTE = col_double(),\n  NUMEROLOTE = col_double(),\n  TPPOS = col_character(),\n  DTINVESTIG = col_date(format = date_format),\n  CAUSABAS_O = col_character(),\n  DTCADASTRO = col_date(format = date_format),\n  ATESTANTE = col_double(),\n  STCODIFICA = col_character(),\n  CODIFICADO = col_character(),\n  VERSAOSIST = col_number(),\n  VERSAOSCB = col_number(),\n  FONTEINV = col_double(),\n  DTRECEBIM = col_date(format = date_format),\n  ATESTADO = col_character(),\n  DTRECORIGA = col_date(format = date_format),\n  CAUSAMAT = col_character(),\n  ESCMAEAGR1 = col_character(),\n  ESCFALAGR1 = col_character(),\n  STDOEPIDEM = col_double(),\n  STDONOVA = col_double(),\n  DIFDATA = col_character(),\n  NUDIASOBCO = col_double(),\n  NUDIASOBIN = col_character(),\n  DTCADINV = col_date(format = date_format),\n  TPOBITOCOR = col_double(),\n  DTCONINV = col_date(format = date_format),\n  FONTES = col_character(),\n  TPRESGINFO = col_double(),\n  TPNIVELINV = col_character(),\n  NUDIASINF = col_character(),\n  DTCADINF = col_date(format = date_format),\n  MORTEPARTO = col_double(),\n  DTCONCASO = col_date(format = date_format),\n  FONTESINF = col_character(),\n  ALTCAUSA = col_double(),\n  CONTADOR = col_double()\n)\n\n\nThe code chunk bellow reads the CSV file with the column types specification from above.\n\nsim19 &lt;- readr::read_csv2(file = \"../../covidbr/Mortalidade_Geral_2019.csv\", col_types = cols_spec)\nsim20 &lt;- readr::read_csv2(file = \"../../covidbr/Mortalidade_Geral_2020.csv\", col_types = cols_spec)\nsim21 &lt;- readr::read_csv2(file = \"../../covidbr/Mortalidade_Geral_2021.csv\", col_types = cols_spec)\nsim22 &lt;- readr::read_csv2(file = \"../../covidbr/DO22OPEN.csv\", col_types = cols_spec)\nsim23 &lt;- readr::read_csv2(file = \"../../covidbr/DO23OPEN.csv\", col_types = cols_spec)\n\nAfter reading the files, let’s create one single data frame with the variables we will use.\n\n1covid &lt;- bind_rows(sim19, sim20, sim21, sim22, sim23) %&gt;%\n2  filter(CAUSABAS == \"B342\") %&gt;%\n3  filter(DTOBITO &gt;= as.Date(\"2020-01-01\") & DTOBITO &lt;= as.Date(\"2022-12-31\")) %&gt;%\n4  select(DTOBITO, DTNASC, CODMUNRES) %&gt;%\n5  na.omit()\n\n6rm(sim19, sim20, sim21, sim22, sim23, cols_spec, date_format)\n\n\n1\n\nBind the sim objects into a single data frame.\n\n2\n\nFilter the records where the basic cause of death is COVID-19 (ICD-10 code B342).\n\n3\n\nFilter the records keeping only the deaths that occurred between 2019 and 2022.\n\n4\n\nSelect the date of death (DTOBITO) and date of birth (DTNASC) to compute the age and the municipality code of residence (CODMUNRES).\n\n5\n\nOmit rows with missing data.\n\n6\n\nRemove the sim* objects as we will no longer need them.\n\n\n\n\nOur dataset has 702284 records.\n\nhead(covid, 10)\n\n# A tibble: 10 × 3\n   DTOBITO    DTNASC     CODMUNRES\n   &lt;date&gt;     &lt;date&gt;         &lt;dbl&gt;\n 1 2020-05-21 1942-08-10    120010\n 2 2020-05-27 1943-02-19    120010\n 3 2020-05-27 1975-01-14    120025\n 4 2020-05-05 1972-06-10    120040\n 5 2020-05-25 1939-10-04    120040\n 6 2020-05-25 1965-02-14    120040\n 7 2020-05-25 1946-06-17    120040\n 8 2020-05-30 1937-05-17    120080\n 9 2020-05-30 1959-06-08    120013\n10 2020-05-13 1982-08-22    120040\n\n\n\nAge groups\nNow we need to label the records into age groups.\n\ncovid &lt;- covid %&gt;%\n  mutate(\n1    age = year(as.period(interval(start = DTNASC, end = DTOBITO))),\n2    age_group = case_when(\n      age &lt;= 4 ~ \"From 0 to 4 years\",\n      age &gt;= 5 & age &lt;= 9 ~ \"From 5 to 9 years\",\n      age &gt;= 10 & age &lt;= 14 ~ \"From 10 to 14 years\",\n      age &gt;= 15 & age &lt;= 19 ~ \"From 15 to 19 years\",\n      age &gt;= 20 & age &lt;= 24 ~ \"From 20 to 24 years\",\n      age &gt;= 25 & age &lt;= 29 ~ \"From 25 to 29 years\",\n      age &gt;= 30 & age &lt;= 34 ~ \"From 30 to 34 years\",\n      age &gt;= 35 & age &lt;= 39 ~ \"From 35 to 39 years\",\n      age &gt;= 40 & age &lt;= 44 ~ \"From 40 to 44 years\",\n      age &gt;= 45 & age &lt;= 49 ~ \"From 45 to 49 years\",\n      age &gt;= 50 & age &lt;= 54 ~ \"From 50 to 54 years\",\n      age &gt;= 55 & age &lt;= 59 ~ \"From 55 to 59 years\",\n      age &gt;= 60 & age &lt;= 64 ~ \"From 60 to 64 years\",\n      age &gt;= 65 & age &lt;= 69 ~ \"From 65 to 69 years\",\n      age &gt;= 70 & age &lt;= 74 ~ \"From 70 to 74 years\",\n      age &gt;= 75 & age &lt;= 79 ~ \"From 75 to 79 years\",\n      age &gt;= 80 ~ \"From 80 years or more\"\n    ),\n3    age_group = fct_relevel(\n      age_group,\n      \"From 0 to 4 years\", \"From 5 to 9 years\",\n      \"From 10 to 14 years\", \"From 15 to 19 years\",\n      \"From 20 to 24 years\", \"From 25 to 29 years\",\n      \"From 30 to 34 years\", \"From 35 to 39 years\",\n      \"From 40 to 44 years\", \"From 45 to 49 years\",\n      \"From 50 to 54 years\", \"From 55 to 59 years\",\n      \"From 60 to 64 years\", \"From 65 to 69 years\",\n      \"From 70 to 74 years\", \"From 75 to 79 years\",\n      \"From 80 years or more\"\n    )\n  ) %&gt;%\n4  select(date = DTOBITO, code_muni = CODMUNRES, age_group)\n\n\n1\n\nCompute the age based on the date of birth and date of death. For this, I used some functions from the {lubridate} package.\n\n2\n\nLabel the age groups.\n\n3\n\nConvert age_group to an ordered factor variable.\n\n4\n\nSelect and rename the desired variables.\n\n\n\n\n\nhead(covid, 10)\n\n# A tibble: 10 × 3\n   date       code_muni age_group            \n   &lt;date&gt;         &lt;dbl&gt; &lt;fct&gt;                \n 1 2020-05-21    120010 From 75 to 79 years  \n 2 2020-05-27    120010 From 75 to 79 years  \n 3 2020-05-27    120025 From 45 to 49 years  \n 4 2020-05-05    120040 From 45 to 49 years  \n 5 2020-05-25    120040 From 80 years or more\n 6 2020-05-25    120040 From 55 to 59 years  \n 7 2020-05-25    120040 From 70 to 74 years  \n 8 2020-05-30    120080 From 80 years or more\n 9 2020-05-30    120013 From 60 to 64 years  \n10 2020-05-13    120040 From 35 to 39 years  \n\n\nLet’s take a look at the epidemiological curves per date and age group.\n\ncovid %&gt;%\n  group_by(date, age_group) %&gt;%\n  summarise(events = n()) %&gt;%\n  ungroup() %&gt;%\n  ggplot(aes(x = date, y = events)) +\n  geom_line() +\n  facet_wrap(~age_group) +\n  theme_bw()\n\n\n\n\n\n\n\n\nIt is pretty clear that COVID-19 mortality incidence is related to age. Thus, to compare mortality rates of different regions, we need to compute age-adjusted rates.\n\n\nAggregate data\nLet’s aggregate our dataset by municipality of residence, epidemiological and epidemiological week.\n\ncovid_agg &lt;- covid %&gt;%\n1  group_by(code_muni, date, age_group) %&gt;%\n  summarise(events = n()) %&gt;%\n  ungroup() %&gt;%\n2  group_by(code_muni) %&gt;%\n  complete(\n    date = seq.Date(as.Date(\"2020-01-01\"), as.Date(\"2022-12-31\"), by = \"day\"),\n    age_group = unique(covid$age_group),\n    fill = list(events = 0)\n  ) %&gt;%\n  ungroup() %&gt;%\n3  mutate(\n    year = epiyear(date),\n    week = epiweek(date)\n  ) %&gt;%\n4  group_by(code_muni, year, week, age_group) %&gt;%\n  summarise(events = sum(events)) %&gt;%\n  ungroup()\n\n\n1\n\nFirst, we aggregate the COVID-19 per municipality of residence, age group and date. But this aggregation have some gaps on date and age groups, as there are no deaths at some specific dates and age groups.\n\n2\n\nWe can complete these gaps using the tidyr::complete, supplying the dates interval and age groups. We will fill the events variables with zero values.\n\n3\n\nWith the complete dataset, we compute the epidemiological year and week.\n\n4\n\nAnd aggregate by municipality code, year, week and age_group."
  },
  {
    "objectID": "posts/std_br_covid_rates.html#population-data",
    "href": "posts/std_br_covid_rates.html#population-data",
    "title": "Age-adjusted COVID-19 mortality rates for Brazilian municipalities",
    "section": "Population data",
    "text": "Population data\nWe need to add the population data to compute the rates. Let’s prepare our population data using the brpop package.\n\nmun_pop &lt;- brpop::mun_pop() %&gt;%\n  filter(year %in% 2020:2022) %&gt;%\n  rename(population = pop) %&gt;%\n  filter(age_group != \"Total\") %&gt;%\n  rename(code_muni = mun)\n\nAs there is no population estimates for 2022 yet, we will repeat the population from 2021.\n\nmun_pop_2022 &lt;- mun_pop %&gt;%\n  filter(year == 2021) %&gt;%\n  mutate(year = 2022)\n\nmun_pop &lt;- bind_rows(mun_pop, mun_pop_2022)\nrm(mun_pop_2022)\n\nLet’s join this population data with the COVID-19 data.\n\ncovid_agg &lt;- right_join(covid_agg, mun_pop, by = c(\"code_muni\", \"year\", \"age_group\")) %&gt;% \n  pivot_longer(cols = c(\"events\", \"population\"))\n\nhead(covid_agg, 10)\n\n# A tibble: 10 × 6\n   code_muni  year  week age_group           name       value\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;      &lt;int&gt;\n 1    110000  2020     1 From 0 to 4 years   events         0\n 2    110000  2020     1 From 0 to 4 years   population     0\n 3    110000  2020     1 From 5 to 9 years   events         0\n 4    110000  2020     1 From 5 to 9 years   population     0\n 5    110000  2020     1 From 10 to 14 years events         0\n 6    110000  2020     1 From 10 to 14 years population     0\n 7    110000  2020     1 From 15 to 19 years events         0\n 8    110000  2020     1 From 15 to 19 years population     0\n 9    110000  2020     1 From 20 to 24 years events         0\n10    110000  2020     1 From 20 to 24 years population     0"
  },
  {
    "objectID": "posts/std_br_covid_rates.html#reference-population",
    "href": "posts/std_br_covid_rates.html#reference-population",
    "title": "Age-adjusted COVID-19 mortality rates for Brazilian municipalities",
    "section": "Reference population",
    "text": "Reference population\nWe will use as reference population, the year of 2020.\n\npop_ref &lt;- mun_pop %&gt;%\n  filter(year == 2020) %&gt;%\n  group_by(age_group) %&gt;%\n  summarise(population = sum(population)) %&gt;%\n  ungroup()\n\nprint(pop_ref)\n\n# A tibble: 17 × 2\n   age_group             population\n   &lt;chr&gt;                      &lt;int&gt;\n 1 From 0 to 4 years       14730300\n 2 From 10 to 14 years     14805480\n 3 From 15 to 19 years     15790890\n 4 From 20 to 24 years     17233273\n 5 From 25 to 29 years     16985859\n 6 From 30 to 34 years     17205414\n 7 From 35 to 39 years     17026565\n 8 From 40 to 44 years     15602995\n 9 From 45 to 49 years     13652508\n10 From 5 to 9 years       14650284\n11 From 50 to 54 years     12617802\n12 From 55 to 59 years     11257270\n13 From 60 to 64 years      9383724\n14 From 65 to 69 years      7349241\n15 From 70 to 74 years      5408657\n16 From 75 to 79 years      3614384\n17 From 80 years or more    4441046"
  },
  {
    "objectID": "posts/std_br_covid_rates.html#crude-and-adjusted-rates",
    "href": "posts/std_br_covid_rates.html#crude-and-adjusted-rates",
    "title": "Age-adjusted COVID-19 mortality rates for Brazilian municipalities",
    "section": "Crude and adjusted rates",
    "text": "Crude and adjusted rates\nTo compute the crude and age-adjusted rates, we will use the {tidyrates} package.\n\nrates &lt;- tidyrates::rate_adj_direct(\n  .data = covid_agg, \n  .std = pop_ref, \n  .keys = c(\"code_muni\", \"year\", \"week\"), \n  .progress = FALSE\n) %&gt;%\n  mutate(\n    crude.rate = crude.rate * 100000,\n    adj.rate = adj.rate * 100000,\n    lci = lci * 100000,\n    uci = uci * 100000,\n  )\n\nThat’s it! Let’s take a look at the rates for Rio de Janeiro, RJ on 2022.\n\nrates %&gt;%\n  filter(code_muni == 330455) %&gt;%\n  filter(year == 2022)\n\n# A tibble: 52 × 7\n   code_muni  year  week crude.rate adj.rate   lci   uci\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1    330455  2022     1      0.251    0.175 0.101 0.297\n 2    330455  2022     2      1.17     0.839 0.662 1.06 \n 3    330455  2022     3      3.81     2.70  2.37  3.06 \n 4    330455  2022     4      5.58     3.96  3.57  4.40 \n 5    330455  2022     5      4.44     3.11  2.76  3.49 \n 6    330455  2022     6      2.83     2.00  1.72  2.32 \n 7    330455  2022     7      1.98     1.43  1.20  1.71 \n 8    330455  2022     8      0.989    0.682 0.528 0.880\n 9    330455  2022     9      0.398    0.283 0.185 0.428\n10    330455  2022    10      0.310    0.217 0.133 0.346\n# ℹ 42 more rows\n\n\nLet’s save the rates data frame as a parquet and CSV file.\n\narrow::write_parquet(x = rates, sink = \"../../covidbr/covid19_adj_rates.parquet\")\nwrite_csv2(x = rates, file = \"../../covidbr/covid19_adj_rates.csv\")\n\nThese files are available for download at Zenodo:"
  },
  {
    "objectID": "posts/std_br_covid_rates.html#mortality-rates-graph-for-capitals",
    "href": "posts/std_br_covid_rates.html#mortality-rates-graph-for-capitals",
    "title": "Age-adjusted COVID-19 mortality rates for Brazilian municipalities",
    "section": "Mortality rates graph for capitals",
    "text": "Mortality rates graph for capitals\nThe graph bellow present the crude and adjusted rates at the Brazilian capitals.\n\ncapitals &lt;- geobr::read_capitals(as_sf = TRUE, showProgress = FALSE) %&gt;%\n  mutate(\n    code_muni = ifelse(code_muni == 2803203, 2800308, code_muni),\n    name_muni = ifelse(code_muni == 2800308, \"Aracaju\", name_muni)\n  ) %&gt;%\n  mutate(code_muni = as.numeric(substr(code_muni, 0, 6))) %&gt;%\n  select(-year) %&gt;%\n  sf::st_drop_geometry()\n\nrates_for_plot &lt;- rates %&gt;%\n  right_join(capitals, by = \"code_muni\") %&gt;%\n  mutate(week = paste0(year, \"-\", str_pad(week, 2, pad = \"0\"))) %&gt;%\n  select(week, name_muni, crude.rate, adj.rate) %&gt;%\n  pivot_longer(cols = c(\"crude.rate\", \"adj.rate\")) \n\nggplot() +\n  geom_line(data = rates_for_plot, aes(x = week, y = value, group = name, color = name)) +\n  facet_wrap(~name_muni) +\n  theme_bw() +\n  theme(\n    legend.position = \"bottom\", \n    legend.direction = \"horizontal\", \n    axis.text.x=element_blank(),\n    axis.ticks.x=element_blank()\n  ) +\n  labs(\n    title = \"Age standardized COVID-19 mortality rates for Brazilian capitals\", \n    x = \"Epi Week\", \n    y = \"Rate per 100,000 inhab.\", \n    color = NULL\n  )"
  },
  {
    "objectID": "posts/std_br_covid_rates.html#session-info",
    "href": "posts/std_br_covid_rates.html#session-info",
    "title": "Age-adjusted COVID-19 mortality rates for Brazilian municipalities",
    "section": "Session info",
    "text": "Session info\n\nsessionInfo()\n\nR version 4.3.2 (2023-10-31)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 22.04.3 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.10.0 \nLAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.10.0\n\nlocale:\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_CA.UTF-8        LC_COLLATE=en_US.UTF-8    \n [5] LC_MONETARY=en_CA.UTF-8    LC_MESSAGES=en_US.UTF-8   \n [7] LC_PAPER=en_CA.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_CA.UTF-8 LC_IDENTIFICATION=C       \n\ntime zone: Europe/Paris\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lubridate_1.9.3 forcats_1.0.0   stringr_1.5.0   dplyr_1.1.3    \n [5] purrr_1.0.2     readr_2.1.4     tidyr_1.3.0     tibble_3.2.1   \n [9] ggplot2_3.4.4   tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.4        xfun_0.41           htmlwidgets_1.6.2  \n [4] brpop_0.3.0         processx_3.8.2      RApiSerialize_0.1.2\n [7] callr_3.7.3         tzdb_0.4.0          vctrs_0.6.4        \n[10] tools_4.3.2         ps_1.7.5            generics_0.1.3     \n[13] curl_5.1.0          parallel_4.3.2      proxy_0.4-27       \n[16] fansi_1.0.5         pkgconfig_2.0.3     KernSmooth_2.23-22 \n[19] data.table_1.14.8   checkmate_2.3.0     assertthat_0.2.1   \n[22] RcppParallel_5.1.7  lifecycle_1.0.3     compiler_4.3.2     \n[25] farver_2.1.1        multidplyr_0.1.3    munsell_0.5.0      \n[28] qs_0.25.5           codetools_0.2-19    class_7.3-22       \n[31] htmltools_0.5.7     yaml_2.3.7          pillar_1.9.0       \n[34] crayon_1.5.2        epitools_0.5-10.1   classInt_0.4-10    \n[37] parallelly_1.36.0   tidyselect_1.2.0    digest_0.6.33      \n[40] stringi_1.7.12      future_1.33.0       sf_1.0-14          \n[43] listenv_0.9.0       labeling_0.4.3      arrow_13.0.0.1     \n[46] fastmap_1.1.1       grid_4.3.2          colorspace_2.1-0   \n[49] cli_3.6.1           magrittr_2.0.3      utf8_1.2.4         \n[52] e1071_1.7-13        withr_2.5.2         scales_1.2.1       \n[55] backports_1.4.1     bit64_4.0.5         timechange_0.2.0   \n[58] httr_1.4.7          rmarkdown_2.25      globals_0.16.2     \n[61] tidyrates_0.0.1     bit_4.0.5           hms_1.1.3          \n[64] stringfish_0.15.8   evaluate_0.23       knitr_1.45         \n[67] rlang_1.1.2         Rcpp_1.0.11         DBI_1.1.3          \n[70] glue_1.6.2          geobr_1.8.1         rstudioapi_0.15.0  \n[73] vroom_1.6.4         jsonlite_1.8.7      R6_2.5.1           \n[76] units_0.8-4"
  },
  {
    "objectID": "posts/some-tips-for-sqlite.html",
    "href": "posts/some-tips-for-sqlite.html",
    "title": "Some tips to work with SQLite database",
    "section": "",
    "text": "Databases are very useful for handling large-than-memory datasets, a common problem in Data Science. Several database engines work very well with R and Posit has a nice guide overview of them.\nSQLite is a very popular engine due its simplicity. You do not need to install a database server on your environment because SQLite stores the database in a simple single file that you can modify, copy, store at Google Drive etc.\nI have been using it for some time and collected some practical tips for some practical situations.\nFirst, lets have some data to use at the examples.\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(nycflights13)\n\nflights &lt;- flights %&gt;% \n  select(year, month, day, hour, minute) %&gt;% \n  mutate(departure = make_date(year, month, day))\n\nglimpse(flights)\n\nRows: 336,776\nColumns: 6\n$ year      &lt;int&gt; 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, …\n$ month     &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ day       &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ hour      &lt;dbl&gt; 5, 5, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, 6, 6, …\n$ minute    &lt;dbl&gt; 15, 29, 40, 45, 0, 58, 0, 0, 0, 0, 0, 0, 0, 0, 0, 59, 0, 0, …\n$ departure &lt;date&gt; 2013-01-01, 2013-01-01, 2013-01-01, 2013-01-01, 2013-01-01,…\nI specifically created a variable called departure to have a date type on our example dataset."
  },
  {
    "objectID": "posts/some-tips-for-sqlite.html#dates",
    "href": "posts/some-tips-for-sqlite.html#dates",
    "title": "Some tips to work with SQLite database",
    "section": "Dates",
    "text": "Dates\nNatively, SQLite databases does not handle dates and this may be difficult in the beginning. Let’s explore some options to handle dates with SQLite.\nFirst, we need to create a database in a temporary file using two packages: DBI and RSQLite.\n\nlibrary(DBI)\nlibrary(RSQLite)\n\ndatabase_file &lt;- tempfile()\nconn &lt;- dbConnect(RSQLite::SQLite(), database_file)\n\nObserve that the conn object is NOT the database, is just a connection instruction to the database stored in a file.\nLets write the flights tibble to the database.\n\ndbWriteTable(conn, name = \"flights_table\", value = flights)\n\nNow, let’s take a look how the tibble was stored.\n\ntbl(conn, \"flights_table\") %&gt;%\n  head() %&gt;% \n  collect()\n\n# A tibble: 6 × 6\n   year month   day  hour minute departure\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n1  2013     1     1     5     15     15706\n2  2013     1     1     5     29     15706\n3  2013     1     1     5     40     15706\n4  2013     1     1     5     45     15706\n5  2013     1     1     6      0     15706\n6  2013     1     1     5     58     15706\n\n\nThe departure variable is no more a human-readable date, it is now a integer value in Unix time. That means: “the number of seconds since 1970-01-01 00:00:00 UTC”. Now very practical…\nThus, there as two options for this: you may convert your date variable to a string variable (as.character(...)) or use an argument called extended_types with the dbConnect command.\nIf you store the date variable as string, you will need to reconvert it to date wherever your collect data from your database, what is not very practical. Let’s see how the extented_types option works.\nFirst, let’s close our connection.\n\ndbDisconnect(conn)\n\nWe will modify our connection using this argument.\n\nconn &lt;- dbConnect(RSQLite::SQLite(), database_file, extended_types = TRUE)\n\nAnd now we will overwrite the data at the same table.\n\ndbWriteTable(conn, name = \"flights_table\", value = flights, overwrite = TRUE)\n\nLet’s see the result.\n\ntbl(conn, \"flights_table\") %&gt;% head() %&gt;% collect()\n\n# A tibble: 6 × 6\n   year month   day  hour minute departure \n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;date&gt;    \n1  2013     1     1     5     15 2013-01-01\n2  2013     1     1     5     29 2013-01-01\n3  2013     1     1     5     40 2013-01-01\n4  2013     1     1     5     45 2013-01-01\n5  2013     1     1     6      0 2013-01-01\n6  2013     1     1     5     58 2013-01-01\n\n\nVoilà! Now we can now see human-readable dates with a UNIX time variable."
  },
  {
    "objectID": "posts/some-tips-for-sqlite.html#parallel-write",
    "href": "posts/some-tips-for-sqlite.html#parallel-write",
    "title": "Some tips to work with SQLite database",
    "section": "Parallel write",
    "text": "Parallel write\nOne nice thing about databases is parallel writing. Imagine a function being executed in parallel and writing the results at the same database and even at the same table.\nThere are some nice tricks to allow it with SQLite. Basically, those are the options that I use.\n\nconn &lt;- DBI::dbConnect(RSQLite::SQLite(), database_file, extended_types = TRUE, synchronous = NULL)\ndbExecute(conn, \"PRAGMA busy_timeout = 5000\")\n\n[1] 0\n\ndbExecute(conn, \"BEGIN IMMEDIATE TRANSACTION\")\n\n[1] 0\n\ndbWriteTable(conn = conn, name = \"flights_table\", value = flights, append = TRUE)\ndbExecute(conn, \"COMMIT TRANSACTION\")\n\n[1] 0\n\ndbDisconnect(conn)\n\nThese options will secure that your connection waits other connections to conclude and immediately commit the transaction to the database."
  },
  {
    "objectID": "posts/some-tips-for-sqlite.html#delete-table",
    "href": "posts/some-tips-for-sqlite.html#delete-table",
    "title": "Some tips to work with SQLite database",
    "section": "Delete table",
    "text": "Delete table\nOne odd thing, after you delete a table in a database you need to vacuum it to get the free space.\nLets delete the file database and do some testing.\n\nunlink(database_file)\n\nWe will create the database with two equal tables.\n\nconn &lt;- dbConnect(RSQLite::SQLite(), database_file, extended_types = TRUE)\ndbWriteTable(conn = conn, name = \"flights_table_1\", value = flights)\ndbWriteTable(conn = conn, name = \"flights_table_2\", value = flights)\ndbDisconnect(conn)\n\nWhat’s is the size of the file?\n\nfs::file_size(database_file)\n\n13.4M\n\n\nNow, lets delete one of the tables.\n\nconn &lt;- dbConnect(RSQLite::SQLite(), database_file, extended_types = TRUE)\ndbRemoveTable(conn = conn, name = \"flights_table_1\")\ndbDisconnect(conn)\n\nfs::file_size(database_file)\n\n13.4M\n\n\nSame size… so, lets vacuum it!\n\nconn &lt;- dbConnect(RSQLite::SQLite(), database_file, extended_types = TRUE)\ndbExecute(conn, \"VACUUM;\")\n\n[1] 0\n\ndbDisconnect(conn)\n\nfs::file_size(database_file)\n\n6.69M\n\n\nAnd we have a smaller size!"
  },
  {
    "objectID": "posts/some-tips-for-sqlite.html#session-info",
    "href": "posts/some-tips-for-sqlite.html#session-info",
    "title": "Some tips to work with SQLite database",
    "section": "Session info",
    "text": "Session info\n\nsessionInfo()\n\nR version 4.2.0 (2022-04-22)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS 14.0\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] RSQLite_2.3.1      DBI_1.1.3          nycflights13_1.0.2 lubridate_1.9.3   \n[5] dplyr_1.1.3       \n\nloaded via a namespace (and not attached):\n [1] pillar_1.9.0      compiler_4.2.0    dbplyr_2.3.4      tools_4.2.0      \n [5] digest_0.6.33     bit_4.0.5         jsonlite_1.8.7    evaluate_0.22    \n [9] memoise_2.0.1     lifecycle_1.0.3   tibble_3.2.1      timechange_0.2.0 \n[13] pkgconfig_2.0.3   rlang_1.1.1       cli_3.6.1         rstudioapi_0.15.0\n[17] yaml_2.3.7        xfun_0.40         fastmap_1.1.1     withr_2.5.1      \n[21] knitr_1.44        fs_1.6.3          hms_1.1.3         generics_0.1.3   \n[25] vctrs_0.6.4       htmlwidgets_1.6.2 bit64_4.0.5       tidyselect_1.2.0 \n[29] glue_1.6.2        R6_2.5.1          fansi_1.0.5       rmarkdown_2.25   \n[33] purrr_1.0.2       blob_1.2.4        magrittr_2.0.3    htmltools_0.5.6.1\n[37] utf8_1.2.3        cachem_1.0.8"
  },
  {
    "objectID": "posts/rates.html",
    "href": "posts/rates.html",
    "title": "Crude and adjusted rates in a tidy way",
    "section": "",
    "text": "Rates allow the comparison between the number of counts between multiple classes with different population sizes. For example, 10 disease cases that occur in 100 population region have a different proportional importance than 10 cases in a 1,000 population region.\nIn epidemiology, those overall rates may be compared if both populations are similar. If the populations have different constitutions, the comparison between overall rates may be misleading as the disease may affect the age groups differently."
  },
  {
    "objectID": "posts/rates.html#introduction",
    "href": "posts/rates.html#introduction",
    "title": "Crude and adjusted rates in a tidy way",
    "section": "",
    "text": "Rates allow the comparison between the number of counts between multiple classes with different population sizes. For example, 10 disease cases that occur in 100 population region have a different proportional importance than 10 cases in a 1,000 population region.\nIn epidemiology, those overall rates may be compared if both populations are similar. If the populations have different constitutions, the comparison between overall rates may be misleading as the disease may affect the age groups differently."
  },
  {
    "objectID": "posts/rates.html#available-packages",
    "href": "posts/rates.html#available-packages",
    "title": "Crude and adjusted rates in a tidy way",
    "section": "Available packages",
    "text": "Available packages\nSome R packages, such as epitools and epiR, allow the computation of crude and adjusted rates. Those packages use as input, vectors or matrices containing the events count and population per age group.\nThe downside of them is that you need to use the rates function manually repeatedly times if you are going to compute rates for several regions."
  },
  {
    "objectID": "posts/rates.html#tidyrates-package",
    "href": "posts/rates.html#tidyrates-package",
    "title": "Crude and adjusted rates in a tidy way",
    "section": "tidyrates package",
    "text": "tidyrates package\nI created the tidyrates to compute direct and indirect adjusted rates for several regions, years, and other keys that you might use. Internally, the package wraps the epitools functions and applies them to each group.\nhttps://rfsaldanha.github.io/tidyrates/"
  },
  {
    "objectID": "posts/rates.html#example-general-mortality-rates-in-brazilian-states-from-2010-to-2021",
    "href": "posts/rates.html#example-general-mortality-rates-in-brazilian-states-from-2010-to-2021",
    "title": "Crude and adjusted rates in a tidy way",
    "section": "Example: General mortality rates in Brazilian states, from 2010 to 2021",
    "text": "Example: General mortality rates in Brazilian states, from 2010 to 2021\n\nMortality data\nThe dataset bellow presents the total number of deaths by any cause (general mortality) for the Brazilian states per age group, from 2010 to 2021. I collected the data from the PCDaS platform.\n\ngm_br_uf &lt;- readRDS(file = \"gm_br_uf.rds\")\n\nDT::datatable(gm_br_uf)\n\n\n\n\n\n\n\nPopulation data\nTo compute rates, we will need the state population size per age group for each year. The brpop package present estimates for this. We just need to do some adaptations.\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(brpop)\n\n1pop_uf &lt;- uf_pop() %&gt;%\n2  filter(year %in% 2010:2021) %&gt;%\n3  rename(population = pop) %&gt;%\n4  filter(age_group != \"Total\") %&gt;%\n5  mutate(year = as.character(year))\n\n\n1\n\nGet population table from brpop package.\n\n2\n\nFilter the desired years\n\n3\n\nRename the population variable\n\n4\n\nRemove population totals, keeping only the population per age group\n\n5\n\nChange year variable from numeric to character.\n\n\n\n\n\nDT::datatable(pop_uf)\n\n\n\n\n\n\n\nReference population\nTo compute standardized rates, we need a reference population. We may use the SEER reference population (which is a corrected version from the WHO reference population) that is present in the tidyrates package. We need to use the same age group labels and also merge the population from 80 years or more to make it compatible with our data.\n\nseer2 &lt;- tidyrates::seer_std_pop %&gt;%\n  mutate(age_group = case_when(\n    age_group == \"0-4\" ~ \"From 0 to 4 years\",\n    age_group == \"5-9\" ~ \"From 5 to 9 years\",\n    age_group == \"10-14\" ~ \"From 10 to 14 years\",\n    age_group == \"15-19\" ~ \"From 15 to 19 years\",\n    age_group == \"20-24\" ~ \"From 20 to 24 years\",\n    age_group == \"25-29\" ~ \"From 25 to 29 years\",\n    age_group == \"30-34\" ~ \"From 30 to 34 years\",\n    age_group == \"35-39\" ~ \"From 35 to 39 years\",\n    age_group == \"40-44\" ~ \"From 40 to 44 years\",\n    age_group == \"45-49\" ~ \"From 45 to 49 years\",\n    age_group == \"50-54\" ~ \"From 50 to 54 years\",\n    age_group == \"55-59\" ~ \"From 55 to 59 years\",\n    age_group == \"60-64\" ~ \"From 60 to 64 years\",\n    age_group == \"65-69\" ~ \"From 65 to 69 years\",\n    age_group == \"70-74\" ~ \"From 70 to 74 years\",\n    age_group == \"75-79\" ~ \"From 75 to 79 years\",\n    age_group == \"80-84\" ~ \"From 80 years or more\",\n    age_group == \"85-89\" ~ \"From 80 years or more\",\n    age_group == \"90-94\" ~ \"From 80 years or more\",\n    age_group == \"95-99\" ~ \"From 80 years or more\",\n    age_group == \"100+\" ~ \"From 80 years or more\",\n    .default = age_group\n  )) %&gt;%\n  group_by(age_group) %&gt;%\n  summarise(population = sum(population)) %&gt;%\n  ungroup()\n\n\nDT::datatable(seer2)\n\n\n\n\n\n\n\nCrude and direct adjusted rates\nNow, we can join the mortality dataset with the population dataset by uf, year, and age groups.\n\ngm_pop &lt;- left_join(gm_br_uf, pop_uf, by = c(\"uf\", \"year\", \"age_group\")) %&gt;%\n  pivot_longer(cols = c(\"events\", \"population\"))\n\nDT::datatable(gm_pop)\n\n\n\n\n\nWith the data ready, we can compute the direct adjusted rates with the tidyrates package for all UFs and years.\n\nlibrary(tidyrates)\n\nrates &lt;- rate_adj_direct(.data = gm_pop, .std = seer2, .keys = c(\"year\", \"uf\"))\n\nDT::datatable(rates)\n\n\n\n\n\n\n\nPlot\nTo make a plot, let’s do some modifications and pivot the data.\n\nlibrary(ggplot2)\nlibrary(geofacet)\n\nrates_for_plot &lt;- rates %&gt;%\n  mutate(year = as.numeric(year)) %&gt;%\n  pivot_longer(cols = c(\"crude.rate\", \"adj.rate\", \"lci\", \"uci\")) %&gt;%\n  filter(name %in% c(\"crude.rate\", \"adj.rate\")) %&gt;%\n  mutate(value = value * 100000)\n\nci_for_plot &lt;- rates %&gt;%\n  mutate(year = as.numeric(year)) %&gt;%\n  select(year, uf, lci, uci) %&gt;%\n  mutate(\n    lci = lci * 100000,\n    uci = uci * 100000\n  )\n\nbr_grid &lt;- br_states_grid1 %&gt;%\n  mutate(code_num = case_when(\n    code == \"RO\" ~ 11,\n    code == \"AC\" ~ 12,\n    code == \"AM\" ~ 13,\n    code == \"RR\" ~ 14,\n    code == \"PA\" ~ 15,\n    code == \"AP\" ~ 16,\n    code == \"TO\" ~ 17,\n    code == \"MA\" ~ 21,\n    code == \"PI\" ~ 22,\n    code == \"CE\" ~ 23,\n    code == \"RN\" ~ 24,\n    code == \"PB\" ~ 25,\n    code == \"PE\" ~ 26,\n    code == \"AL\" ~ 27,\n    code == \"SE\" ~ 28,\n    code == \"BA\" ~ 29,\n    code == \"MG\" ~ 31,\n    code == \"ES\" ~ 32,\n    code == \"RJ\" ~ 33,\n    code == \"SP\" ~ 35,\n    code == \"PR\" ~ 41,\n    code == \"SC\" ~ 42,\n    code == \"RS\" ~ 43,\n    code == \"MS\" ~ 50,\n    code == \"MT\" ~ 51,\n    code == \"GO\" ~ 52,\n    code == \"DF\" ~ 53,\n  ))\n\n\nggplot(data = rates_for_plot) +\n  geom_line(aes(x = year, y = value, color = name)) +\n  geom_ribbon(data = ci_for_plot, aes(x = year, ymin = lci, ymax = uci), alpha=0.3, fill = \"lightpink\", color = \"pink\") +\n  facet_geo(~uf, grid = br_grid, label = \"name\") +\n  theme_bw() +\n  theme(legend.position = \"bottom\", legend.direction = \"horizontal\") +\n  labs(color = \"Rate\", x = \"Year\", y = \"Rate per 100,000 inhab.\")"
  },
  {
    "objectID": "posts/rates.html#session-info",
    "href": "posts/rates.html#session-info",
    "title": "Crude and adjusted rates in a tidy way",
    "section": "Session info",
    "text": "Session info\n\nsessionInfo()\n\nR version 4.3.2 (2023-10-31)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 22.04.3 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.10.0 \nLAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.10.0\n\nlocale:\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_CA.UTF-8        LC_COLLATE=en_US.UTF-8    \n [5] LC_MONETARY=en_CA.UTF-8    LC_MESSAGES=en_US.UTF-8   \n [7] LC_PAPER=en_CA.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_CA.UTF-8 LC_IDENTIFICATION=C       \n\ntime zone: Europe/Paris\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] geofacet_0.2.0  ggplot2_3.4.4   tidyrates_0.0.1 brpop_0.3.0    \n[5] tidyr_1.3.0     dplyr_1.1.3    \n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.4        xfun_0.41           bslib_0.5.1        \n [4] htmlwidgets_1.6.2   ggrepel_0.9.4       processx_3.8.2     \n [7] lattice_0.22-5      RApiSerialize_0.1.2 callr_3.7.3        \n[10] vctrs_0.6.4         tools_4.3.2         crosstalk_1.2.0    \n[13] ps_1.7.5            generics_0.1.3      parallel_4.3.2     \n[16] tibble_3.2.1        proxy_0.4-27        fansi_1.0.5        \n[19] pkgconfig_2.0.3     KernSmooth_2.23-22  checkmate_2.3.0    \n[22] RcppParallel_5.1.7  lifecycle_1.0.3     farver_2.1.1       \n[25] compiler_4.3.2      multidplyr_0.1.3    munsell_0.5.0      \n[28] qs_0.25.5           codetools_0.2-19    imguR_1.0.3        \n[31] htmltools_0.5.7     class_7.3-22        sass_0.4.7         \n[34] yaml_2.3.7          pillar_1.9.0        jquerylib_0.1.4    \n[37] ellipsis_0.3.2      epitools_0.5-10.1   classInt_0.4-10    \n[40] DT_0.30             cachem_1.0.8        parallelly_1.36.0  \n[43] tidyselect_1.2.0    digest_0.6.33       future_1.33.0      \n[46] sf_1.0-14           purrr_1.0.2         listenv_0.9.0      \n[49] labeling_0.4.3      forcats_1.0.0       rnaturalearth_0.3.4\n[52] geogrid_0.1.2       fastmap_1.1.1       grid_4.3.2         \n[55] colorspace_2.1-0    cli_3.6.1           magrittr_2.0.3     \n[58] utf8_1.2.4          e1071_1.7-13        withr_2.5.2        \n[61] scales_1.2.1        backports_1.4.1     sp_2.1-1           \n[64] httr_1.4.7          rmarkdown_2.25      jpeg_0.1-10        \n[67] globals_0.16.2      gridExtra_2.3       png_0.1-8          \n[70] stringfish_0.15.8   evaluate_0.23       knitr_1.45         \n[73] rlang_1.1.2         Rcpp_1.0.11         glue_1.6.2         \n[76] DBI_1.1.3           rstudioapi_0.15.0   jsonlite_1.8.7     \n[79] R6_2.5.1            units_0.8-4"
  },
  {
    "objectID": "posts/query_local_parquet_files.html",
    "href": "posts/query_local_parquet_files.html",
    "title": "Query local parquet files",
    "section": "",
    "text": "After releasing parquet files with zonal statistics of climate indicators for Brazilian municipalities, I received some inquiries about how to query the files in an efficient way, avoiding loading all data to the memory. This blog post shows some examples about how to do it.\nAfter downloading the desired data, load the following packages on R."
  },
  {
    "objectID": "posts/query_local_parquet_files.html#packages",
    "href": "posts/query_local_parquet_files.html#packages",
    "title": "Query local parquet files",
    "section": "Packages",
    "text": "Packages\n\n1library(tidyverse)\n2library(arrow)\n\n\n1\n\nThe {tidyverse} load several packages like {dplyr} that will help us to handle the data.\n\n2\n\nThe {arrow} package present efficient ways to connect to parquet files."
  },
  {
    "objectID": "posts/query_local_parquet_files.html#access-the-data",
    "href": "posts/query_local_parquet_files.html#access-the-data",
    "title": "Query local parquet files",
    "section": "Access the data",
    "text": "Access the data\n\nSmall datasets\nThe most straightforward way to read a parquet file is with the read_parquet() command. But this will load all parquet data to the memory. Use this command only with small parquet files (and remember that parquet files are usually compressed).\n\n\nLarge datasets\nFor large datasets, we can use the open_dataset() command. This will create a virtual link to the parquet file but will not load it to the memory.\n\ntemp_mean &lt;- open_dataset(sources = \"~/Downloads/2m_temperature_mean.parquet\")\n\nprint(temp_mean)\n\nFileSystemDataset with 1 Parquet file\ncode_muni: int64\ndate: date32[day]\nname: string\nvalue: float\n\n\nWith the sources argument, you can point to a specific parquet file or to a folder with several files. Check the help to learn more about this command ?open_dataset.\nWhen we print the temp_mean object, the console shows how many parquet files are linked and the available variables.\n\nDimension\nTo check the dimension (number of lines and columns), you can use the dim() command.\n\ndim(temp_mean)\n\n[1] 742164605         4\n\n\nThat is a lot of rows!\n\n\nPrint first rows\nTo take a look at the file, we can print the first rows.\n\n1temp_mean |&gt;\n2  head() |&gt;\n3  collect() |&gt;\n4  gt::gt()\n\n\n1\n\nThe object that points to the parquet file.\n\n2\n\nR base command to access the first rows of a data frame. You can use head(10) to access more lines if you want.\n\n3\n\nCollect the data from the parquet file to the memory.\n\n4\n\nPresent the result in a nice table.\n\n\n\n\n\n\n\n\n\n\ncode_muni\ndate\nname\nvalue\n\n\n\n\n1100015\n1950-01-01\n2m_temperature_mean_mean\n298.4606\n\n\n1100015\n1950-01-02\n2m_temperature_mean_mean\n298.1530\n\n\n1100015\n1950-01-03\n2m_temperature_mean_mean\n298.6412\n\n\n1100015\n1950-01-04\n2m_temperature_mean_mean\n297.9100\n\n\n1100015\n1950-01-05\n2m_temperature_mean_mean\n298.3538\n\n\n1100015\n1950-01-06\n2m_temperature_mean_mean\n297.4383"
  },
  {
    "objectID": "posts/query_local_parquet_files.html#query",
    "href": "posts/query_local_parquet_files.html#query",
    "title": "Query local parquet files",
    "section": "Query",
    "text": "Query\nNow we are ready to query the data. We can use some dplyr verbs to filter the data we want.\n\nres1 &lt;- temp_mean |&gt;\n1  filter(code_muni %in% c(3303401, 3136702, 3304557)) |&gt;\n2  filter(year(date) &gt;= 2000) |&gt;\n3  filter(name == \"2m_temperature_mean_mean\") |&gt;\n4  collect()\n\n\n1\n\nFilter some municipalities (Nova Friburgo, Juiz de Fora and Rio de Janeiro IBGE codes).\n\n2\n\nFilter dates where the year is 2000 or greater.\n\n3\n\nFilter the indicator.\n\n4\n\nCollect the data from the parquet to memory.\n\n\n\n\nLet’s inspect the results.\n\ndim(res1)\n\n[1] 25203     4\n\n\n\nres1 |&gt; head() |&gt; gt::gt()\n\n\n\n\n\n\n\ncode_muni\ndate\nname\nvalue\n\n\n\n\n3136702\n2000-01-01\n2m_temperature_mean_mean\n294.2239\n\n\n3136702\n2000-01-02\n2m_temperature_mean_mean\n293.3698\n\n\n3136702\n2000-01-03\n2m_temperature_mean_mean\n293.7604\n\n\n3136702\n2000-01-04\n2m_temperature_mean_mean\n295.7519\n\n\n3136702\n2000-01-05\n2m_temperature_mean_mean\n296.5222\n\n\n3136702\n2000-01-06\n2m_temperature_mean_mean\n296.7838\n\n\n\n\n\n\n\nAs you can see, the command collect() does all the magic. Filters and other verbs are translated and executed with the file on disk and only the results are returned.\nWe can also make some aggregations before loading the data into memory. By example, let’s convert this data from daily to weekly.\n\nres2 &lt;- temp_mean |&gt;\n  filter(code_muni %in% c(3303401, 3136702, 3304557)) |&gt;\n  filter(year(date) &gt;= 2000) |&gt;\n  filter(name == \"2m_temperature_mean_mean\") |&gt;\n1  mutate(date_w = ceiling_date(date, unit = \"week\")) |&gt;\n2  group_by(code_muni, date_w, name) |&gt;\n3  summarise(value = mean(value, na.rm = TRUE)) |&gt;\n4  ungroup() |&gt;\n5  collect()\n\n\n1\n\nFirst, we create a date variable with the first date of the week. Take a look on ?ceiling_date to learn more about this function.\n\n2\n\nNow we will group or data by municipality code, week start date and indicator name.\n\n3\n\nSummarize value by the mean of values.\n\n4\n\nUngroup the data.\n\n5\n\nCollect the data from the parquet file to memory.\n\n\n\n\n\ndim(res2)\n\n[1] 3603    4\n\n\n\nres2 |&gt; head() |&gt; gt::gt()\n\n\n\n\n\n\n\ncode_muni\ndate_w\nname\nvalue\n\n\n\n\n3136702\n2000-01-02\n2m_temperature_mean_mean\n294.2239\n\n\n3136702\n2000-01-09\n2m_temperature_mean_mean\n295.3651\n\n\n3136702\n2000-01-16\n2m_temperature_mean_mean\n296.8024\n\n\n3136702\n2000-01-23\n2m_temperature_mean_mean\n295.6669\n\n\n3136702\n2000-01-30\n2m_temperature_mean_mean\n293.7877\n\n\n3136702\n2000-02-06\n2m_temperature_mean_mean\n295.9360\n\n\n\n\n\n\n\nAnd a simple plot.\n\nggplot(data = res2, aes(x = date_w, y = value, group = code_muni, color = as.character(code_muni))) +\n  geom_line(stat = \"identity\", alpha = .5) +\n  theme_bw() +\n  theme(legend.position = \"bottom\", legend.direction = \"horizontal\") +\n  labs(title = \"Average temperature\", color = \"Municipality\",\n       x = \"Week start\", y = \"Temp (k)\")"
  },
  {
    "objectID": "posts/query_local_parquet_files.html#bonus-indicator-meaning",
    "href": "posts/query_local_parquet_files.html#bonus-indicator-meaning",
    "title": "Query local parquet files",
    "section": "Bonus: indicator meaning",
    "text": "Bonus: indicator meaning\nWhat the indicator 2m_temperature_mean_mean means?\n\n2m_temperature is the base indicator, it is the atmospheric temperature at 2m height.\n_mean (the first mean): we are using the average temperature of the day. It could be the maximum (max) or minimum (min) for example.\n_mean (the second mean): we are dealing with zonal statistics. This is the zonal statistic that was applied on all spatial cells that intersects the municipality. We also have the max, min, sd (standard deviation) and count (cells count). For volume indicators there is also the sum.\n\nUsually, for temperature we will use the mean, and for precipitation the sum."
  },
  {
    "objectID": "posts/query_local_parquet_files.html#session-info",
    "href": "posts/query_local_parquet_files.html#session-info",
    "title": "Query local parquet files",
    "section": "Session info",
    "text": "Session info\n\nsessioninfo::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.3 (2024-02-29)\n os       Ubuntu 22.04.4 LTS\n system   x86_64, linux-gnu\n ui       X11\n language en_US:en\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Paris\n date     2024-04-04\n pandoc   3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n arrow       * 15.0.1  2024-03-12 [1] CRAN (R 4.3.3)\n assertthat    0.2.1   2019-03-21 [1] CRAN (R 4.3.1)\n bit           4.0.5   2022-11-15 [1] CRAN (R 4.3.1)\n bit64         4.0.5   2020-08-30 [1] CRAN (R 4.3.1)\n cli           3.6.2   2023-12-11 [1] CRAN (R 4.3.2)\n colorspace    2.1-0   2023-01-23 [1] CRAN (R 4.3.1)\n digest        0.6.35  2024-03-11 [1] CRAN (R 4.3.3)\n dplyr       * 1.1.4   2023-11-17 [1] CRAN (R 4.3.2)\n evaluate      0.23    2023-11-01 [1] CRAN (R 4.3.1)\n fansi         1.0.6   2023-12-08 [1] CRAN (R 4.3.2)\n farver        2.1.1   2022-07-06 [1] CRAN (R 4.3.1)\n fastmap       1.1.1   2023-02-24 [1] CRAN (R 4.3.1)\n forcats     * 1.0.0   2023-01-29 [1] CRAN (R 4.3.1)\n generics      0.1.3   2022-07-05 [1] CRAN (R 4.3.1)\n ggplot2     * 3.5.0   2024-02-23 [1] CRAN (R 4.3.2)\n glue          1.7.0   2024-01-09 [1] CRAN (R 4.3.2)\n gt            0.10.1  2024-01-17 [1] CRAN (R 4.3.2)\n gtable        0.3.4   2023-08-21 [1] CRAN (R 4.3.1)\n hms           1.1.3   2023-03-21 [1] CRAN (R 4.3.1)\n htmltools     0.5.8.1 2024-04-04 [1] CRAN (R 4.3.3)\n htmlwidgets   1.6.4   2023-12-06 [1] CRAN (R 4.3.2)\n jsonlite      1.8.8   2023-12-04 [1] CRAN (R 4.3.2)\n knitr         1.45    2023-10-30 [1] CRAN (R 4.3.1)\n labeling      0.4.3   2023-08-29 [1] CRAN (R 4.3.1)\n lifecycle     1.0.4   2023-11-07 [1] CRAN (R 4.3.2)\n lubridate   * 1.9.3   2023-09-27 [1] CRAN (R 4.3.1)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.3.1)\n munsell       0.5.1   2024-04-01 [1] CRAN (R 4.3.3)\n pillar        1.9.0   2023-03-22 [1] CRAN (R 4.3.1)\n pkgconfig     2.0.3   2019-09-22 [1] CRAN (R 4.3.1)\n purrr       * 1.0.2   2023-08-10 [1] CRAN (R 4.3.1)\n R6            2.5.1   2021-08-19 [1] CRAN (R 4.3.1)\n readr       * 2.1.5   2024-01-10 [1] CRAN (R 4.3.2)\n rlang         1.1.3   2024-01-10 [1] CRAN (R 4.3.2)\n rmarkdown     2.26    2024-03-05 [1] CRAN (R 4.3.3)\n rstudioapi    0.16.0  2024-03-24 [1] CRAN (R 4.3.3)\n sass          0.4.9   2024-03-15 [1] CRAN (R 4.3.3)\n scales        1.3.0   2023-11-28 [1] CRAN (R 4.3.2)\n sessioninfo   1.2.2   2021-12-06 [1] CRAN (R 4.3.1)\n stringi       1.8.3   2023-12-11 [1] CRAN (R 4.3.2)\n stringr     * 1.5.1   2023-11-14 [1] CRAN (R 4.3.2)\n tibble      * 3.2.1   2023-03-20 [1] CRAN (R 4.3.1)\n tidyr       * 1.3.1   2024-01-24 [1] CRAN (R 4.3.2)\n tidyselect    1.2.1   2024-03-11 [1] CRAN (R 4.3.3)\n tidyverse   * 2.0.0   2023-02-22 [1] CRAN (R 4.3.1)\n timechange    0.3.0   2024-01-18 [1] CRAN (R 4.3.2)\n tzdb          0.4.0   2023-05-12 [1] CRAN (R 4.3.1)\n utf8          1.2.4   2023-10-22 [1] CRAN (R 4.3.1)\n vctrs         0.6.5   2023-12-01 [1] CRAN (R 4.3.2)\n withr         3.0.0   2024-01-16 [1] CRAN (R 4.3.2)\n xfun          0.43    2024-03-25 [1] CRAN (R 4.3.3)\n xml2          1.3.6   2023-12-04 [1] CRAN (R 4.3.2)\n yaml          2.3.8   2023-12-11 [1] CRAN (R 4.3.2)\n\n [1] /home/raphael/R/x86_64-pc-linux-gnu-library/4.3\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/hot_2023.html",
    "href": "posts/hot_2023.html",
    "title": "How 2023 was hot in different Brazilian municipalities?",
    "section": "",
    "text": "According to the last Copernicus report, 2023 was exceptional hot year. The image bellow from this report circulated a lot at the social media.\nClearly, 2023 temperatures are the highest in comparison to other years and the temperature difference to the previous years is unmatched.\nIt is important to comprehend that this graph used data from all the globe surface, including the oceans, giving us an overall picture. At some places this temperature anomaly may be different, with weaker or even stronger differences.\nBut how are 2023 temperatures at the local level, where I live?\nTo answer this question, first we need to have specific data by location. One way to produce this specific data is with zonal statistics.\nI created a dataset with zonal statistics for the Brazilian municipalities, with data from 1950 to 2022 using data from the ERA5-Land reanalysis.\nThus, to make similar graphs to Brazilian municipalities, I used the same methodology with the available 2023 ERA5-Land data (from January to October).\nThe data is ready, let’s make some plots! First, load the necessary packages.\nlibrary(arrow)\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(timetk)\nlibrary(ggplot2)\nlibrary(directlabels)\nlibrary(viridis)\nlibrary(ggdark)\nNow, we will read the data from the 1950-2022 period. This parquet file is available to download at Zenodo with daily data to all Brazilian municipalities, from 1950 to 2022.\n1temp &lt;- open_dataset(sources = \"../../brclim2/output_data/parquet/2m_temperature_mean.parquet\") %&gt;%\n2  filter(name == \"2m_temperature_mean_mean\") %&gt;%\n3  filter(date &gt;= as.Date(\"1990-01-01\")) %&gt;%\n4  collect() %&gt;%\n  mutate(\n5    year = year(date),\n6    date = update(date, year = 2023),\n7    value = value - 273.15\n  )\n\n\n1\n\nOpen the parquet dataset without loading it all to the memory.\n\n2\n\nFilter the desired indicator, on this case, the average temperature.\n\n3\n\nKeep the data only after 1990 to produce a cleaner graph.\n\n4\n\nCollect the data from the parquet file to memory.\n\n5\n\nCreate a new variable with the year.\n\n6\n\nChange the year of all dates to 2023. This will make easer to overlay the time series of each year.\n\n7\n\nThe original values are in Kelvin. Let’s convert it to Celsius Degrees.\nAnd now load the 2023 data. I will share this parquet file later…\ntemp_23 &lt;- open_dataset(sources = \"../../brclim2/output_data/data_2023/parquet/2m_temperature_mean.parquet\") %&gt;%\n  filter(name == \"2m_temperature_mean_mean\") %&gt;%\n  collect() %&gt;%\n  mutate(value = value - 273.15)\nTo make a graph, we can set a municipality code for reference (e.g. Rio de Janeiro, RJ) and create a list object with all the necessary data. Then, we create a simple and nice ggplot ;-)\nCode\ncode_ref &lt;- 3304557\n\ntemp_list &lt;- list(\n  temp = temp %&gt;% filter(code_muni == code_ref),\n  temp_23 = temp_23 %&gt;% filter(code_muni == code_ref)\n)\n\nggplot() +\n  geom_line(\n    data = temp_list$temp, \n    aes(x = date, y = value, color = year, group = year),\n    alpha = .3, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n  scale_color_viridis(option = \"plasma\") +\n  geom_line(\n    data = temp_list$temp_23, \n    aes(x = date, y = value), \n    color = \"red\",\n    lwd = 1, alpha = .7, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n  geom_dl(\n    data = temp_list$temp_23, \n    aes(x = date, y = value, label = \"2023\"),\n    method = list(dl.combine(\"last.points\")), \n    color = \"red\",\n    cex = 0.8\n  ) +\n  ylim(15, 35) +\n  scale_x_date(date_breaks = \"1 month\", date_labels = \"%b\") +\n  dark_theme_gray() +\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\"\n  ) +\n  labs(\n    title = \"Rio de Janeiro, RJ\", \n    x = \"Date\",\n    y = \"Average temperature (°C)\", \n    color = NULL\n  )\nFollowing the global trend, the temperature at Rio Janeiro, RJ at 2023 has been exceptional hot after September.\nBellow, the same plot for the Brazilian capitals and some selected municipalities. You will see that at some municipalities the 2023 temperatures are higher than other years, but at some others places the values are lower. This is the climate change effect, leading climate indicators to different extremes, increasing its variabilities."
  },
  {
    "objectID": "posts/hot_2023.html#porto-velho-ro",
    "href": "posts/hot_2023.html#porto-velho-ro",
    "title": "How 2023 was hot in different Brazilian municipalities?",
    "section": "Porto Velho, RO",
    "text": "Porto Velho, RO\n\n\nCode\ncode_ref &lt;- 1100205\n\ntemp_list &lt;- list(\n  temp = temp %&gt;% filter(code_muni == code_ref),\n  temp_23 = temp_23 %&gt;% filter(code_muni == code_ref)\n)\n\nggplot() +\n  geom_line(\n    data = temp_list$temp, \n    aes(x = date, y = value, color = year, group = year),\n    alpha = .3, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n  scale_color_viridis(option = \"plasma\") +\n  geom_line(\n    data = temp_list$temp_23, \n    aes(x = date, y = value), \n    color = \"red\",\n    lwd = 1, alpha = .7, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n    geom_dl(\n    data = temp_list$temp_23, \n    aes(x = date, y = value, label = \"2023\"),\n    method = list(dl.combine(\"last.points\")), \n    color = \"red\",\n    cex = 0.8\n  ) +\n  ylim(15, 35) +\n  scale_x_date(date_breaks = \"1 month\", date_labels = \"%b\") +\n  dark_theme_gray() +\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\"\n  ) +\n  labs(\n    title = \"Porto Velho, RO\", \n    x = \"Date\",\n    y = \"Average temperature (°C)\", \n    color = NULL\n  )"
  },
  {
    "objectID": "posts/hot_2023.html#manaus-am",
    "href": "posts/hot_2023.html#manaus-am",
    "title": "How 2023 was hot in different Brazilian municipalities?",
    "section": "Manaus, AM",
    "text": "Manaus, AM\n\n\nCode\ncode_ref &lt;- 1302603\n\ntemp_list &lt;- list(\n  temp = temp %&gt;% filter(code_muni == code_ref),\n  temp_23 = temp_23 %&gt;% filter(code_muni == code_ref)\n)\n\nggplot() +\n  geom_line(\n    data = temp_list$temp, \n    aes(x = date, y = value, color = year, group = year),\n    alpha = .3, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n  scale_color_viridis(option = \"plasma\") +\n  geom_line(\n    data = temp_list$temp_23, \n    aes(x = date, y = value), \n    color = \"red\",\n    lwd = 1, alpha = .7, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n    geom_dl(\n    data = temp_list$temp_23, \n    aes(x = date, y = value, label = \"2023\"),\n    method = list(dl.combine(\"last.points\")), \n    color = \"red\",\n    cex = 0.8\n  ) +\n  ylim(15, 35) +\n  scale_x_date(date_breaks = \"1 month\", date_labels = \"%b\") +\n  dark_theme_gray() +\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\"\n  ) +\n  labs(\n    title = \"Manaus, AM\", \n    x = \"Date\",\n    y = \"Average temperature (°C)\", \n    color = NULL\n  )"
  },
  {
    "objectID": "posts/hot_2023.html#rio-branco-ac",
    "href": "posts/hot_2023.html#rio-branco-ac",
    "title": "How 2023 was hot in different Brazilian municipalities?",
    "section": "Rio Branco, AC",
    "text": "Rio Branco, AC\n\n\nCode\ncode_ref &lt;- 1200401\n\ntemp_list &lt;- list(\n  temp = temp %&gt;% filter(code_muni == code_ref),\n  temp_23 = temp_23 %&gt;% filter(code_muni == code_ref)\n)\n\nggplot() +\n  geom_line(\n    data = temp_list$temp, \n    aes(x = date, y = value, color = year, group = year),\n    alpha = .3, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n  scale_color_viridis(option = \"plasma\") +\n  geom_line(\n    data = temp_list$temp_23, \n    aes(x = date, y = value), \n    color = \"red\",\n    lwd = 1, alpha = .7, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n    geom_dl(\n    data = temp_list$temp_23, \n    aes(x = date, y = value, label = \"2023\"),\n    method = list(dl.combine(\"last.points\")), \n    color = \"red\",\n    cex = 0.8\n  ) +\n  ylim(15, 35) +\n  scale_x_date(date_breaks = \"1 month\", date_labels = \"%b\") +\n  dark_theme_gray() +\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\"\n  ) +\n  labs(\n    title = \"Rio Branco, AC\", \n    x = \"Date\",\n    y = \"Average temperature (°C)\", \n    color = NULL\n  )"
  },
  {
    "objectID": "posts/hot_2023.html#campo-grande-ms",
    "href": "posts/hot_2023.html#campo-grande-ms",
    "title": "How 2023 was hot in different Brazilian municipalities?",
    "section": "Campo Grande, MS",
    "text": "Campo Grande, MS\n\n\nCode\ncode_ref &lt;- 5002704\n\ntemp_list &lt;- list(\n  temp = temp %&gt;% filter(code_muni == code_ref),\n  temp_23 = temp_23 %&gt;% filter(code_muni == code_ref)\n)\n\nggplot() +\n  geom_line(\n    data = temp_list$temp, \n    aes(x = date, y = value, color = year, group = year),\n    alpha = .3, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n  scale_color_viridis(option = \"plasma\") +\n  geom_line(\n    data = temp_list$temp_23, \n    aes(x = date, y = value), \n    color = \"red\",\n    lwd = 1, alpha = .7, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n    geom_dl(\n    data = temp_list$temp_23, \n    aes(x = date, y = value, label = \"2023\"),\n    method = list(dl.combine(\"last.points\")), \n    color = \"red\",\n    cex = 0.8\n  ) +\n  ylim(15, 35) +\n  scale_x_date(date_breaks = \"1 month\", date_labels = \"%b\") +\n  dark_theme_gray() +\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\"\n  ) +\n  labs(\n    title = \"Campo Grande, MS\", \n    x = \"Date\",\n    y = \"Average temperature (°C)\", \n    color = NULL\n  )"
  },
  {
    "objectID": "posts/hot_2023.html#macapá-ap",
    "href": "posts/hot_2023.html#macapá-ap",
    "title": "How 2023 was hot in different Brazilian municipalities?",
    "section": "Macapá, AP",
    "text": "Macapá, AP\n\n\nCode\ncode_ref &lt;- 1600303\n\ntemp_list &lt;- list(\n  temp = temp %&gt;% filter(code_muni == code_ref),\n  temp_23 = temp_23 %&gt;% filter(code_muni == code_ref)\n)\n\nggplot() +\n  geom_line(\n    data = temp_list$temp, \n    aes(x = date, y = value, color = year, group = year),\n    alpha = .3, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n  scale_color_viridis(option = \"plasma\") +\n  geom_line(\n    data = temp_list$temp_23, \n    aes(x = date, y = value), \n    color = \"red\",\n    lwd = 1, alpha = .7, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n    geom_dl(\n    data = temp_list$temp_23, \n    aes(x = date, y = value, label = \"2023\"),\n    method = list(dl.combine(\"last.points\")), \n    color = \"red\",\n    cex = 0.8\n  ) +\n  ylim(15, 35) +\n  scale_x_date(date_breaks = \"1 month\", date_labels = \"%b\") +\n  dark_theme_gray() +\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\"\n  ) +\n  labs(\n    title = \"Macapá, AP\", \n    x = \"Date\",\n    y = \"Average temperature (°C)\", \n    color = NULL\n  )"
  },
  {
    "objectID": "posts/hot_2023.html#brasília-df",
    "href": "posts/hot_2023.html#brasília-df",
    "title": "How 2023 was hot in different Brazilian municipalities?",
    "section": "Brasília, DF",
    "text": "Brasília, DF\n\n\nCode\ncode_ref &lt;- 5300108\n\ntemp_list &lt;- list(\n  temp = temp %&gt;% filter(code_muni == code_ref),\n  temp_23 = temp_23 %&gt;% filter(code_muni == code_ref)\n)\n\nggplot() +\n  geom_line(\n    data = temp_list$temp, \n    aes(x = date, y = value, color = year, group = year),\n    alpha = .3, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n  scale_color_viridis(option = \"plasma\") +\n  geom_line(\n    data = temp_list$temp_23, \n    aes(x = date, y = value), \n    color = \"red\",\n    lwd = 1, alpha = .7, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n    geom_dl(\n    data = temp_list$temp_23, \n    aes(x = date, y = value, label = \"2023\"),\n    method = list(dl.combine(\"last.points\")), \n    color = \"red\",\n    cex = 0.8\n  ) +\n  ylim(15, 35) +\n  scale_x_date(date_breaks = \"1 month\", date_labels = \"%b\") +\n  dark_theme_gray() +\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\"\n  ) +\n  labs(\n    title = \"Brasília, DF\", \n    x = \"Date\",\n    y = \"Average temperature (°C)\", \n    color = NULL\n  )"
  },
  {
    "objectID": "posts/hot_2023.html#boa-vista-rr",
    "href": "posts/hot_2023.html#boa-vista-rr",
    "title": "How 2023 was hot in different Brazilian municipalities?",
    "section": "Boa Vista, RR",
    "text": "Boa Vista, RR\n\n\nCode\ncode_ref &lt;- 1400100\n\ntemp_list &lt;- list(\n  temp = temp %&gt;% filter(code_muni == code_ref),\n  temp_23 = temp_23 %&gt;% filter(code_muni == code_ref)\n)\n\nggplot() +\n  geom_line(\n    data = temp_list$temp, \n    aes(x = date, y = value, color = year, group = year),\n    alpha = .3, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n  scale_color_viridis(option = \"plasma\") +\n  geom_line(\n    data = temp_list$temp_23, \n    aes(x = date, y = value), \n    color = \"red\",\n    lwd = 1, alpha = .7, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n    geom_dl(\n    data = temp_list$temp_23, \n    aes(x = date, y = value, label = \"2023\"),\n    method = list(dl.combine(\"last.points\")), \n    color = \"red\",\n    cex = 0.8\n  ) +\n  ylim(15, 35) +\n  scale_x_date(date_breaks = \"1 month\", date_labels = \"%b\") +\n  dark_theme_gray() +\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\"\n  ) +\n  labs(\n    title = \"Boa Vista, RR\", \n    x = \"Date\",\n    y = \"Average temperature (°C)\", \n    color = NULL\n  )"
  },
  {
    "objectID": "posts/hot_2023.html#cuiabá-mt",
    "href": "posts/hot_2023.html#cuiabá-mt",
    "title": "How 2023 was hot in different Brazilian municipalities?",
    "section": "Cuiabá, MT",
    "text": "Cuiabá, MT\n\n\nCode\ncode_ref &lt;- 5103403\n\ntemp_list &lt;- list(\n  temp = temp %&gt;% filter(code_muni == code_ref),\n  temp_23 = temp_23 %&gt;% filter(code_muni == code_ref)\n)\n\nggplot() +\n  geom_line(\n    data = temp_list$temp, \n    aes(x = date, y = value, color = year, group = year),\n    alpha = .3, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n  scale_color_viridis(option = \"plasma\") +\n  geom_line(\n    data = temp_list$temp_23, \n    aes(x = date, y = value), \n    color = \"red\",\n    lwd = 1, alpha = .7, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n    geom_dl(\n    data = temp_list$temp_23, \n    aes(x = date, y = value, label = \"2023\"),\n    method = list(dl.combine(\"last.points\")), \n    color = \"red\",\n    cex = 0.8\n  ) +\n  ylim(15, 35) +\n  scale_x_date(date_breaks = \"1 month\", date_labels = \"%b\") +\n  dark_theme_gray() +\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\"\n  ) +\n  labs(\n    title = \"Cuiabá, MT\", \n    x = \"Date\",\n    y = \"Average temperature (°C)\", \n    color = NULL\n  )"
  },
  {
    "objectID": "posts/hot_2023.html#palmas-to",
    "href": "posts/hot_2023.html#palmas-to",
    "title": "How 2023 was hot in different Brazilian municipalities?",
    "section": "Palmas, TO",
    "text": "Palmas, TO\n\n\nCode\ncode_ref &lt;- 1721000\n\ntemp_list &lt;- list(\n  temp = temp %&gt;% filter(code_muni == code_ref),\n  temp_23 = temp_23 %&gt;% filter(code_muni == code_ref)\n)\n\nggplot() +\n  geom_line(\n    data = temp_list$temp, \n    aes(x = date, y = value, color = year, group = year),\n    alpha = .3, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n  scale_color_viridis(option = \"plasma\") +\n  geom_line(\n    data = temp_list$temp_23, \n    aes(x = date, y = value), \n    color = \"red\",\n    lwd = 1, alpha = .7, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n    geom_dl(\n    data = temp_list$temp_23, \n    aes(x = date, y = value, label = \"2023\"),\n    method = list(dl.combine(\"last.points\")), \n    color = \"red\",\n    cex = 0.8\n  ) +\n  ylim(15, 35) +\n  scale_x_date(date_breaks = \"1 month\", date_labels = \"%b\") +\n  dark_theme_gray() +\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\"\n  ) +\n  labs(\n    title = \"Palmas, TO\", \n    x = \"Date\",\n    y = \"Average temperature (°C)\", \n    color = NULL\n  )"
  },
  {
    "objectID": "posts/hot_2023.html#são-paulo-sp",
    "href": "posts/hot_2023.html#são-paulo-sp",
    "title": "How 2023 was hot in different Brazilian municipalities?",
    "section": "São Paulo, SP",
    "text": "São Paulo, SP\n\n\nCode\ncode_ref &lt;- 3550308\n\ntemp_list &lt;- list(\n  temp = temp %&gt;% filter(code_muni == code_ref),\n  temp_23 = temp_23 %&gt;% filter(code_muni == code_ref)\n)\n\nggplot() +\n  geom_line(\n    data = temp_list$temp, \n    aes(x = date, y = value, color = year, group = year),\n    alpha = .3, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n  scale_color_viridis(option = \"plasma\") +\n  geom_line(\n    data = temp_list$temp_23, \n    aes(x = date, y = value), \n    color = \"red\",\n    lwd = 1, alpha = .7, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n    geom_dl(\n    data = temp_list$temp_23, \n    aes(x = date, y = value, label = \"2023\"),\n    method = list(dl.combine(\"last.points\")), \n    color = \"red\",\n    cex = 0.8\n  ) +\n  ylim(15, 35) +\n  scale_x_date(date_breaks = \"1 month\", date_labels = \"%b\") +\n  dark_theme_gray() +\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\"\n  ) +\n  labs(\n    title = \"São Paulo, SP\", \n    x = \"Date\",\n    y = \"Average temperature (°C)\", \n    color = NULL\n  )"
  },
  {
    "objectID": "posts/hot_2023.html#teresina-pi",
    "href": "posts/hot_2023.html#teresina-pi",
    "title": "How 2023 was hot in different Brazilian municipalities?",
    "section": "Teresina, PI",
    "text": "Teresina, PI\n\n\nCode\ncode_ref &lt;- 2211001\n\ntemp_list &lt;- list(\n  temp = temp %&gt;% filter(code_muni == code_ref),\n  temp_23 = temp_23 %&gt;% filter(code_muni == code_ref)\n)\n\nggplot() +\n  geom_line(\n    data = temp_list$temp, \n    aes(x = date, y = value, color = year, group = year),\n    alpha = .3, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n  scale_color_viridis(option = \"plasma\") +\n  geom_line(\n    data = temp_list$temp_23, \n    aes(x = date, y = value), \n    color = \"red\",\n    lwd = 1, alpha = .7, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n    geom_dl(\n    data = temp_list$temp_23, \n    aes(x = date, y = value, label = \"2023\"),\n    method = list(dl.combine(\"last.points\")), \n    color = \"red\",\n    cex = 0.8\n  ) +\n  ylim(15, 35) +\n  scale_x_date(date_breaks = \"1 month\", date_labels = \"%b\") +\n  dark_theme_gray() +\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\"\n  ) +\n  labs(\n    title = \"Teresina, PI\", \n    x = \"Date\",\n    y = \"Average temperature (°C)\", \n    color = NULL\n  )"
  },
  {
    "objectID": "posts/hot_2023.html#rio-de-janeiro-rj",
    "href": "posts/hot_2023.html#rio-de-janeiro-rj",
    "title": "How 2023 was hot in different Brazilian municipalities?",
    "section": "Rio de Janeiro, RJ",
    "text": "Rio de Janeiro, RJ\n\n\nCode\ncode_ref &lt;- 3304557\n\ntemp_list &lt;- list(\n  temp = temp %&gt;% filter(code_muni == code_ref),\n  temp_23 = temp_23 %&gt;% filter(code_muni == code_ref)\n)\n\nggplot() +\n  geom_line(\n    data = temp_list$temp, \n    aes(x = date, y = value, color = year, group = year),\n    alpha = .3, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n  scale_color_viridis(option = \"plasma\") +\n  geom_line(\n    data = temp_list$temp_23, \n    aes(x = date, y = value), \n    color = \"red\",\n    lwd = 1, alpha = .7, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n    geom_dl(\n    data = temp_list$temp_23, \n    aes(x = date, y = value, label = \"2023\"),\n    method = list(dl.combine(\"last.points\")), \n    color = \"red\",\n    cex = 0.8\n  ) +\n  ylim(15, 35) +\n  scale_x_date(date_breaks = \"1 month\", date_labels = \"%b\") +\n  dark_theme_gray() +\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\"\n  ) +\n  labs(\n    title = \"Rio de Janeiro, RJ\", \n    x = \"Date\",\n    y = \"Average temperature (°C)\", \n    color = NULL\n  )"
  },
  {
    "objectID": "posts/hot_2023.html#belém-pa",
    "href": "posts/hot_2023.html#belém-pa",
    "title": "How 2023 was hot in different Brazilian municipalities?",
    "section": "Belém, PA",
    "text": "Belém, PA\n\n\nCode\ncode_ref &lt;- 1501402\n\ntemp_list &lt;- list(\n  temp = temp %&gt;% filter(code_muni == code_ref),\n  temp_23 = temp_23 %&gt;% filter(code_muni == code_ref)\n)\n\nggplot() +\n  geom_line(\n    data = temp_list$temp, \n    aes(x = date, y = value, color = year, group = year),\n    alpha = .3, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n  scale_color_viridis(option = \"plasma\") +\n  geom_line(\n    data = temp_list$temp_23, \n    aes(x = date, y = value), \n    color = \"red\",\n    lwd = 1, alpha = .7, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n    geom_dl(\n    data = temp_list$temp_23, \n    aes(x = date, y = value, label = \"2023\"),\n    method = list(dl.combine(\"last.points\")), \n    color = \"red\",\n    cex = 0.8\n  ) +\n  ylim(15, 35) +\n  scale_x_date(date_breaks = \"1 month\", date_labels = \"%b\") +\n  dark_theme_gray() +\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\"\n  ) +\n  labs(\n    title = \"Belém, PA\", \n    x = \"Date\",\n    y = \"Average temperature (°C)\", \n    color = NULL\n  )"
  },
  {
    "objectID": "posts/hot_2023.html#goiânia-go",
    "href": "posts/hot_2023.html#goiânia-go",
    "title": "How 2023 was hot in different Brazilian municipalities?",
    "section": "Goiânia, GO",
    "text": "Goiânia, GO\n\n\nCode\ncode_ref &lt;- 5208707\n\ntemp_list &lt;- list(\n  temp = temp %&gt;% filter(code_muni == code_ref),\n  temp_23 = temp_23 %&gt;% filter(code_muni == code_ref)\n)\n\nggplot() +\n  geom_line(\n    data = temp_list$temp, \n    aes(x = date, y = value, color = year, group = year),\n    alpha = .3, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n  scale_color_viridis(option = \"plasma\") +\n  geom_line(\n    data = temp_list$temp_23, \n    aes(x = date, y = value), \n    color = \"red\",\n    lwd = 1, alpha = .7, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n    geom_dl(\n    data = temp_list$temp_23, \n    aes(x = date, y = value, label = \"2023\"),\n    method = list(dl.combine(\"last.points\")), \n    color = \"red\",\n    cex = 0.8\n  ) +\n  ylim(15, 35) +\n  scale_x_date(date_breaks = \"1 month\", date_labels = \"%b\") +\n  dark_theme_gray() +\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\"\n  ) +\n  labs(\n    title = \"Goiânia, GO\", \n    x = \"Date\",\n    y = \"Average temperature (°C)\", \n    color = NULL\n  )"
  },
  {
    "objectID": "posts/hot_2023.html#salvador-ba",
    "href": "posts/hot_2023.html#salvador-ba",
    "title": "How 2023 was hot in different Brazilian municipalities?",
    "section": "Salvador, BA",
    "text": "Salvador, BA\n\n\nCode\ncode_ref &lt;- 2927408\n\ntemp_list &lt;- list(\n  temp = temp %&gt;% filter(code_muni == code_ref),\n  temp_23 = temp_23 %&gt;% filter(code_muni == code_ref)\n)\n\nggplot() +\n  geom_line(\n    data = temp_list$temp, \n    aes(x = date, y = value, color = year, group = year),\n    alpha = .3, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n  scale_color_viridis(option = \"plasma\") +\n  geom_line(\n    data = temp_list$temp_23, \n    aes(x = date, y = value), \n    color = \"red\",\n    lwd = 1, alpha = .7, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n    geom_dl(\n    data = temp_list$temp_23, \n    aes(x = date, y = value, label = \"2023\"),\n    method = list(dl.combine(\"last.points\")), \n    color = \"red\",\n    cex = 0.8\n  ) +\n  ylim(15, 35) +\n  scale_x_date(date_breaks = \"1 month\", date_labels = \"%b\") +\n  dark_theme_gray() +\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\"\n  ) +\n  labs(\n    title = \"Salvador, BA\", \n    x = \"Date\",\n    y = \"Average temperature (°C)\", \n    color = NULL\n  )"
  },
  {
    "objectID": "posts/hot_2023.html#florianópolis-sc",
    "href": "posts/hot_2023.html#florianópolis-sc",
    "title": "How 2023 was hot in different Brazilian municipalities?",
    "section": "Florianópolis, SC",
    "text": "Florianópolis, SC\n\n\nCode\ncode_ref &lt;- 4205407\n\ntemp_list &lt;- list(\n  temp = temp %&gt;% filter(code_muni == code_ref),\n  temp_23 = temp_23 %&gt;% filter(code_muni == code_ref)\n)\n\nggplot() +\n  geom_line(\n    data = temp_list$temp, \n    aes(x = date, y = value, color = year, group = year),\n    alpha = .3, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n  scale_color_viridis(option = \"plasma\") +\n  geom_line(\n    data = temp_list$temp_23, \n    aes(x = date, y = value), \n    color = \"red\",\n    lwd = 1, alpha = .7, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n    geom_dl(\n    data = temp_list$temp_23, \n    aes(x = date, y = value, label = \"2023\"),\n    method = list(dl.combine(\"last.points\")), \n    color = \"red\",\n    cex = 0.8\n  ) +\n  ylim(15, 35) +\n  scale_x_date(date_breaks = \"1 month\", date_labels = \"%b\") +\n  dark_theme_gray() +\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\"\n  ) +\n  labs(\n    title = \"Florianópolis, SC\", \n    x = \"Date\",\n    y = \"Average temperature (°C)\", \n    color = NULL\n  )"
  },
  {
    "objectID": "posts/hot_2023.html#são-luiz-ma",
    "href": "posts/hot_2023.html#são-luiz-ma",
    "title": "How 2023 was hot in different Brazilian municipalities?",
    "section": "São Luiz, MA",
    "text": "São Luiz, MA\n\n\nCode\ncode_ref &lt;- 2111300\n\ntemp_list &lt;- list(\n  temp = temp %&gt;% filter(code_muni == code_ref),\n  temp_23 = temp_23 %&gt;% filter(code_muni == code_ref)\n)\n\nggplot() +\n  geom_line(\n    data = temp_list$temp, \n    aes(x = date, y = value, color = year, group = year),\n    alpha = .3, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n  scale_color_viridis(option = \"plasma\") +\n  geom_line(\n    data = temp_list$temp_23, \n    aes(x = date, y = value), \n    color = \"red\",\n    lwd = 1, alpha = .7, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n    geom_dl(\n    data = temp_list$temp_23, \n    aes(x = date, y = value, label = \"2023\"),\n    method = list(dl.combine(\"last.points\")), \n    color = \"red\",\n    cex = 0.8\n  ) +\n  ylim(15, 35) +\n  scale_x_date(date_breaks = \"1 month\", date_labels = \"%b\") +\n  dark_theme_gray() +\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\"\n  ) +\n  labs(\n    title = \"São Luiz, MA\", \n    x = \"Date\",\n    y = \"Average temperature (°C)\", \n    color = NULL\n  )"
  },
  {
    "objectID": "posts/hot_2023.html#maceió-al",
    "href": "posts/hot_2023.html#maceió-al",
    "title": "How 2023 was hot in different Brazilian municipalities?",
    "section": "Maceió, AL",
    "text": "Maceió, AL\n\n\nCode\ncode_ref &lt;- 2704302\n\ntemp_list &lt;- list(\n  temp = temp %&gt;% filter(code_muni == code_ref),\n  temp_23 = temp_23 %&gt;% filter(code_muni == code_ref)\n)\n\nggplot() +\n  geom_line(\n    data = temp_list$temp, \n    aes(x = date, y = value, color = year, group = year),\n    alpha = .3, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n  scale_color_viridis(option = \"plasma\") +\n  geom_line(\n    data = temp_list$temp_23, \n    aes(x = date, y = value), \n    color = \"red\",\n    lwd = 1, alpha = .7, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n    geom_dl(\n    data = temp_list$temp_23, \n    aes(x = date, y = value, label = \"2023\"),\n    method = list(dl.combine(\"last.points\")), \n    color = \"red\",\n    cex = 0.8\n  ) +\n  ylim(15, 35) +\n  scale_x_date(date_breaks = \"1 month\", date_labels = \"%b\") +\n  dark_theme_gray() +\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\"\n  ) +\n  labs(\n    title = \"Maceió, AL\", \n    x = \"Date\",\n    y = \"Average temperature (°C)\", \n    color = NULL\n  )"
  },
  {
    "objectID": "posts/hot_2023.html#porto-alegre-rs",
    "href": "posts/hot_2023.html#porto-alegre-rs",
    "title": "How 2023 was hot in different Brazilian municipalities?",
    "section": "Porto Alegre, RS",
    "text": "Porto Alegre, RS\n\n\nCode\ncode_ref &lt;- 4314902\n\ntemp_list &lt;- list(\n  temp = temp %&gt;% filter(code_muni == code_ref),\n  temp_23 = temp_23 %&gt;% filter(code_muni == code_ref)\n)\n\nggplot() +\n  geom_line(\n    data = temp_list$temp, \n    aes(x = date, y = value, color = year, group = year),\n    alpha = .3, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n  scale_color_viridis(option = \"plasma\") +\n  geom_line(\n    data = temp_list$temp_23, \n    aes(x = date, y = value), \n    color = \"red\",\n    lwd = 1, alpha = .7, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n    geom_dl(\n    data = temp_list$temp_23, \n    aes(x = date, y = value, label = \"2023\"),\n    method = list(dl.combine(\"last.points\")), \n    color = \"red\",\n    cex = 0.8\n  ) +\n  ylim(15, 35) +\n  scale_x_date(date_breaks = \"1 month\", date_labels = \"%b\") +\n  dark_theme_gray() +\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\"\n  ) +\n  labs(\n    title = \"Porto Alegre, RS\", \n    x = \"Date\",\n    y = \"Average temperature (°C)\", \n    color = NULL\n  )"
  },
  {
    "objectID": "posts/hot_2023.html#curitiba-pr",
    "href": "posts/hot_2023.html#curitiba-pr",
    "title": "How 2023 was hot in different Brazilian municipalities?",
    "section": "Curitiba, PR",
    "text": "Curitiba, PR\n\n\nCode\ncode_ref &lt;- 4106902\n\ntemp_list &lt;- list(\n  temp = temp %&gt;% filter(code_muni == code_ref),\n  temp_23 = temp_23 %&gt;% filter(code_muni == code_ref)\n)\n\nggplot() +\n  geom_line(\n    data = temp_list$temp, \n    aes(x = date, y = value, color = year, group = year),\n    alpha = .3, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n  scale_color_viridis(option = \"plasma\") +\n  geom_line(\n    data = temp_list$temp_23, \n    aes(x = date, y = value), \n    color = \"red\",\n    lwd = 1, alpha = .7, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n    geom_dl(\n    data = temp_list$temp_23, \n    aes(x = date, y = value, label = \"2023\"),\n    method = list(dl.combine(\"last.points\")), \n    color = \"red\",\n    cex = 0.8\n  ) +\n  ylim(15, 35) +\n  scale_x_date(date_breaks = \"1 month\", date_labels = \"%b\") +\n  dark_theme_gray() +\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\"\n  ) +\n  labs(\n    title = \"Curitiba, PR\", \n    x = \"Date\",\n    y = \"Average temperature (°C)\", \n    color = NULL\n  )"
  },
  {
    "objectID": "posts/hot_2023.html#belo-horizonte-mg",
    "href": "posts/hot_2023.html#belo-horizonte-mg",
    "title": "How 2023 was hot in different Brazilian municipalities?",
    "section": "Belo Horizonte, MG",
    "text": "Belo Horizonte, MG\n\n\nCode\ncode_ref &lt;- 3106200\n\ntemp_list &lt;- list(\n  temp = temp %&gt;% filter(code_muni == code_ref),\n  temp_23 = temp_23 %&gt;% filter(code_muni == code_ref)\n)\n\nggplot() +\n  geom_line(\n    data = temp_list$temp, \n    aes(x = date, y = value, color = year, group = year),\n    alpha = .3, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n  scale_color_viridis(option = \"plasma\") +\n  geom_line(\n    data = temp_list$temp_23, \n    aes(x = date, y = value), \n    color = \"red\",\n    lwd = 1, alpha = .7, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n    geom_dl(\n    data = temp_list$temp_23, \n    aes(x = date, y = value, label = \"2023\"),\n    method = list(dl.combine(\"last.points\")), \n    color = \"red\",\n    cex = 0.8\n  ) +\n  ylim(15, 35) +\n  scale_x_date(date_breaks = \"1 month\", date_labels = \"%b\") +\n  dark_theme_gray() +\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\"\n  ) +\n  labs(\n    title = \"Belo Horizonte, MG\", \n    x = \"Date\",\n    y = \"Average temperature (°C)\", \n    color = NULL\n  )"
  },
  {
    "objectID": "posts/hot_2023.html#fortaleza-ce",
    "href": "posts/hot_2023.html#fortaleza-ce",
    "title": "How 2023 was hot in different Brazilian municipalities?",
    "section": "Fortaleza, CE",
    "text": "Fortaleza, CE\n\n\nCode\ncode_ref &lt;- 2304400\n\ntemp_list &lt;- list(\n  temp = temp %&gt;% filter(code_muni == code_ref),\n  temp_23 = temp_23 %&gt;% filter(code_muni == code_ref)\n)\n\nggplot() +\n  geom_line(\n    data = temp_list$temp, \n    aes(x = date, y = value, color = year, group = year),\n    alpha = .3, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n  scale_color_viridis(option = \"plasma\") +\n  geom_line(\n    data = temp_list$temp_23, \n    aes(x = date, y = value), \n    color = \"red\",\n    lwd = 1, alpha = .7, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n    geom_dl(\n    data = temp_list$temp_23, \n    aes(x = date, y = value, label = \"2023\"),\n    method = list(dl.combine(\"last.points\")), \n    color = \"red\",\n    cex = 0.8\n  ) +\n  ylim(15, 35) +\n  scale_x_date(date_breaks = \"1 month\", date_labels = \"%b\") +\n  dark_theme_gray() +\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\"\n  ) +\n  labs(\n    title = \"Fortaleza, CE\", \n    x = \"Date\",\n    y = \"Average temperature (°C)\", \n    color = NULL\n  )"
  },
  {
    "objectID": "posts/hot_2023.html#recife-pe",
    "href": "posts/hot_2023.html#recife-pe",
    "title": "How 2023 was hot in different Brazilian municipalities?",
    "section": "Recife, PE",
    "text": "Recife, PE\n\n\nCode\ncode_ref &lt;- 2611606\n\ntemp_list &lt;- list(\n  temp = temp %&gt;% filter(code_muni == code_ref),\n  temp_23 = temp_23 %&gt;% filter(code_muni == code_ref)\n)\n\nggplot() +\n  geom_line(\n    data = temp_list$temp, \n    aes(x = date, y = value, color = year, group = year),\n    alpha = .3, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n  scale_color_viridis(option = \"plasma\") +\n  geom_line(\n    data = temp_list$temp_23, \n    aes(x = date, y = value), \n    color = \"red\",\n    lwd = 1, alpha = .7, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n    geom_dl(\n    data = temp_list$temp_23, \n    aes(x = date, y = value, label = \"2023\"),\n    method = list(dl.combine(\"last.points\")), \n    color = \"red\",\n    cex = 0.8\n  ) +\n  ylim(15, 35) +\n  scale_x_date(date_breaks = \"1 month\", date_labels = \"%b\") +\n  dark_theme_gray() +\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\"\n  ) +\n  labs(\n    title = \"Recife, PE\", \n    x = \"Date\",\n    y = \"Average temperature (°C)\", \n    color = NULL\n  )"
  },
  {
    "objectID": "posts/hot_2023.html#joão-pessoa-pb",
    "href": "posts/hot_2023.html#joão-pessoa-pb",
    "title": "How 2023 was hot in different Brazilian municipalities?",
    "section": "João Pessoa, PB",
    "text": "João Pessoa, PB\n\n\nCode\ncode_ref &lt;- 2507507\n\ntemp_list &lt;- list(\n  temp = temp %&gt;% filter(code_muni == code_ref),\n  temp_23 = temp_23 %&gt;% filter(code_muni == code_ref)\n)\n\nggplot() +\n  geom_line(\n    data = temp_list$temp, \n    aes(x = date, y = value, color = year, group = year),\n    alpha = .3, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n  scale_color_viridis(option = \"plasma\") +\n  geom_line(\n    data = temp_list$temp_23, \n    aes(x = date, y = value), \n    color = \"red\",\n    lwd = 1, alpha = .7, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n    geom_dl(\n    data = temp_list$temp_23, \n    aes(x = date, y = value, label = \"2023\"),\n    method = list(dl.combine(\"last.points\")), \n    color = \"red\",\n    cex = 0.8\n  ) +\n  ylim(15, 35) +\n  scale_x_date(date_breaks = \"1 month\", date_labels = \"%b\") +\n  dark_theme_gray() +\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\"\n  ) +\n  labs(\n    title = \"João Pessoa, PB\", \n    x = \"Date\",\n    y = \"Average temperature (°C)\", \n    color = NULL\n  )"
  },
  {
    "objectID": "posts/hot_2023.html#aracaju-se",
    "href": "posts/hot_2023.html#aracaju-se",
    "title": "How 2023 was hot in different Brazilian municipalities?",
    "section": "Aracaju, SE",
    "text": "Aracaju, SE\n\n\nCode\ncode_ref &lt;- 2800308\n\ntemp_list &lt;- list(\n  temp = temp %&gt;% filter(code_muni == code_ref),\n  temp_23 = temp_23 %&gt;% filter(code_muni == code_ref)\n)\n\nggplot() +\n  geom_line(\n    data = temp_list$temp, \n    aes(x = date, y = value, color = year, group = year),\n    alpha = .3, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n  scale_color_viridis(option = \"plasma\") +\n  geom_line(\n    data = temp_list$temp_23, \n    aes(x = date, y = value), \n    color = \"red\",\n    lwd = 1, alpha = .7, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n    geom_dl(\n    data = temp_list$temp_23, \n    aes(x = date, y = value, label = \"2023\"),\n    method = list(dl.combine(\"last.points\")), \n    color = \"red\",\n    cex = 0.8\n  ) +\n  ylim(15, 35) +\n  scale_x_date(date_breaks = \"1 month\", date_labels = \"%b\") +\n  dark_theme_gray() +\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\"\n  ) +\n  labs(\n    title = \"Aracaju, SE\", \n    x = \"Date\",\n    y = \"Average temperature (°C)\", \n    color = NULL\n  )"
  },
  {
    "objectID": "posts/hot_2023.html#natal-rn",
    "href": "posts/hot_2023.html#natal-rn",
    "title": "How 2023 was hot in different Brazilian municipalities?",
    "section": "Natal, RN",
    "text": "Natal, RN\n\n\nCode\ncode_ref &lt;- 2408102\n\ntemp_list &lt;- list(\n  temp = temp %&gt;% filter(code_muni == code_ref),\n  temp_23 = temp_23 %&gt;% filter(code_muni == code_ref)\n)\n\nggplot() +\n  geom_line(\n    data = temp_list$temp, \n    aes(x = date, y = value, color = year, group = year),\n    alpha = .3, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n  scale_color_viridis(option = \"plasma\") +\n  geom_line(\n    data = temp_list$temp_23, \n    aes(x = date, y = value), \n    color = \"red\",\n    lwd = 1, alpha = .7, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n    geom_dl(\n    data = temp_list$temp_23, \n    aes(x = date, y = value, label = \"2023\"),\n    method = list(dl.combine(\"last.points\")), \n    color = \"red\",\n    cex = 0.8\n  ) +\n  ylim(15, 35) +\n  scale_x_date(date_breaks = \"1 month\", date_labels = \"%b\") +\n  dark_theme_gray() +\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\"\n  ) +\n  labs(\n    title = \"Natal, RN\", \n    x = \"Date\",\n    y = \"Average temperature (°C)\", \n    color = NULL\n  )"
  },
  {
    "objectID": "posts/hot_2023.html#vitória-es",
    "href": "posts/hot_2023.html#vitória-es",
    "title": "How 2023 was hot in different Brazilian municipalities?",
    "section": "Vitória, ES",
    "text": "Vitória, ES\n\n\nCode\ncode_ref &lt;- 3205309\n\ntemp_list &lt;- list(\n  temp = temp %&gt;% filter(code_muni == code_ref),\n  temp_23 = temp_23 %&gt;% filter(code_muni == code_ref)\n)\n\nggplot() +\n  geom_line(\n    data = temp_list$temp, \n    aes(x = date, y = value, color = year, group = year),\n    alpha = .3, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n  scale_color_viridis(option = \"plasma\") +\n  geom_line(\n    data = temp_list$temp_23, \n    aes(x = date, y = value), \n    color = \"red\",\n    lwd = 1, alpha = .7, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n    geom_dl(\n    data = temp_list$temp_23, \n    aes(x = date, y = value, label = \"2023\"),\n    method = list(dl.combine(\"last.points\")), \n    color = \"red\",\n    cex = 0.8\n  ) +\n  ylim(15, 35) +\n  scale_x_date(date_breaks = \"1 month\", date_labels = \"%b\") +\n  dark_theme_gray() +\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\"\n  ) +\n  labs(\n    title = \"Vitória, ES\", \n    x = \"Date\",\n    y = \"Average temperature (°C)\", \n    color = NULL\n  )"
  },
  {
    "objectID": "posts/hot_2023.html#nova-friburgo-rj",
    "href": "posts/hot_2023.html#nova-friburgo-rj",
    "title": "How 2023 was hot in different Brazilian municipalities?",
    "section": "Nova Friburgo, RJ",
    "text": "Nova Friburgo, RJ\n\n\nCode\ncode_ref &lt;- 3303401\n\ntemp_list &lt;- list(\n  temp = temp %&gt;% filter(code_muni == code_ref),\n  temp_23 = temp_23 %&gt;% filter(code_muni == code_ref)\n)\n\nggplot() +\n  geom_line(\n    data = temp_list$temp, \n    aes(x = date, y = value, color = year, group = year),\n    alpha = .3, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n  scale_color_viridis(option = \"plasma\") +\n  geom_line(\n    data = temp_list$temp_23, \n    aes(x = date, y = value), \n    color = \"red\",\n    lwd = 1, alpha = .7, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n    geom_dl(\n    data = temp_list$temp_23, \n    aes(x = date, y = value, label = \"2023\"),\n    method = list(dl.combine(\"last.points\")), \n    color = \"red\",\n    cex = 0.8\n  ) +\n  ylim(15, 35) +\n  scale_x_date(date_breaks = \"1 month\", date_labels = \"%b\") +\n  dark_theme_gray() +\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\"\n  ) +\n  labs(\n    title = \"Nova Friburgo, RJ\", \n    x = \"Date\",\n    y = \"Average temperature (°C)\", \n    color = NULL\n  )"
  },
  {
    "objectID": "posts/hot_2023.html#juiz-de-fora-mg",
    "href": "posts/hot_2023.html#juiz-de-fora-mg",
    "title": "How 2023 was hot in different Brazilian municipalities?",
    "section": "Juiz de Fora, MG",
    "text": "Juiz de Fora, MG\n\n\nCode\ncode_ref &lt;- 3136702\n\ntemp_list &lt;- list(\n  temp = temp %&gt;% filter(code_muni == code_ref),\n  temp_23 = temp_23 %&gt;% filter(code_muni == code_ref)\n)\n\nggplot() +\n  geom_line(\n    data = temp_list$temp, \n    aes(x = date, y = value, color = year, group = year),\n    alpha = .3, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n  scale_color_viridis(option = \"plasma\") +\n  geom_line(\n    data = temp_list$temp_23, \n    aes(x = date, y = value), \n    color = \"red\",\n    lwd = 1, alpha = .7, \n    stat = \"smooth\", \n    method = \"loess\"\n  ) +\n    geom_dl(\n    data = temp_list$temp_23, \n    aes(x = date, y = value, label = \"2023\"),\n    method = list(dl.combine(\"last.points\")), \n    color = \"red\",\n    cex = 0.8\n  ) +\n  ylim(15, 35) +\n  scale_x_date(date_breaks = \"1 month\", date_labels = \"%b\") +\n  dark_theme_gray() +\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\"\n  ) +\n  labs(\n    title = \"Juiz de Fora, MG\",\n    x = \"Date\",\n    y = \"Average temperature (°C)\", \n    color = NULL\n  )"
  },
  {
    "objectID": "posts/data_package_guide.html",
    "href": "posts/data_package_guide.html",
    "title": "Writing R packages with large datasets",
    "section": "",
    "text": "This post aims to be a guide for those writing R packages that will contain datasets, presenting some approaches and solutions for some pitfalls.\nR packages commonly contain some datasets. They are useful to provide functions examples of usage and also internally test the package functions."
  },
  {
    "objectID": "posts/data_package_guide.html#creating-package-data-files",
    "href": "posts/data_package_guide.html#creating-package-data-files",
    "title": "Writing R packages with large datasets",
    "section": "Creating package data files",
    "text": "Creating package data files\nFollowing the R Packages book’s recommendation, you may use\n\nusethis::use_data_raw()\n\nThis command will create a data-raw folder on your package and an R script. The idea is to put all steps necessary to create the data there. But this folder is not used to build your package. For this reason, at the end of this script, you will find this command\n\nusethis::use_data()\n\nThe use_data() command will write the final data to the official data folder of the package."
  },
  {
    "objectID": "posts/data_package_guide.html#data-size-restrictions",
    "href": "posts/data_package_guide.html#data-size-restrictions",
    "title": "Writing R packages with large datasets",
    "section": "Data size restrictions",
    "text": "Data size restrictions\nConsidering CRAN guidelines, a package should be small. Remember that a package, once on CRAN, will be compiled to some operational systems and all package versions are archived. Thus, the package sources, binaries and its versions will occupy more space than you think. For this reason, it is asked to have less than 5MB of data. If you need more, you will need to ask for exception and present a good justification.\nYou can try different compression methods with the use_data() command to get smaller files. On my experience, xz compressions gives you smaller files with data frames.\n\nusethis::use_data(..., compress = \"gzip\") # The default\nusethis::use_data(..., compress = \"bzip2\")\nusethis::use_data(..., compress = \"xz\")"
  },
  {
    "objectID": "posts/data_package_guide.html#document-your-data",
    "href": "posts/data_package_guide.html#document-your-data",
    "title": "Writing R packages with large datasets",
    "section": "Document your data",
    "text": "Document your data\nRemember to create an R file at the R folder of your package with the documentation of your dataset.\n\nusethis::use_r(\"mydata\")\n\nAnd inside it, describe your data. An example:\n\n#' My data name\n#'\n#' Some description.\n#'\n#' \\describe{\n#'   \\item{variable_1}{description}\n#'   \\item{variable_2}{description}\n#'   \\item{variable_3}{description}\n#' }\n\"mydata\""
  },
  {
    "objectID": "posts/data_package_guide.html#separate-packages",
    "href": "posts/data_package_guide.html#separate-packages",
    "title": "Writing R packages with large datasets",
    "section": "Separate packages",
    "text": "Separate packages\nTo reduce the load of versioning packages with data, some package authors create two packages: a main package with functions, and a data package with only the data.\nThe main package will import the data package. The vantage here is that you can upgrade your package without creating useless versions of your data."
  },
  {
    "objectID": "posts/data_package_guide.html#solutions-for-large-datasets",
    "href": "posts/data_package_guide.html#solutions-for-large-datasets",
    "title": "Writing R packages with large datasets",
    "section": "Solutions for large datasets",
    "text": "Solutions for large datasets\nUntil now, those are the standard procedures to include small datasets with your package. But what to do when you need large files, occupying more than 5MB?\n\nThe piggyback solution\nThere is a nice package called piggyback. It provides functions to store files as GitHub releases. As Git and GitHub are not ideal to version data, it uses the GitHub releases feature to host files.\nWith this package, you can store your large datasets as a GitHub release and download them on your package.\n\n\nThe pins solution\nThe package pins present very interesting ways to publish data, models and R objects on “boards”. You can store your data on Azure, Google Drive, Google Cloud Storage, Amazon’s S3 and others.\nSomething very useful on pins is its capacity to cache files. This means that once the file was downloaded, the user will not need to download it again, as it will be cached on the computer.\nThe package has a nice article about web-hosted boards, including a suggestion to use the a pkgdown asset to store your data. If you store your pkgdown on GitHub, remember the file size limits (usually ok for less than 20MB).\n\n\nMy solution: zendown\nZenodo is a scientific data repository maintained by CERN. It is very stable and easy to use. You create an account, make an upload and instantly your dataset is available to other people to download. Also, your data gets a nice DOI code for citation\nI created a package called zendown, with functions to access data stored at Zenodo. The package will download and cache the requested data, avoiding unnecessary downloads.\nIts usage is very straighfoward. You just need the Zenodo deposit code (the number on the end of the URL of your repository) and the desired file name.\n\n# https://zenodo.org/records/10959197\nmy_iris &lt;- zendown::zen_file(deposit_id = 10959197, file_name = \"iris.rds\") |&gt;\n  readRDS()\n\nhead(my_iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa"
  },
  {
    "objectID": "posts/data_package_guide.html#using-zendown-in-a-package",
    "href": "posts/data_package_guide.html#using-zendown-in-a-package",
    "title": "Writing R packages with large datasets",
    "section": "Using zendown in a package",
    "text": "Using zendown in a package\nAfter importing zendown in your package (usethis::use_package(\"zendown\")), create an R file just like the documentation example, but it will contain a function.\n\n#' My data name\n#'\n#' Some description.\n#'\n#' \\describe{\n#'   \\item{variable_1}{description}\n#'   \\item{variable_2}{description}\n#'   \\item{variable_3}{description}\n#' }\nmydata &lt;- function(){\n  path &lt;- zendown::zen_file(10959197,  \"iris.rds\")\n  readRDS(file = path)\n}\n\nModify the deposit ID and file name and adapt the read function accordingly to your data. It can be a CSV, parquet file, etc.\nThen, on your package functions, call your data function.\n\n#' My Fuction\n#'\n#' Some description.\n#'\n#' @param x\nmyfunction &lt;- function(x){\n  df &lt;- mydata()\n  df$Sepal.Length * x\n  \n  return(df)\n}\n\nAs zendown cache the files, the user will download the data just once, and it will be always available to the package."
  },
  {
    "objectID": "posts/data_package_guide.html#the-internet-caveat",
    "href": "posts/data_package_guide.html#the-internet-caveat",
    "title": "Writing R packages with large datasets",
    "section": "The Internet caveat",
    "text": "The Internet caveat\nAll of those solutions depends on an Internet connection, and now your package will rely on the Internet to work. Remember to use functions to verify if Internet is available.\n\ncurl::has_internet()\n\nAnd to check if the host is online.\n\nRCurl::url.exists(url = \"...\")"
  },
  {
    "objectID": "posts/data_package_guide.html#package-tests",
    "href": "posts/data_package_guide.html#package-tests",
    "title": "Writing R packages with large datasets",
    "section": "Package tests",
    "text": "Package tests\nAs your package relies on Internet now, it can be important to skip some tests on CRAN, as the host may be offline or some connection problem can occur. Include testthat::skip_on_cran() on the tests that will require online data."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Raphael Saldanha",
    "section": "",
    "text": "Research assistant at Fiocruz,  Inria alumni.\nAssociate editor of RECIIS.\nPasteur Network Climate & Health fellow.\nHealth geographer and health data scientist. Undergraduate in Geography, specialization in Statistics, master’s in Public Health, and Ph.D. in Health Information.\nBrazilian from Nova Friburgo (RJ), lived for several years on Minas Gerais, Rio de Janeiro and Montpellier, France. Currently based on Rio de Janeiro.\nLoves to code and crunch huge health and climate datasets with R.\n\n\n Back to top"
  },
  {
    "objectID": "data-projects/usa_road_distances.html",
    "href": "data-projects/usa_road_distances.html",
    "title": "Road distances and trip duration matrix for USA counties",
    "section": "",
    "text": "Distance information between places is useful to evaluate the proximity and interconnection of regions. The Euclidean distance between two places, although simple and easy to compute, is not realistic in terms of dislocation costs. This dataset present the road distance and trip duration metrics between all USA counties."
  },
  {
    "objectID": "data-projects/usa_road_distances.html#introduction",
    "href": "data-projects/usa_road_distances.html#introduction",
    "title": "Road distances and trip duration matrix for USA counties",
    "section": "",
    "text": "Distance information between places is useful to evaluate the proximity and interconnection of regions. The Euclidean distance between two places, although simple and easy to compute, is not realistic in terms of dislocation costs. This dataset present the road distance and trip duration metrics between all USA counties."
  },
  {
    "objectID": "data-projects/usa_road_distances.html#methods",
    "href": "data-projects/usa_road_distances.html#methods",
    "title": "Road distances and trip duration matrix for USA counties",
    "section": "Methods",
    "text": "Methods\nI used the county database from Simple Maps (basic 2024 version), with 3,144 entries.\nA list of pairs of counties is computed using simple combinatorial analysis. Example:\n\nx &lt;- c(\"a\", \"b\", \"c\", \"d\")\n\ncombn(x, m = 2)\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,] \"a\"  \"a\"  \"a\"  \"b\"  \"b\"  \"c\" \n[2,] \"b\"  \"c\"  \"d\"  \"c\"  \"d\"  \"d\" \n\n\n\n\n\n\n\n\nNote\n\n\n\nWith this, I assume that the distance between two counties is the same, independently of the direction.\n\n\nThus, for the 3,144 counties, we will have a total number of routes to be computed:\n\nchoose(n = 3144, k = 2)\n\n[1] 4940796\n\n\nTo compute the road distance between pairs of counties, the OSRM API service was used, with the {osrm} package (Giraud 2022). More specifically, the table service was used, considering the “car” profile, returning as result the distance in meters and estimated trip duration in minutes for the fastest route found.\n\nRoute example\nFor example, lets compute the road distance between New York County, NY and Los Angeles Counties, CA with the {osrm} package.\n\nlibrary(osrm)\nlibrary(leaflet)\nlibrary(sf)\n\nnew_york &lt;- c(-73.9668, 40.7792)\nlos_angeles &lt;- c(-118.2247, 34.3219)\n\nroute &lt;- osrmRoute(\n  src = new_york, dst = los_angeles, \n  overview = \"full\",\n  osrm.profile = \"car\"\n)\n\n\n# Route distance, in meters\nroute$distance\n\n[1] 4516.204\n\n\n\n# Route duration, in minutes\nroute$duration\n\n[1] 2999.772\n\n\n\nroute |&gt;\n  st_transform(4326) |&gt;\n  leaflet() |&gt;\n  addTiles() |&gt;\n  addPolylines()\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nFor some pairs of municipalities, the OSRM service is not able to determine a possible road route. This is expected, as some municipalities are not reachable by road.\n\n\nThe scripts used to prepare the dataset are available here."
  },
  {
    "objectID": "data-projects/usa_road_distances.html#dataset-download",
    "href": "data-projects/usa_road_distances.html#dataset-download",
    "title": "Road distances and trip duration matrix for USA counties",
    "section": "Dataset download",
    "text": "Dataset download\nThe dataset with the road distances and trip duration are available on Zenodo, on RDS format, parquet format, and CSV format.\nClick the link below to access and download the data.\n\nYou can also download the dataset directly from R, using the {zendown} package.\n\n# install.packages(\"zendown\")\nlibrary(zendown)\n\ndist_usa_file &lt;- zen_file(13906981, \"dist_usa.rds\")\n\ndist_usa &lt;- readRDS(dist_usa_file)\n\nhead(dist_usa)\n\n   orig  dest    dist    dur\n1 06037 01073 3270056 2124.1\n2 06037 01089 3251589 2136.9\n3 06037 01097 3282597 2143.5\n4 06037 04013  600220  412.8\n5 06037 04019  823167  627.0\n6 06037 04021  745854  524.8\n\n\n\nGraphs\n\nlibrary(ggplot2)\n\nggplot(data = dist_usa, aes(x = dist/1000)) +\n  geom_histogram(bins = 100) +\n  labs(\n    title = \"Fastest route road distance between USA counties\", \n    x = \"Distance (km)\", y = \"count\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nggplot(data = dist_usa, aes(x = dur/60)) +\n  geom_histogram(bins = 100) +\n  labs(\n    title = \"Fastest route estimated trip duration between USA counties\", \n    x = \"Trip duration (hours)\", y = \"count\"\n  ) +\n  theme_bw()"
  },
  {
    "objectID": "data-projects/usa_road_distances.html#future-plans",
    "href": "data-projects/usa_road_distances.html#future-plans",
    "title": "Road distances and trip duration matrix for USA counties",
    "section": "Future plans",
    "text": "Future plans\n\nCompute routes using other available routing services.\nYearly updates of the dataset, as the road infrastructure may change."
  },
  {
    "objectID": "data-projects/s3-datasus-ftp-mirror.html",
    "href": "data-projects/s3-datasus-ftp-mirror.html",
    "title": "S3 DataSUS FTP mirror",
    "section": "",
    "text": "The Department of Informatics (DataSUS) of the Brazilian Health Ministry hosts microdata anonymized files from several health information systems, covering themes such as mortality, newborns, hospitalization, and transmissible diseases.\nThose files are hosted in a public FTP server, but its access is geographically restricted to Brazil.\nIn order to offer an alternative way to access those files, I created a partial mirror of the FTP server in a S3 object storage architecture. This structure allows worldwide access, CDN distribution of files, and a redudancy in case of failure of the DataSUS FTP server."
  },
  {
    "objectID": "data-projects/s3-datasus-ftp-mirror.html#available-health-information-systems-and-files",
    "href": "data-projects/s3-datasus-ftp-mirror.html#available-health-information-systems-and-files",
    "title": "S3 DataSUS FTP mirror",
    "section": "Available health information systems and files",
    "text": "Available health information systems and files\nCurrently, the following health information systems are mirrored:\n\nSIM – Sistema de Informações de Mortalidade\nSINASC – Sistema de Informações de Nascidos Vivos\nSINAN – Sistema de Informações de Agravos de Notificação\nSIH – Sistema de Informações Hospitalares do SUS\nSIA – Sistema de Informações Ambulatoriais do SUS\nCNES – Cadastro Nacional de Estalecimentos de Saúde"
  },
  {
    "objectID": "data-projects/s3-datasus-ftp-mirror.html#file-access",
    "href": "data-projects/s3-datasus-ftp-mirror.html#file-access",
    "title": "S3 DataSUS FTP mirror",
    "section": "File Access",
    "text": "File Access\nThe S3 mirror is available at this endpoint:\nhttps://datasus-ftp-mirror.nyc3.cdn.digitaloceanspaces.com\n\n\n\n\n\n\nNote\n\n\n\n\nThe file structure at the S3 mirror follows the same directory structure of the FTP server.\nAll files available at the FTP are mirrored, except expanded XML and CSV files."
  },
  {
    "objectID": "data-projects/s3-datasus-ftp-mirror.html#mirror-update-and-files-tree",
    "href": "data-projects/s3-datasus-ftp-mirror.html#mirror-update-and-files-tree",
    "title": "S3 DataSUS FTP mirror",
    "section": "Mirror update and files tree",
    "text": "Mirror update and files tree\nThe mirror is synced daily at 3:00 am Brazilian time. On each update, some file lists are produced:\n\nDirectory tree\nFiles tree\nFull path list\n\n\n\n\n\n\n\nTip\n\n\n\nThe S3 bucket versioning option was enabled on November 7, 2024. Since then, the version history of all files (including deleted files) has been kept."
  },
  {
    "objectID": "data-projects/s3-datasus-ftp-mirror.html#how-to-access-a-file",
    "href": "data-projects/s3-datasus-ftp-mirror.html#how-to-access-a-file",
    "title": "S3 DataSUS FTP mirror",
    "section": "How to access a file?",
    "text": "How to access a file?\nCheck the desired file name at full path list and append it to the endpoint access.\nExample: SIM file for Bahia, 2022\nhttps://datasus-ftp-mirror.nyc3.cdn.digitaloceanspaces.com/SIM/CID10/DORES/DOBA2022.dbc"
  },
  {
    "objectID": "data-projects/s3-datasus-ftp-mirror.html#update-logs",
    "href": "data-projects/s3-datasus-ftp-mirror.html#update-logs",
    "title": "S3 DataSUS FTP mirror",
    "section": "Update logs",
    "text": "Update logs\nA log of the last update is provided and all update logs are stored in the folder rclone-logs\n\nLast update log\nAll logs on rclone-logs folder\n\n\nHow to access an old update log?\nFist, locate the log file name here, then access the file. Example:\nhttps://datasus-ftp-mirror.nyc3.cdn.digitaloceanspaces.com/rclone-logs/rclone_datasus_log_2024-11-07_03:04:55.txt"
  },
  {
    "objectID": "data-projects/s3-datasus-ftp-mirror.html#cdn",
    "href": "data-projects/s3-datasus-ftp-mirror.html#cdn",
    "title": "S3 DataSUS FTP mirror",
    "section": "CDN",
    "text": "CDN\nThe files are cached in a CDN (content delivery network) to increase transfer speeds. This cache is refreshed every hour. To access directly the file, without the CDN, remove the cdn from the address. Example:\nhttps://datasus-ftp-mirror.nyc3.digitaloceanspaces.com/SIM/CID10/DORES/DOBA2022.dbc"
  },
  {
    "objectID": "data-projects/s3-datasus-ftp-mirror.html#costs",
    "href": "data-projects/s3-datasus-ftp-mirror.html#costs",
    "title": "S3 DataSUS FTP mirror",
    "section": "Costs",
    "text": "Costs\nThis S3 mirror is available for free use, but I have running costs for storage and transfer volume at Digital Ocean. Please use it carefully and consciously."
  },
  {
    "objectID": "data-projects/s3-datasus-ftp-mirror.html#script",
    "href": "data-projects/s3-datasus-ftp-mirror.html#script",
    "title": "S3 DataSUS FTP mirror",
    "section": "Script",
    "text": "Script\nIf you are curious to see how this works, check this code repository."
  },
  {
    "objectID": "data-projects/health_indicators/indi_0001.html",
    "href": "data-projects/health_indicators/indi_0001.html",
    "title": "Taxa de mortalidade específica por causas externas",
    "section": "",
    "text": "Número de óbitos por causas externas (acidentes e violência), por 100 mil habitantes, na população residente em determinado espaço geográfico, no ano considerado (RIPSA 2008).\n\n\n\nDados de mortalidade: Sistema de Informação de Mortalidade (SIM), indexados na PCDaS\nEstimativas populacionais: DataSUS"
  },
  {
    "objectID": "data-projects/health_indicators/indi_0001.html#conceituação",
    "href": "data-projects/health_indicators/indi_0001.html#conceituação",
    "title": "Taxa de mortalidade específica por causas externas",
    "section": "",
    "text": "Número de óbitos por causas externas (acidentes e violência), por 100 mil habitantes, na população residente em determinado espaço geográfico, no ano considerado (RIPSA 2008).\n\n\n\nDados de mortalidade: Sistema de Informação de Mortalidade (SIM), indexados na PCDaS\nEstimativas populacionais: DataSUS"
  },
  {
    "objectID": "data-projects/health_indicators/indi_0001.html#agregações-computadas",
    "href": "data-projects/health_indicators/indi_0001.html#agregações-computadas",
    "title": "Taxa de mortalidade específica por causas externas",
    "section": "Agregações computadas",
    "text": "Agregações computadas\n\nUF de residência, ano, mês e semana.\nRegião de saúde de residência, ano, mês e semana.\nMunicípio de residência, ano, mês e semana.\nAnos de 2010 a 2021"
  },
  {
    "objectID": "data-projects/health_indicators/indi_0001.html#download",
    "href": "data-projects/health_indicators/indi_0001.html#download",
    "title": "Taxa de mortalidade específica por causas externas",
    "section": "Download",
    "text": "Download"
  },
  {
    "objectID": "data-projects/health_indicators/indi_0001.html#gráfico",
    "href": "data-projects/health_indicators/indi_0001.html#gráfico",
    "title": "Taxa de mortalidade específica por causas externas",
    "section": "Gráfico",
    "text": "Gráfico\n\n\nCódigo\nlibrary(tidyverse)\nlibrary(arrow)\n\nnames_helper &lt;- tibble(\n  uf_code = c(\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"31\",\"32\",\"33\",\"35\",\"41\",\"42\",\"43\",\"50\",\"51\",\"52\",\"53\"),\n  uf_name = c(\"Rondônia\",\"Acre\",\"Amazonas\",\"Roraima\",\"Pará\",\"Amapá\",\"Tocantins\",\"Maranhão\",\"Piauí\",\"Ceará\",\"Rio Grande do Norte\",\"Paraíba\",\"Pernambuco\",\"Alagoas\",\"Sergipe\",\"Bahia\",\"Minas Gerais\",\"Espírito Santo\",\"Rio de Janeiro\",\"São Paulo\",\"Paraná\",\"Santa Catarina\",\"Rio Grande do Sul\",\"Mato Grosso do Sul\",\"Mato Grosso\",\"Goiás\",\"Distrito Federal\"),\n  cap_code7 = c(1100205,1200401,1302603,1400100,1501402,1600303,1721000,2111300,2211001,2304400,2408102,2507507,2611606,2704302,2800308,2927408,3106200,3205309,3304557,3550308,4106902,4205407,4314902,5002704,5103403,5208707,5300108),\n  cap_code6 = c(110020,120040,130260,140010,150140,160030,172100,211130,221100,230440,240810,250750,261160,270430,280030,292740,310620,320530,330455,355030,410690,420540,431490,500270,510340,520870,530010),\n  cap_name = c(\"Porto Velho\",\"Rio Branco\",\"Manaus\",\"Boa Vista\",\"Belém\",\"Macapá\",\"Palmas\",\"São Luís\",\"Teresina\",\"Fortaleza\",\"Natal\",\"João Pessoa\",\"Recife\",\"Maceió\",\"Aracaju\",\"Salvador\",\"Belo Horizonte\",\"Vitória\",\"Rio de Janeiro\",\"São Paulo\",\"Curitiba\",\"Florianópolis\",\"Porto Alegre\",\"Campo Grande\",\"Cuiabá\",\"Goiânia\",\"Brasília\")\n)\n\nread_parquet(\"health_indicators_data/indi_0001_uf_res_year.parquet\") |&gt;\n  filter(cod != 5) |&gt;\n  mutate(cod = as.character(cod)) |&gt;\n  left_join(names_helper, by = c(\"cod\" = \"uf_code\")) |&gt;\n  ggplot(aes(x = date, y = value, group = uf_name, color = uf_name)) + \n  geom_line() +\n  facet_wrap(~uf_name, ncol = 4) +\n  theme_bw() +\n  theme(legend.position = \"none\") +\n  labs(\n    title = \"Taxa de mortalidade específica por causas externas\",\n    x = \"Ano\", y = \"Taxa\"\n  )"
  },
  {
    "objectID": "data-projects/health_indicators/indi_0001.html#computação",
    "href": "data-projects/health_indicators/indi_0001.html#computação",
    "title": "Taxa de mortalidade específica por causas externas",
    "section": "Computação",
    "text": "Computação\n\n\nCódigo\n# UF of residence and year\nbrindi::expand_indi_parquet(\n  agg = \"uf_res\", agg_time = \"year\", \n  anos = years_range, \n  dir = \"health_indicators_data/\", \n  indi = \"indi_0001\"\n)\nbrindi::expand_indi_parquet(\n  agg = \"uf_res\", agg_time = \"month\", \n  anos = years_range, \n  dir = \"health_indicators_data/\", \n  indi = \"indi_0001\"\n)\nbrindi::expand_indi_parquet(\n  agg = \"uf_res\", agg_time = \"week\", \n  anos = years_range, \n  dir = \"health_indicators_data/\", \n  indi = \"indi_0001\"\n)\n\n# Health region of residence and year\nbrindi::expand_indi_parquet(\n  agg = \"regsaude_res\", agg_time = \"year\", \n  anos = years_range, \n  dir = \"health_indicators_data/\", \n  indi = \"indi_0001\"\n)\nbrindi::expand_indi_parquet(\n  agg = \"regsaude_res\", agg_time = \"month\", \n  anos = years_range, \n  dir = \"health_indicators_data/\", \n  indi = \"indi_0001\"\n)\nbrindi::expand_indi_parquet(\n  agg = \"regsaude_res\", agg_time = \"week\", \n  anos = years_range, \n  dir = \"health_indicators_data/\", \n  indi = \"indi_0001\"\n)\n\n# Municipality of residence and year\nbrindi::expand_indi_parquet(\n  agg = \"mun_res\", agg_time = \"year\", \n  anos = years_range, \n  dir = \"health_indicators_data/\", \n  indi = \"indi_0001\"\n)\nbrindi::expand_indi_parquet(\n  agg = \"mun_res\", agg_time = \"month\", \n  anos = years_range, \n  dir = \"health_indicators_data/\", \n  indi = \"indi_0001\"\n)\nbrindi::expand_indi_parquet(\n  agg = \"mun_res\", agg_time = \"week\", \n  anos = years_range, \n  dir = \"health_indicators_data/\", \n  indi = \"indi_0001\"\n)"
  },
  {
    "objectID": "data-projects/era5land-daily-latin-america.html#introduction",
    "href": "data-projects/era5land-daily-latin-america.html#introduction",
    "title": "ERA5-Land selected indicators daily aggregates for Latin America",
    "section": "Introduction",
    "text": "Introduction\nThe ERA5-Land reanalysis from The Copernicus Programme is an incredible source of climate data with global coverage of land areas from 1950 to the present, at 10km spatial resolution. Its original data is at hourly interval, and monthly aggregates are also available at the Copernicus Data Store (CDS).\nFor some applications like Climate-Sensitive Diseases (CSD) modelling, the hourly interval may be too much detailed, but the monthly aggregation is too coarse.\nFor this reason, I created daily aggregates from some ERA5-Land indicators for some regions."
  },
  {
    "objectID": "data-projects/era5land-daily-latin-america.html#methodology",
    "href": "data-projects/era5land-daily-latin-america.html#methodology",
    "title": "ERA5-Land selected indicators daily aggregates for Latin America",
    "section": "Methodology",
    "text": "Methodology\nI developed an R script using the KrigR package (Kusch and Davy 2022). The script downloads a set of indicators, starting on 1950, for a geographical bounding box covering the Latin America region (coordinates -118.47,-34.1,-56.65, 33.28) and aggregates the data from hourly to daily, saving its results as NetCDF files. Each resulting file covers a year’s month and presents data layers for each day of the respective month.\nThe table bellow contains the time aggregation functions applied to each climate indicator.\n\n\n\nIndicator\nDaily aggregation function\n\n\n\n\n2m temperature\nmean, max, min\n\n\n2m dewpoint temperature\nmean\n\n\nu component of wind\nmean\n\n\nv component of wind\nmean\n\n\nsurface pressure\nmean\n\n\ntotal precipitation\nsum"
  },
  {
    "objectID": "data-projects/era5land-daily-latin-america.html#datasets",
    "href": "data-projects/era5land-daily-latin-america.html#datasets",
    "title": "ERA5-Land selected indicators daily aggregates for Latin America",
    "section": "Datasets",
    "text": "Datasets\nThe resulting NetCDF files are available for download at Zenodo.\n\n\n\nYear\nZenodo deposit\n\n\n\n\n1950\n\n\n\n1951\n\n\n\n1952\n\n\n\n1953\n\n\n\n1954\n\n\n\n1955\n\n\n\n1956\n\n\n\n1957\n\n\n\n1958\n\n\n\n1959\n\n\n\n1960\n\n\n\n1961\n\n\n\n1962\n\n\n\n1963\n\n\n\n1964\n\n\n\n1965\n\n\n\n1966\n\n\n\n1967\n\n\n\n1968\n\n\n\n1969\n\n\n\n1970\n\n\n\n1971\n\n\n\n1972\n\n\n\n1973\n\n\n\n1974\n\n\n\n1975\n\n\n\n1976\n\n\n\n1977\n\n\n\n1978\n\n\n\n1979\n\n\n\n1980\n\n\n\n1981\n\n\n\n1982\n\n\n\n1983\n\n\n\n1984\n\n\n\n1985\n\n\n\n1986\n\n\n\n1987\n\n\n\n1988\n\n\n\n1989\n\n\n\n1990\n\n\n\n1991\n\n\n\n1992\n\n\n\n1993\n\n\n\n1994\n\n\n\n1995\n\n\n\n1996\n\n\n\n1997\n\n\n\n1998\n\n\n\n1999\n\n\n\n2000\n\n\n\n2001\n\n\n\n2002\n\n\n\n2003\n\n\n\n2004\n\n\n\n2005\n\n\n\n2006\n\n\n\n2007\n\n\n\n2008\n\n\n\n2009\n\n\n\n2010\n\n\n\n2011\n\n\n\n2012\n\n\n\n2013\n\n\n\n2014\n\n\n\n2015\n\n\n\n2016\n\n\n\n2017\n\n\n\n2018\n\n\n\n2019\n\n\n\n2020\n\n\n\n2021\n\n\n\n2022\n\n\n\n2023\n\n\n\n\n\nUsage statistics\nUsage statistics of this and other datasets are available here."
  },
  {
    "objectID": "data-projects/brazil_road_distances.html",
    "href": "data-projects/brazil_road_distances.html",
    "title": "Road distances and trip duration matrix for Brazilian municipalities",
    "section": "",
    "text": "Distance information between places is useful to evaluate the proximity and interconnection of regions. The Euclidean distance between two places, although simple and easy to compute, is not realistic in terms of dislocation costs. This dataset present the road distance and trip duration metrics between all Brazilian municipalities."
  },
  {
    "objectID": "data-projects/brazil_road_distances.html#introduction",
    "href": "data-projects/brazil_road_distances.html#introduction",
    "title": "Road distances and trip duration matrix for Brazilian municipalities",
    "section": "",
    "text": "Distance information between places is useful to evaluate the proximity and interconnection of regions. The Euclidean distance between two places, although simple and easy to compute, is not realistic in terms of dislocation costs. This dataset present the road distance and trip duration metrics between all Brazilian municipalities."
  },
  {
    "objectID": "data-projects/brazil_road_distances.html#methods",
    "href": "data-projects/brazil_road_distances.html#methods",
    "title": "Road distances and trip duration matrix for Brazilian municipalities",
    "section": "Methods",
    "text": "Methods\nAs reference to starting and ending point of the routes, I opted to use the municipality seat localization (“sede dos municípios”), which is usually the city hall or historic downtown localization. This data is originally generated by the Brazilian Institute of Geography and Statistics (IBGE) and available on the {geobr} package. The last available data is for the year of 20101 and present 5,565 features.\nWith the list of municipalities and respective seat geographic coordinates, a list of pairs of municipalities is computed using simple combinatorial analysis. Example:\n\nx &lt;- c(\"a\", \"b\", \"c\", \"d\")\n\ncombn(x, m = 2)\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,] \"a\"  \"a\"  \"a\"  \"b\"  \"b\"  \"c\" \n[2,] \"b\"  \"c\"  \"d\"  \"c\"  \"d\"  \"d\" \n\n\n\n\n\n\n\n\nNote\n\n\n\nWith this, I assume that the distance between two municipalities is the same, independently of the direction.\n\n\nThus, for the 5,565 municipalities, we will have a total number of routes to be computed:\n\nchoose(n = 5565, k = 2)\n\n[1] 15481830\n\n\nTo compute the road distance between pairs of municipalities, the OSRM API service was used, with the {osrm} package (Giraud 2022). More specifically, the table service was used, considering the “car” profile, returning as result the distance in meters and estimated trip duration in minutes for the fastest route found.\n\nRoute example\nFor example, lets compute the road distance between Rio de Janeiro, RJ and São Paulo, SP with the {osrm} package.\n\nlibrary(geobr)\nlibrary(osrm)\nlibrary(leaflet)\nlibrary(sf)\n\nseats &lt;- read_municipal_seat(showProgress = FALSE)\n\nrj &lt;- subset(seats, code_muni == 3304557)\nsp &lt;- subset(seats, code_muni == 3550308)\n\nroute &lt;- osrmRoute(\n  src = rj, dst = sp, \n  overview = \"full\",\n  osrm.profile = \"car\"\n)\n\n\n# Route distance, in meters\nroute$distance\n\n[1] 422.436\n\n\n\n# Route duration, in minutes\nroute$duration\n\n[1] 320.7633\n\n\n\nroute |&gt;\n  st_transform(4326) |&gt;\n  leaflet() |&gt;\n  addTiles() |&gt;\n  addPolylines()\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nFor some pairs of municipalities, the OSRM service is not able to determine a possible road route. This is expected, as some municipalities are reachable only by plane or boat.\n\n\nThe scripts used to prepare the dataset are available here."
  },
  {
    "objectID": "data-projects/brazil_road_distances.html#dataset-download",
    "href": "data-projects/brazil_road_distances.html#dataset-download",
    "title": "Road distances and trip duration matrix for Brazilian municipalities",
    "section": "Dataset download",
    "text": "Dataset download\nThe dataset with the road distances and trip duration are available on Zenodo, on RDS format, parquet format, and CSV (zipped) format.\nClick the link below to access and download the data.\n\nYou can also download the dataset directly from R, using the {zendown} package.\n\n# install.packages(\"zendown\")\nlibrary(zendown)\n\ndist_brasil_file &lt;- zen_file(11400243, \"dist_brasil.rds\")\n\ndist_brasil &lt;- readRDS(dist_brasil_file)\n\nhead(dist_brasil)\n\n     orig    dest   dist   dur\n1 1100015 1100023 330534 313.6\n2 1100015 1100031 397600 380.7\n3 1100015 1100049 112805 113.9\n4 1100015 1100056 390938 373.2\n5 1100015 1100064 353606 333.9\n6 1100015 1100072 321636 348.5\n\n\n\nGraphs\n\nlibrary(ggplot2)\n\nggplot(data = dist_brasil, aes(x = dist/1000)) +\n  geom_histogram(bins = 100) +\n  labs(\n    title = \"Fastest route road distance between Brazilian municipalities\", \n    x = \"Distance (km)\", y = \"count\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nggplot(data = dist_brasil, aes(x = dur/60)) +\n  geom_histogram(bins = 100) +\n  labs(\n    title = \"Fastest route estimated trip duration between Brazilian municipalities\", \n    x = \"Trip duration (hours)\", y = \"count\"\n  ) +\n  theme_bw()"
  },
  {
    "objectID": "data-projects/brazil_road_distances.html#future-plans",
    "href": "data-projects/brazil_road_distances.html#future-plans",
    "title": "Road distances and trip duration matrix for Brazilian municipalities",
    "section": "Future plans",
    "text": "Future plans\n\nCompute routes using other available routing services.\nYearly updates of the dataset, as the road infrastructure may change."
  },
  {
    "objectID": "data-projects/brazil_road_distances.html#footnotes",
    "href": "data-projects/brazil_road_distances.html#footnotes",
    "title": "Road distances and trip duration matrix for Brazilian municipalities",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs new data for the 2022 Brazilian Census be available, the dataset will be likely updated.↩︎"
  },
  {
    "objectID": "data-projects/brazil-climate-zonal-indicators.html#introduction",
    "href": "data-projects/brazil-climate-zonal-indicators.html#introduction",
    "title": "Zonal statistics of climate indicators for Brazilian municipalities",
    "section": "Introduction",
    "text": "Introduction\nClimate indicators are used on several statistical models for many research areas and are specially important for modeling Climate Sensitive Diseases (CSD) incidence. Those models usually adopts a lattice structure, where its data is aggregated at administrative boundaries (e.g. disease incidence), but climate indicators are usually presented in a continuous regular grid format.\nTo make climate indicators compatible with lattice structures, zonal statistics may be adopted. Zonal statistics are descriptive statistics calculated using a set of cells that spatially intersects a given spatial boundary. For each boundary in a map, statistics like average, maximum value, minimum value, standard deviation, and sum are obtained to represent the cell’s values that intersect the boundary.\nI created some zonal statistics of climate indicators datasets for Brazilian municipalities from some climate data products."
  },
  {
    "objectID": "data-projects/brazil-climate-zonal-indicators.html#zonal-era5-land",
    "href": "data-projects/brazil-climate-zonal-indicators.html#zonal-era5-land",
    "title": "Zonal statistics of climate indicators for Brazilian municipalities",
    "section": "Zonal ERA5-Land",
    "text": "Zonal ERA5-Land\n1950-2022: \n2023: \nThe ERA5-Land data (Muñoz-Sabater et al. 2021) presents hourly climate indicators at 0.1° × 0.1° horizontal resolution with global coverage, from 1950 to present.\nSelected indicators hourly data were downloaded and aggregated at daily basis to compute zonal statistics for the Brazilian municipalities.\nThe following indicators and aggregations were computed.\n\n\n\n\n\n\n\n\nERA5-Land indicators\nDaily time-aggregating functions\nSpatial zonal statistics\n\n\n\n\nTemperature (2m)\nmean, max, min\nmax, min, stdev, count\n\n\nDewpoint temp. (2m)\nmean\nmax, min, stdev, count\n\n\n\\(u\\) component of wind\nmean\nmax, min, stdev, count\n\n\n\\(v\\) component of wind\nmean\nmax, min, stdev, count\n\n\nSurface pressure\nmean\nmax, min, stdev, count\n\n\nTotal precipitation\nsum\nmax, min, stdev, count, sum\n\n\n\nA paper with the full methodology was published in the Environmental Data Science journal.\n\n\n\n\n\n\n\nTip\n\n\n\nThose files are in parquet format. Not sure how to open parquet files? Take a look on this blog post.\n\n\n\nUsage statistics\nUsage statistics of this and other datasets are available here."
  },
  {
    "objectID": "data-projects/brazil-climate-zonal-indicators.html#zonal-br-dwgd",
    "href": "data-projects/brazil-climate-zonal-indicators.html#zonal-br-dwgd",
    "title": "Zonal statistics of climate indicators for Brazilian municipalities",
    "section": "Zonal BR-DWGD",
    "text": "Zonal BR-DWGD\n1961-01-01 to 2024-03-20: \nThe BR-DWGD dataset (Xavier et al. 2022) presents daily meteorological data interpolated to a grid with 0.1° × 0.1° of spatial resolution for the Brazilian territory, with daily data starting on January 1, 1961. It uses data from several weather stations rain gauges in its interpolation methods, cross-validated to the selection of the best method for each weather indicator.\nThe following weather indicators are available from the BR-DWGD study: precipitation (mm), minimum temperature (°C), maximum temperature (°C), solar radiation (MJ⋅m−2 ), wind speed at 2m height (m⋅s−1) and relative humidity (%).\nThe following zonal statistics were computed.\n\n\n\n\n\n\n\n\nBR-DGWD indicators\nDaily time-aggregating functions\nSpatial zonal statistics\n\n\n\n\nTemperature\nmax, min\nmax, min, stdev, count\n\n\nRelative humidity\nmean\nmax, min, stdev, count\n\n\n\\(u\\) component of wind\nmean\nmax, min, stdev, count\n\n\nEvapotranspiration\nmean\nmax, min, stdev, count\n\n\nSolar radiation\nmean\nmax, min, stdev, count\n\n\nPrecipitation\nsum\nmax, min, stdev, count, sum\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThose files are in parquet format. Not sure how to open parquet files? Take a look on this blog post."
  },
  {
    "objectID": "data-projects/brazil-climate-zonal-indicators.html#zonal-terraclimate",
    "href": "data-projects/brazil-climate-zonal-indicators.html#zonal-terraclimate",
    "title": "Zonal statistics of climate indicators for Brazilian municipalities",
    "section": "Zonal TerraClimate",
    "text": "Zonal TerraClimate\n\nThe TerraClimate (Abatzoglou et al. 2018) dataset presents monthly meteorological data interpolated to a grid with 0.04° × 0.04° (1/24th degree) of spatial resolution with world cover, with monthly data from January, 1958, to December, 2021.\nThe following weather indicators are available from the TerraClimate study: Actual Evapotranspiration (mm), Climate Water Deficit (mm), Potential evapotranspiration(mm), Precipitation (mm), Runoff (mm), Soil Moisture (mm), Downward surface shortwave radiation (W/m2), Snow water equivalent (mm), Minimum temperature (°C), Maximum temperature (°C), Vapor pressure (kPa), Wind speed (m/s), Vapor Pressure Deficit (kpa) and Palmer Drought Severity Index.\nThe following zonal statistics were computed.\n\nbrclimr::product_info(product = \"terraclimate\")\n\n&lt;list&gt;\n├─tmax: &lt;list&gt;\n│ ├─link: \"https://terraclimate.nyc3.cdn.di...\"\n│ ├─name: \"Maximum temperature\"\n│ ├─detail: \"Average for month\"\n│ ├─unit: \"°C\"\n│ ├─date_range: \"Monthly, 1958-01 to 2021-12\"\n│ └─stats: &lt;list&gt;\n│   ├─min: \"tmax_min\"\n│   ├─max: \"tmax_max\"\n│   ├─mean: \"tmax_mean\"\n│   └─sd: \"tmax_sd\"\n├─tmin: &lt;list&gt;\n│ ├─link: \"https://terraclimate.nyc3.cdn.di...\"\n│ ├─name: \"Minimum temperature\"\n│ ├─detail: \"Average for month\"\n│ ├─unit: \"°C\"\n│ ├─date_range: \"Monthly, 1958-01 to 2021-12\"\n│ └─stats: &lt;list&gt;\n│   ├─min: \"tmin_min\"\n│   ├─max: \"tmin_max\"\n│   ├─mean: \"tmin_mean\"\n│   └─sd: \"tmin_sd\"\n├─ppt: &lt;list&gt;\n│ ├─link: \"https://terraclimate.nyc3.cdn.di...\"\n│ ├─name: \"Precipitation\"\n│ ├─detail: \"Monthly total\"\n│ ├─unit: \"mm\"\n│ ├─date_range: \"Monthly, 1958-01 to 2021-12\"\n│ └─stats: &lt;list&gt;\n│   ├─min: \"ppt_min\"\n│   ├─max: \"ppt_max\"\n│   ├─mean: \"ppt_mean\"\n│   ├─sd: \"ppt_sd\"\n│   └─sum: \"ppt_sum\"\n├─aet: &lt;list&gt;\n│ ├─link: \"https://terraclimate.nyc3.cdn.di...\"\n│ ├─name: \"Actual Evapotranspiration\"\n│ ├─detail: \"Monthly total\"\n│ ├─unit: \"mm\"\n│ ├─date_range: \"Monthly, 1958-01 to 2021-12\"\n│ └─stats: &lt;list&gt;\n│   ├─min: \"aet_min\"\n│   ├─max: \"aet_max\"\n│   ├─mean: \"aet_mean\"\n│   ├─sd: \"aet_sd\"\n│   └─sum: \"aet_sum\"\n├─def: &lt;list&gt;\n│ ├─link: \"https://terraclimate.nyc3.cdn.di...\"\n│ ├─name: \"Climate Water Deficit\"\n│ ├─detail: \"Monthly total\"\n│ ├─unit: \"mm\"\n│ ├─date_range: \"Monthly, 1958-01 to 2021-12\"\n│ └─stats: &lt;list&gt;\n│   ├─min: \"def_min\"\n│   ├─max: \"def_max\"\n│   ├─mean: \"def_mean\"\n│   ├─sd: \"def_sd\"\n│   └─sum: \"def_sum\"\n├─pdsi: &lt;list&gt;\n│ ├─link: \"https://terraclimate.nyc3.cdn.di...\"\n│ ├─name: \"Palmer Drought Severity Index\"\n│ ├─detail: \"At end of month\"\n│ ├─unit: \"unitless\"\n│ ├─date_range: \"Monthly, 1958-01 to 2021-12\"\n│ └─stats: &lt;list&gt;\n│   ├─min: \"PDSI_min\"\n│   ├─max: \"PDSI_max\"\n│   ├─mean: \"PDSI_mean\"\n│   └─sd: \"PDSI_sd\"\n├─pet: &lt;list&gt;\n│ ├─link: \"https://terraclimate.nyc3.cdn.di...\"\n│ ├─name: \"Potential evapotranspiration\"\n│ ├─detail: \"Monthly total\"\n│ ├─unit: \"mm\"\n│ ├─date_range: \"Monthly, 1958-01 to 2021-12\"\n│ └─stats: &lt;list&gt;\n│   ├─min: \"pet_min\"\n│   ├─max: \"pet_max\"\n│   ├─mean: \"pet_mean\"\n│   ├─sd: \"pet_sd\"\n│   └─sum: \"pet_sum\"\n├─q: &lt;list&gt;\n│ ├─link: \"https://terraclimate.nyc3.cdn.di...\"\n│ ├─name: \"Runoff\"\n│ ├─detail: \"Monthly total\"\n│ ├─unit: \"mm\"\n│ ├─date_range: \"Monthly, 1958-01 to 2021-12\"\n│ └─stats: &lt;list&gt;\n│   ├─min: \"q_min\"\n│   ├─max: \"q_max\"\n│   ├─mean: \"q_mean\"\n│   ├─sd: \"q_sd\"\n│   └─sum: \"q_sum\"\n├─soil: &lt;list&gt;\n│ ├─link: \"https://terraclimate.nyc3.cdn.di...\"\n│ ├─name: \"Soil Moisture\"\n│ ├─detail: \"Total column, at end of month\"\n│ ├─unit: \"mm\"\n│ ├─date_range: \"Monthly, 1958-01 to 2021-12\"\n│ └─stats: &lt;list&gt;\n│   ├─min: \"soil_min\"\n│   ├─max: \"soil_max\"\n│   ├─mean: \"soil_mean\"\n│   ├─sd: \"soil_sd\"\n│   └─sum: \"soil_sum\"\n├─srad: &lt;list&gt;\n│ ├─link: \"https://terraclimate.nyc3.cdn.di...\"\n│ ├─name: \"Downward surface shortwave radia...\"\n│ ├─unit: \"W/m2\"\n│ ├─date_range: \"Monthly, 1958-01 to 2021-12\"\n│ └─stats: &lt;list&gt;\n│   ├─min: \"srad_min\"\n│   ├─max: \"srad_max\"\n│   ├─mean: \"srad_mean\"\n│   ├─sd: \"srad_sd\"\n│   └─sum: \"srad_sum\"\n├─swe: &lt;list&gt;\n│ ├─link: \"https://terraclimate.nyc3.cdn.di...\"\n│ ├─name: \"Snow water equivalent\"\n│ ├─detail: \"At end of month\"\n│ ├─unit: \"mm\"\n│ ├─date_range: \"Monthly, 1958-01 to 2021-12\"\n│ └─stats: &lt;list&gt;\n│   ├─min: \"swe_min\"\n│   ├─max: \"swe_max\"\n│   ├─mean: \"swe_mean\"\n│   ├─sd: \"swe_sd\"\n│   └─sum: \"swe_sum\"\n├─vap: &lt;list&gt;\n│ ├─link: \"https://terraclimate.nyc3.cdn.di...\"\n│ ├─name: \"Vapor pressure\"\n│ ├─detail: \"Average for month\"\n│ ├─unit: \"kPa\"\n│ ├─date_range: \"Monthly, 1958-01 to 2021-12\"\n│ └─stats: &lt;list&gt;\n│   ├─min: \"vap_min\"\n│   ├─max: \"vap_max\"\n│   ├─mean: \"vap_mean\"\n│   └─sd: \"vap_sd\"\n├─vpd: &lt;list&gt;\n│ ├─link: \"https://terraclimate.nyc3.cdn.di...\"\n│ ├─name: \"Vapor Pressure Deficit\"\n│ ├─detail: \"Average for month\"\n│ ├─unit: \"kPa\"\n│ ├─date_range: \"Monthly, 1958-01 to 2021-12\"\n│ └─stats: &lt;list&gt;\n│   ├─min: \"vpd_min\"\n│   ├─max: \"vpd_max\"\n│   ├─mean: \"vpd_mean\"\n│   └─sd: \"vpd_sd\"\n└─ws: &lt;list&gt;\n  ├─link: \"https://terraclimate.nyc3.cdn.di...\"\n  ├─name: \"Wind speed\"\n  ├─detail: \"Average for month\"\n  ├─unit: \"m/s\"\n  ├─date_range: \"Monthly, 1958-01 to 2021-12\"\n  └─stats: &lt;list&gt;\n    ├─min: \"ws_min\"\n    ├─max: \"ws_max\"\n    ├─mean: \"ws_mean\"\n    └─sd: \"ws_sd\"\n\n\nThe results are availabe as parquet files, available at Zenodo and can be also accessed with the brclimr R package.\n\n\n\n\n\n\nTip\n\n\n\nThose files are in parquet format. Not sure how to open parquet files? Take a look on this blog post."
  },
  {
    "objectID": "code.html#r-packages",
    "href": "code.html#r-packages",
    "title": "Packages & Code",
    "section": "R packages",
    "text": "R packages\n\nmicrodatasus\nR package to download and pre-process datasets from DataSUS, Brazilian Health Ministry.\n  \n\n\nbrpop\nR package with Brazilian population estimates.\n   \n\n\ntidyrates\nR package to compute age-standardized epidemiological rates in a tidy way.\n   \n\n\nzendown\nR package to download and cache files from Zenodo deposits.\n   \n\n\nzenstats\nR package to fetch statistics of views, downloads and volume from Zenodo deposits.\n   \n\n\nnseq\nR package to compute counts and statistics of occurrence of events in sequences that meets a criterion.\n   \n\n\nrspell\nR package to grammar check documents inside RStudio.\n  \n\n\nzonalclim\nR package to compute zonal statistics of climate indicators.\n  \n\n\nbrclimr\nR package to retrieve municipal climate indicators of Brazilian municipalities.\n   \n\n\nzonalclim\nR package with helper functions to compute zonal statistics using NetCDF and sf objects for climate studies.\n  \n\n\nplugfieldapi\nR package to access the Plugfield API and retrieve data from weather stations.\n  \n\n\nopendenguedata\nR package to download dengue data from the OpenDengue project.\n  \n\n\ninmetrss\nR package to fetch and parse INMET alerts RSS feed."
  },
  {
    "objectID": "code.html#packages-download-statistics",
    "href": "code.html#packages-download-statistics",
    "title": "Packages & Code",
    "section": "Packages download statistics",
    "text": "Packages download statistics\nDownload statistics of packages on CRAN are available here."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Handling 187 millions hospital admissions in Brazil with DuckDB\n\n\nA patient geographical flow study\n\n\n\nduckdb\n\n\nsih\n\n\n\n\n\n\n\n\n\nJun 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPersistent heat in Brazil: 1993 and 2023\n\n\n\n\n\n\nclimate\n\n\n\n\n\n\n\n\n\nApr 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWriting R packages with large datasets\n\n\n\n\n\n\npackage\n\n\ndata\n\n\n\n\n\n\n\n\n\nApr 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nQuery local parquet files\n\n\n\n\n\n\ndatabase\n\n\nparquet\n\n\narrow\n\n\n\n\n\n\n\n\n\nApr 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHow 2023 was hot in different Brazilian municipalities?\n\n\n\n\n\n\nclimate\n\n\nzonal statistics\n\n\n\n\n\n\n\n\n\nNov 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nUnivariate and multivariate time series clustering\n\n\nExamples with Brazilian climate data\n\n\n\nclustering\n\n\ntime series\n\n\ndtwclust\n\n\n\n\n\n\n\n\n\nNov 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nAge-adjusted COVID-19 mortality rates for Brazilian municipalities\n\n\n\n\n\n\ncovid19\n\n\ntidyrates\n\n\n\n\n\n\n\n\n\nNov 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nCounting consecutive sequences of events: run length encoding and warm spell occurence example\n\n\n\n\n\n\nrle\n\n\nwarm spell\n\n\nsequences\n\n\n\n\n\n\n\n\n\nNov 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nCrude and adjusted rates in a tidy way\n\n\n\n\n\n\nrates\n\n\nepidemiology\n\n\ntidyrates\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nQuery remote parquet files with DuckDB\n\n\n\n\n\n\ndatabase\n\n\nduckdb\n\n\nparquet\n\n\n\n\n\n\n\n\n\nOct 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nSQLite database conversion to DuckDB and Parquet files\n\n\n\n\n\n\ndatabase\n\n\nsqlite\n\n\nduckdb\n\n\nparquet\n\n\n\n\n\n\n\n\n\nOct 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nSome tips to work with SQLite database\n\n\n\n\n\n\ndatabase\n\n\nsqlite\n\n\n\n\n\n\n\n\n\nOct 20, 2023\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "CV",
    "section": "",
    "text": "Full name: Raphael de Freitas Saldanha\nDate of birth: April 18, 1985\nLanguages: Portuguese, English, French"
  },
  {
    "objectID": "cv.html#general-information",
    "href": "cv.html#general-information",
    "title": "CV",
    "section": "",
    "text": "Full name: Raphael de Freitas Saldanha\nDate of birth: April 18, 1985\nLanguages: Portuguese, English, French"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "CV",
    "section": "Education",
    "text": "Education\n\n2022 - current. Post-doctoral researcher at Inria and LNCC. Artificial Intelligence applications on Public Health and Climate-Sensitive Diseases forecast.\n2018 - 2021. Doctorate at Fiocruz, PPGICS. Information and Communication in Health. Data Science Applications for Public Health.\n2015 - 2017. Master’s at UFJF - PPGSC on Public Health. Traffic violence and economic growth, an Spatial Econometrics analysis.\n2008 - 2009. Specialization at UFJF. Computational Statistical Methods. Traffic violence and zero-alcohol-consumption law, an policy evaluation study.\n2003 - 2007. Bachelor Degree at UFJF - ICHL-DG. Geography. Regionalization proposal for the Juiz de Fora city based on Census 2000 data."
  },
  {
    "objectID": "cv.html#academic-interests",
    "href": "cv.html#academic-interests",
    "title": "CV",
    "section": "Academic interests",
    "text": "Academic interests\n\nData science\n\nR\nMachine Learning\ntidyverse, tidymodels\nShiny apps\nQuarto\nPlotly\nduckdb, SQLite\nElasticSearch\nKibana\n\n\n\nGIS\n\nQGIS\nESRI ArcGIS\nGeoServer\n\n\n\nOther technologies\n\nGephi\nLaTeX\nLinux\nMacOS"
  },
  {
    "objectID": "cv.html#work-and-teaching-experience",
    "href": "cv.html#work-and-teaching-experience",
    "title": "CV",
    "section": "Work and teaching experience",
    "text": "Work and teaching experience\n\n2024 - current. Associate editor of RECIIS.\n2017 - current. Research assistant at Fiocruz, ICICT, PCDaS.\n2017 - current. Research assistant at Fiocruz, ICICT, Observatório de Clima e Saúde.\n2021 - 2022 Research assistant at Fiocruz, IOC, Lathema Lab (ArboAlvo).\n2017 - 2018 Research assistant at IRD, Maison de la Télédétection (Montpellier, FR).\n2017 - current. Workshop teacher of Introduction to Spatial Econometry with R.\n2013 - Teaching Assistant, Scientific Methodology at UFJF, CRITT.\n2005 - 2016 GIS and statistics analyst at Plangeo (private company).\n2012-2013 Distance learning supervisor of Statistics, course of Public Management.\n2012 Approved for substitute teacher position of Thematic Cartography.\n2011 Distance learning tutor for Business Statistics (Curso de Administração Pública à Distância – PNAP), UFJF.\n2010 Teacher of Technologies for Environmental Planning, FAFISM."
  },
  {
    "objectID": "cv.html#additional-training",
    "href": "cv.html#additional-training",
    "title": "CV",
    "section": "Additional training",
    "text": "Additional training\n\n2023 - Nature-based solutions for tackling global health challenges. Heidelberg Institute of Global Health, HIGH, Alemanha.\n2022 - Algorithms and Programming Methods for Big Data Laboratório Nacional de Computação Científica (LNCC, Brazil)\n2023 - VII Escola Avançada em Big Data Analytics (USP, Brazil)\n2022 - Public Health Disparities Geocoding Project 2.0. Harvard School of Public Health\n2022 - Developing the SIR Model. Imperial College (Coursera)\n2022 - Machine Learning methods with R, Curso-R school.\n2021 - R Deploy methods, Curso-R school.\n2021 - QBA/On-line - Sensibilização em Gestão da Qualidade, Biossegurança e Ambiente, Fiocruz.\n2020 - Comunicação, Mídia e Saúde em Contexto de Emergências Sanitárias, UFAL.\n2018 - Anthropological and Historical Approaches of Visualising Epidemics, COC, Fiocruz.\n2018 - Information Visualization, Foundations and Applied Perception, NYU Tandon School of Engineering (Coursera).\n2017 - Practical Machine Learning, Johns Hopkins University (Coursera).\n2016 - Harvard - Brazil Collaborative Public Health Course, Harvard School of Public Health.\n2015 - Introduction to Big Data, University of California, San Diego (Coursera).\n2011 - Geoestatística com o uso do R, UFJF.\n2007 - Workshop em modelagem em estudos longitudinais, UFJF.\n2007 - Geomarketing - Conceitos e aplicações, Centro Universitário Senac.\n2004 - Geografia e movimentos sociais, A América Latina, AGB.\n2003 - Introdução à Gestão Ambiental Urbana, UFJF.\n2003 - Flash Mx, CRITT, UFJF."
  },
  {
    "objectID": "cv.html#hobbies",
    "href": "cv.html#hobbies",
    "title": "CV",
    "section": "Hobbies",
    "text": "Hobbies\n\nPhotography\nCycling"
  },
  {
    "objectID": "data-projects/brazil_climate_normals_indicators.html",
    "href": "data-projects/brazil_climate_normals_indicators.html",
    "title": "Climatological normals and indicators of Brazilian municipalities",
    "section": "",
    "text": "Climatological normals are averages of climate variables observed for a given time – usually months – in a 30-year range. The normals are usually used as comparison benchmarks against recent or current conditions, being useful to recognize anomalies and to characterize global warming impacts.\nThe normals are usually computed with surface meteorological stations data, maintained by government meteorological institutions, and its distribution over the territory may be scarce or uneven, like in Brazil. Thus, the availability of climatological normals to different regions and administrative divisions like municipalities is difficult to obtain.\nI propose here a method to compute monthly climatological normals for the 1961–1990 and monthly aggregated climate indicators from 1991 to 2023 for Brazilian municipalities using a climate reanalysis dataset.\n\n\n\n\n\n\nTip\n\n\n\nIn a hurry? Jump to the download section ;-)"
  },
  {
    "objectID": "data-projects/brazil_climate_normals_indicators.html#introduction",
    "href": "data-projects/brazil_climate_normals_indicators.html#introduction",
    "title": "Climatological normals and indicators of Brazilian municipalities",
    "section": "",
    "text": "Climatological normals are averages of climate variables observed for a given time – usually months – in a 30-year range. The normals are usually used as comparison benchmarks against recent or current conditions, being useful to recognize anomalies and to characterize global warming impacts.\nThe normals are usually computed with surface meteorological stations data, maintained by government meteorological institutions, and its distribution over the territory may be scarce or uneven, like in Brazil. Thus, the availability of climatological normals to different regions and administrative divisions like municipalities is difficult to obtain.\nI propose here a method to compute monthly climatological normals for the 1961–1990 and monthly aggregated climate indicators from 1991 to 2023 for Brazilian municipalities using a climate reanalysis dataset.\n\n\n\n\n\n\nTip\n\n\n\nIn a hurry? Jump to the download section ;-)"
  },
  {
    "objectID": "data-projects/brazil_climate_normals_indicators.html#methods",
    "href": "data-projects/brazil_climate_normals_indicators.html#methods",
    "title": "Climatological normals and indicators of Brazilian municipalities",
    "section": "Methods",
    "text": "Methods\nA climatological normal can be computed with data from different sources, including remote sensing sensors and “area averages or points in gridded datasets” (WMO 2017). Some gridded climatological datasets are available for the Brazilian territory, including the ERA5-Land from Copernicus (Muñoz-Sabater et al. 2021) and the BR-DWGD dataset (Xavier et al. 2022), offering several climatological indicators for a long time range, and continuously updates.\nSome research methods demands that climate data must be aggregated in the same spatial and temporal units of other data to be used in statistical models, being a fairly common procedure in epidemiology and economy studies. In order to approach this issue, spatial gridded data can be aggregated using zonal statistics (Saldanha et al. 2024).\nI used the same methodology from the study above to create zonal statistics of climate indicators from the BR-DWGD project (Xavier et al. 2022). This data is available here, presenting climatological indicators from 1961 to March 2024 for all Brazilian municipalities. I propose here to compute the climatological normals and other climate aggregated indicators from this dataset of zonal statistics.\nIn order to compute these indicators, I created an R package named {climindi}. The package provides helper functions to compute climatological normals and other aggregated indicators in a tidy way.\n\nNormal indicators\nThe {climindi} package computes the average, 10th and 90th percentile as climatological normals.\n\n\nAggregated indicators\nThe {climindi} package present functions to compute the following statistics for time-aggregated data: count of data points, average, median, standard deviation, standard error, maximum and minimum values, the 10th, 25th, 75th and 90th percentiles, and indicator-specific indicators, listed bellow.\n\nPrecipitation\n\nRain spells: count of rain spells occurrences, with 3 and 5 or more consecutive days with rain above the climatological normal average value\nCount of days with precipitation above 1mm, 5mm, 10mm, 50mm, and 100mm\nCount of sequences of 3 days, 5 days, 10 days, 15 days, 20 days, and 25 days or more without precipitation\n\nMaximum temperature\n\nHeat waves: Count of heat waves occurrences, with 3 and 5 or more consecutive days with maximum temperature above the climatological normal value plus 5 Celsius degrees\nHot days: count of warm days, when the maximum temperature is above the normal 90th percentile\nCount of days with temperatures above or equal to 25, 30, 35, and 40 Celsius degrees\n\nMinimum temperature\n\nCold spells: count of cold spells occurrences, with 3 and 5 or more consecutive days with minimum temperature bellow the climatological normal value minus 5 Celsius degrees\nCold days: count of cold days, when the minimum temperature is bellow the normal 10th percentile\nCount of days with temperatures bellow or equal to 0, 5, 10, 15, and 20 Celsius degrees\n\nRelative humidity\n\nCount of dry spells occurrences, with 3 and 5 or more consecutive days with relative humidity bellow the climatological normal value minus 10 percent\nCount of wet spells occurrences, with 3 and 5 or more consecutive days with relative humidity above the climatological normal value plus 10 percent\nCount of dry days, when the relative humidity is bellow the normal 10th percentile\nCount of wet days, when the relative humidity is above the normal 90th percentile\nCount of days with relative humidity between 21% and 30% (Attention level)\nCount of days with relative humidity between 12% and 20% (Alert level)\nCount of days with relative humidity bellow 12% (Emergence level)\n\nWind speed\n\nCount of sequences of 3 and 5 days or more with wind speed bellow the climatological average normal\nCount of sequences of 3 and 5 days or more with wind speed above the climatological average normal\n\nEvapotranspirations\n\nCount of sequences of 3 and 5 days or more with evapotranspirations bellow the climatological average normal\nCount of sequences of 3 and 5 days or more with evapotranspirations above the climatological average normal\n\nSolar radiation\n\nCount of sequences of 3 and 5 days or more with solar radiation bellow the climatological normal\nCount of sequences of 3 and 5 days or more with solar radiation above the climatological normal\n\n\n\n\nData source\nThe zonal statistics for the Brazilian municipalities computed with the BR-DWGD project is described here. We can use the {zendown} package to download the data files directly from from Zenodo.\n\n\nPackages\n\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(arrow)\nlibrary(readr)\nlibrary(climindi) # http://rfsaldanha.github.io/climindi/\nlibrary(zendown) # https://rfsaldanha.github.io/zendown/\n\n\n\n\n\n\n\nWarning\n\n\n\nTo perform those computations, I needed to increase the envinroment variable R_MAX_VSIZE, as explained here.\n\n\n\n\nPrecipitation (mm)\n\nData\n\npr_data &lt;- zen_file(13906834, \"pr_3.2.3.parquet\") |&gt;\n    open_dataset() |&gt;\n    filter(name == \"pr_3.2.3_mean\") |&gt;\n    filter(date &gt;= as.Date(\"1961-01-01\")) |&gt;\n    filter(date &lt;= as.Date(\"2023-12-31\")) |&gt;\n    select(-name) |&gt;\n    collect()\n\n\n\nNormal\n\npr_normal &lt;- pr_data |&gt;\n    # Identify month\n    mutate(month = month(date)) |&gt;\n    # Group by id variable and month\n    group_by(code_muni, month) |&gt;\n    # Compute normal\n    summarise_normal(\n        date_var = date, value_var = value, \n        year_start = 1961, year_end = 1990\n    ) |&gt;\n    # Ungroup\n    ungroup()\n\n\n\nIndicators\n\npr_indi &lt;- pr_data |&gt;\n    # Identify year\n    mutate(year = year(date)) |&gt;\n    # Identify month\n    mutate(month = month(date)) |&gt;\n    # Filter year\n    filter(year &gt;= 1991) |&gt;\n    # Group by id variable, year and month\n    group_by(code_muni, year, month) |&gt;\n    # Compute precipitation indicators\n    summarise_precipitation(\n        value_var = value, \n        normals_df = pr_normal\n    ) |&gt;\n    # Ungroup\n    ungroup()\n\n\n\nExport\n\nwrite_parquet(x = pr_normal, sink = \"pr_normal.parquet\")\nwrite_csv2(x = pr_normal, file = \"pr_normal.csv\")\nwrite_parquet(x = pr_indi, sink = \"pr_indi.parquet\")\nwrite_csv2(x = pr_indi, file = \"pr_indi.csv\")\n\n\n\n\nEvapotranspiration (mm)\n\nData\n\neto_data &lt;- zen_file(13906834, \"ETo_3.2.3.parquet\") |&gt;\n    open_dataset() |&gt;\n    filter(name == \"ETo_3.2.3_mean\") |&gt;\n    filter(date &gt;= as.Date(\"1961-01-01\")) |&gt;\n    filter(date &lt;= as.Date(\"2023-12-31\")) |&gt;\n    select(-name) |&gt;\n    collect()\n\n\n\nNormal\n\neto_normal &lt;- eto_data |&gt;\n    # Identify month\n    mutate(month = month(date)) |&gt;\n    # Group by id variable and month\n    group_by(code_muni, month) |&gt;\n    # Compute normal\n    summarise_normal(\n        date_var = date, value_var = value, \n        year_start = 1961, year_end = 1990\n    ) |&gt;\n    # Ungroup\n    ungroup()\n\n\n\nIndicators\n\neto_indi &lt;- eto_data |&gt;\n    # Identify year\n    mutate(year = year(date)) |&gt;\n    # Identify month\n    mutate(month = month(date)) |&gt;\n    # Filter year\n    filter(year &gt;= 1991) |&gt;\n    # Group by id variable, year and month\n    group_by(code_muni, year, month) |&gt;\n    # Compute precipitation indicators\n    summarise_evapotrapiration(\n        value_var = value, \n        normals_df = eto_normal\n    ) |&gt;\n    # Ungroup\n    ungroup()\n\n\n\nExport\n\nwrite_parquet(x = eto_normal, sink = \"eto_normal.parquet\")\nwrite_csv2(x = eto_normal, file = \"eto_normal.csv\")\nwrite_parquet(x = eto_indi, sink = \"eto_indi.parquet\")\nwrite_csv2(x = eto_indi, file = \"eto_indi.csv\")\n\n\n\n\nMaximum temperature (°C)\n\nData\n\ntmax_data &lt;- zen_file(13906834, \"Tmax_3.2.3.parquet\") |&gt;\n    open_dataset() |&gt;\n    filter(name == \"Tmax_3.2.3_mean\") |&gt;\n    filter(date &gt;= as.Date(\"1961-01-01\")) |&gt;\n    filter(date &lt;= as.Date(\"2023-12-31\")) |&gt;\n    select(-name) |&gt;\n    collect()\n\n\n\nNormal\n\ntmax_normal &lt;- tmax_data |&gt;\n    # Identify month\n    mutate(month = month(date)) |&gt;\n    # Group by id variable and month\n    group_by(code_muni, month) |&gt;\n    # Compute normal\n    summarise_normal(\n        date_var = date, value_var = value, \n        year_start = 1961, year_end = 1990\n    ) |&gt;\n    # Ungroup\n    ungroup()\n\n\n\nIndicators\n\ntmax_indi &lt;- tmax_data |&gt;\n    # Identify year\n    mutate(year = year(date)) |&gt;\n    # Identify month\n    mutate(month = month(date)) |&gt;\n    # Filter year\n    filter(year &gt;= 1991) |&gt;\n    # Group by id variable, year and month\n    group_by(code_muni, year, month) |&gt;\n    # Compute precipitation indicators\n    summarise_temp_max(\n        value_var = value, \n        normals_df = tmax_normal\n    ) |&gt;\n    # Ungroup\n    ungroup()\n\n\n\nExport\n\nwrite_parquet(x = tmax_normal, sink = \"tmax_normal.parquet\")\nwrite_csv2(x = tmax_normal, file = \"tmax_normal.csv\")\nwrite_parquet(x = tmax_indi, sink = \"tmax_indi.parquet\")\nwrite_csv2(x = tmax_indi, file = \"tmax_indi.csv\")\n\n\n\n\nMinimum temperature (°C)\n\nData\n\ntmin_data &lt;- zen_file(13906834, \"Tmin_3.2.3.parquet\") |&gt;\n    open_dataset() |&gt;\n    filter(name == \"Tmin_3.2.3_mean\") |&gt;\n    filter(date &gt;= as.Date(\"1961-01-01\")) |&gt;\n    filter(date &lt;= as.Date(\"2023-12-31\")) |&gt;\n    select(-name) |&gt;\n    collect()\n\n\n\nNormal\n\ntmin_normal &lt;- tmin_data |&gt;\n    # Identify month\n    mutate(month = month(date)) |&gt;\n    # Group by id variable and month\n    group_by(code_muni, month) |&gt;\n    # Compute normal\n    summarise_normal(\n        date_var = date, value_var = value, \n        year_start = 1961, year_end = 1990\n    ) |&gt;\n    # Ungroup\n    ungroup()\n\n\n\nIndicators\n\ntmin_indi &lt;- tmin_data |&gt;\n    # Identify year\n    mutate(year = year(date)) |&gt;\n    # Identify month\n    mutate(month = month(date)) |&gt;\n    # Filter year\n    filter(year &gt;= 1991) |&gt;\n    # Group by id variable, year and month\n    group_by(code_muni, year, month) |&gt;\n    # Compute precipitation indicators\n    summarise_temp_min(\n        value_var = value, \n        normals_df = tmin_normal\n    ) |&gt;\n    # Ungroup\n    ungroup()\n\n\n\nExport\n\nwrite_parquet(x = tmin_normal, sink = \"tmin_normal.parquet\")\nwrite_csv2(x = tmin_normal, file = \"tmin_normal.csv\")\nwrite_parquet(x = tmin_indi, sink = \"tmin_indi.parquet\")\nwrite_csv2(x = tmin_indi, file = \"tmin_indi.csv\")\n\n\n\n\nSolar radiation (MJm-2)\n\nData\n\nrs_data &lt;- zen_file(13906834, \"Rs_3.2.3.parquet\") |&gt;\n    open_dataset() |&gt;\n    filter(name == \"Rs_3.2.3_mean\") |&gt;\n    filter(date &gt;= as.Date(\"1961-01-01\")) |&gt;\n    filter(date &lt;= as.Date(\"2023-12-31\")) |&gt;\n    select(-name) |&gt;\n    collect()\n\n\n\nNormal\n\nrs_normal &lt;- rs_data |&gt;\n    # Identify month\n    mutate(month = month(date)) |&gt;\n    # Group by id variable and month\n    group_by(code_muni, month) |&gt;\n    # Compute normal\n    summarise_normal(\n        date_var = date, value_var = value, \n        year_start = 1961, year_end = 1990\n    ) |&gt;\n    # Ungroup\n    ungroup()\n\n\n\nIndicators\n\nrs_indi &lt;- rs_data |&gt;\n    # Identify year\n    mutate(year = year(date)) |&gt;\n    # Identify month\n    mutate(month = month(date)) |&gt;\n    # Filter year\n    filter(year &gt;= 1991) |&gt;\n    # Group by id variable, year and month\n    group_by(code_muni, year, month) |&gt;\n    # Compute precipitation indicators\n    summarise_solar_radiation(\n        value_var = value, \n        normals_df = rs_normal\n    ) |&gt;\n    # Ungroup\n    ungroup()\n\n\n\nExport\n\nwrite_parquet(x = rs_normal, sink = \"rs_normal.parquet\")\nwrite_csv2(x = rs_normal, file = \"rs_normal.csv\")\nwrite_parquet(x = rs_indi, sink = \"rs_indi.parquet\")\nwrite_csv2(x = rs_indi, file = \"rs_indi.csv\")\n\n\n\n\nWind speed at 2m height (m/s)\n\nData\n\nu2_data &lt;- zen_file(13906834, \"u2_3.2.3.parquet\") |&gt;\n    open_dataset() |&gt;\n    filter(name == \"u2_3.2.3_mean\") |&gt;\n    filter(date &gt;= as.Date(\"1961-01-01\")) |&gt;\n    filter(date &lt;= as.Date(\"2023-12-31\")) |&gt;\n    select(-name) |&gt;\n    collect()\n\n\n\nNormal\n\nu2_normal &lt;- u2_data |&gt;\n    # Identify month\n    mutate(month = month(date)) |&gt;\n    # Group by id variable and month\n    group_by(code_muni, month) |&gt;\n    # Compute normal\n    summarise_normal(\n        date_var = date, value_var = value, \n        year_start = 1961, year_end = 1990\n    ) |&gt;\n    # Ungroup\n    ungroup()\n\n\n\nIndicators\n\nu2_indi &lt;- u2_data |&gt;\n    # Identify year\n    mutate(year = year(date)) |&gt;\n    # Identify month\n    mutate(month = month(date)) |&gt;\n    # Filter year\n    filter(year &gt;= 1991) |&gt;\n    # Group by id variable, year and month\n    group_by(code_muni, year, month) |&gt;\n    # Compute precipitation indicators\n    summarise_windspeed(\n        value_var = value, \n        normals_df = u2_normal\n    ) |&gt;\n    # Ungroup\n    ungroup()\n\n\n\nExport\n\nwrite_parquet(x = u2_normal, sink = \"u2_normal.parquet\")\nwrite_csv2(x = u2_normal, file = \"u2_normal.csv\")\nwrite_parquet(x = u2_indi, sink = \"u2_indi.parquet\")\nwrite_csv2(x = u2_indi, file = \"u2_indi.csv\")\n\n\n\n\nRelative humidity (%)\n\nData\n\nrh_data &lt;- zen_file(13906834, \"RH_3.2.3.parquet\") |&gt;\n    open_dataset() |&gt;\n    filter(name == \"RH_3.2.3_mean\") |&gt;\n    filter(date &gt;= as.Date(\"1961-01-01\")) |&gt;\n    filter(date &lt;= as.Date(\"2023-12-31\")) |&gt;\n    select(-name) |&gt;\n    collect()\n\n\n\nNormal\n\nrh_normal &lt;- rh_data |&gt;\n    # Identify month\n    mutate(month = month(date)) |&gt;\n    # Group by id variable and month\n    group_by(code_muni, month) |&gt;\n    # Compute normal\n    summarise_normal(\n        date_var = date, value_var = value, \n        year_start = 1961, year_end = 1990\n    ) |&gt;\n    # Ungroup\n    ungroup()\n\n\n\nIndicators\n\nrh_indi &lt;- rh_data |&gt;\n    # Identify year\n    mutate(year = year(date)) |&gt;\n    # Identify month\n    mutate(month = month(date)) |&gt;\n    # Filter year\n    filter(year &gt;= 1991) |&gt;\n    # Group by id variable, year and month\n    group_by(code_muni, year, month) |&gt;\n    # Compute precipitation indicators\n    summarise_rel_humidity(\n        value_var = value, \n        normals_df = rh_normal\n    ) |&gt;\n    # Ungroup\n    ungroup()\n\n\n\nExport\n\nwrite_parquet(x = rh_normal, sink = \"rh_normal.parquet\")\nwrite_csv2(x = rh_normal, file = \"rh_normal.csv\")\nwrite_parquet(x = rh_indi, sink = \"rh_indi.parquet\")\nwrite_csv2(x = rh_indi, file = \"rh_indi.csv\")"
  },
  {
    "objectID": "data-projects/brazil_climate_normals_indicators.html#sec-download",
    "href": "data-projects/brazil_climate_normals_indicators.html#sec-download",
    "title": "Climatological normals and indicators of Brazilian municipalities",
    "section": "Results and dataset download",
    "text": "Results and dataset download\nThe climatological normals and aggregated indicators of Brazilian municipalities can be downloaded from Zenodo on CSV and parquet formats. Click the link bellow to access and download the data.\n\nYou can also download the dataset directly from R, using the {zendown} package.\nLet’s check some results.\n\nMaximum temperature, Rio de Janeiro, RJ, 2023\n\nObserved and normal\n\ntmax_data &lt;- zen_file(13906834, \"Tmax_3.2.3.parquet\") |&gt;\n    open_dataset() |&gt;\n    filter(name == \"Tmax_3.2.3_mean\") |&gt;\n    filter(code_muni == 3304557) |&gt;\n    filter(date &gt;= as.Date(\"2023-01-01\")) |&gt;\n    filter(date &lt;= as.Date(\"2023-12-31\")) |&gt;\n    select(-name) |&gt;\n    collect()\n\ntmax_normal &lt;- zen_file(13934888, \"tmax_normal.parquet\") |&gt;\n    open_dataset() |&gt;\n    filter(code_muni == 3304557) |&gt;\n    collect()\n\n\nlibrary(ggplot2)\nlibrary(tidyr)\n\ntmax_normal_exp &lt;- tmax_normal |&gt;\n    mutate(date = as_date(paste0(\"2023-\",month,\"-01\"))) |&gt;\n    group_by(month) %&gt;%\n    expand(\n        date = seq.Date(floor_date(date, unit = \"month\"), ceiling_date(date, unit=\"month\")-days(1), by=\"day\"), normal_mean, normal_p10, normal_p90\n    ) |&gt;\n    pivot_longer(cols = starts_with(\"normal_\")) |&gt;\n    mutate(name = substr(name, 8, 100))\n\nggplot() +\n    geom_line(data = tmax_data, aes(x = date, y = value)) +\n    geom_line(data = tmax_normal_exp, aes(x = date, y = value, color = name)) +\n    theme_bw() +\n    labs(\n        title = \"Maximum temperature and climatological normal\", \n        subtitle = \"Rio de Janeiro, RJ\",\n        color = \"Normal (1961-1990)\",\n        x = \"Date\", y = \"Celsius degrees\"\n    ) +\n    theme(legend.position = \"bottom\", legend.direction = \"horizontal\")\n\n\n\n\n\n\n\n\n\n\nIndicators\n\nlibrary(gt)\n\nzen_file(13934888, \"tmax_indi.parquet\") |&gt;\n    open_dataset() |&gt;\n    filter(code_muni == 3304557) |&gt;\n    filter(year == 2023) |&gt;\n    select(-code_muni, -year) |&gt;\n    collect() |&gt;\n    gt() |&gt;\n    fmt_number(\n        columns = where(is.double),\n        decimals = 2,\n        use_seps = FALSE\n    )\n\n\n\n\n\n\n\nmonth\ncount\nnormal_mean\nnormal_p10\nnormal_p90\nmean\nmedian\nsd\nse\nmax\nmin\np10\np25\np75\np90\nheat_waves_3d\nheat_waves_5d\nhot_days\nt_25\nt_30\nt_35\nt_40\n\n\n\n\n1.00\n31\n31.73\n26.93\n36.51\n30.56\n31.44\n3.92\n0.70\n35.32\n21.36\n23.99\n29.28\n32.96\n34.65\n0\n0\n0\n2\n4\n1\n0\n\n\n2.00\n28\n32.23\n27.80\n36.05\n32.67\n32.22\n2.47\n0.47\n37.20\n28.26\n30.04\n30.92\n34.91\n35.86\n0\n0\n3\n1\n3\n5\n0\n\n\n3.00\n31\n31.15\n26.95\n34.75\n31.63\n31.01\n2.21\n0.40\n36.72\n27.88\n29.21\n30.32\n33.14\n35.03\n0\n0\n4\n1\n5\n4\n0\n\n\n4.00\n30\n28.83\n25.03\n32.98\n27.86\n27.79\n2.33\n0.42\n32.16\n23.10\n24.92\n26.17\n29.54\n30.68\n0\n0\n0\n3\n4\n0\n0\n\n\n5.00\n31\n27.19\n23.23\n31.03\n26.33\n27.45\n2.55\n0.46\n30.42\n20.67\n23.26\n24.11\n28.06\n29.22\n0\n0\n0\n3\n1\n0\n0\n\n\n6.00\n30\n26.14\n21.77\n30.30\n26.55\n26.39\n2.56\n0.47\n31.11\n21.69\n23.39\n25.09\n28.06\n30.46\n0\n0\n2\n2\n2\n0\n0\n\n\n7.00\n31\n25.85\n21.44\n30.29\n25.99\n25.40\n3.40\n0.61\n35.50\n20.93\n22.13\n23.04\n28.41\n29.46\n0\n0\n3\n6\n3\n1\n0\n\n\n8.00\n31\n26.79\n21.34\n31.84\n27.29\n26.51\n4.91\n0.88\n37.73\n17.49\n22.32\n24.37\n30.79\n33.19\n1\n0\n4\n6\n5\n1\n0\n\n\n9.00\n30\n26.82\n21.49\n32.93\n30.39\n30.81\n4.17\n0.76\n36.94\n21.40\n23.91\n27.73\n33.35\n35.00\n3\n0\n6\n4\n6\n3\n0\n\n\n10.00\n31\n27.59\n22.33\n33.40\n28.49\n29.49\n4.54\n0.82\n37.85\n20.91\n23.06\n24.81\n32.65\n33.71\n1\n0\n4\n5\n6\n1\n0\n\n\n11.00\n30\n29.02\n23.87\n34.44\n31.21\n30.75\n4.79\n0.88\n40.94\n24.80\n25.35\n26.20\n34.72\n37.07\n1\n1\n5\n4\n5\n5\n1\n\n\n12.00\n31\n30.41\n25.32\n35.41\n31.30\n31.32\n3.27\n0.59\n38.47\n25.13\n26.78\n29.20\n33.47\n34.47\n0\n0\n2\n1\n5\n3\n0"
  },
  {
    "objectID": "data-projects/brazil_climate_normals_indicators.html#session-info",
    "href": "data-projects/brazil_climate_normals_indicators.html#session-info",
    "title": "Climatological normals and indicators of Brazilian municipalities",
    "section": "Session info",
    "text": "Session info\n\nsessioninfo::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.2 (2023-10-31)\n os       macOS 15.0.1\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Paris\n date     2024-10-16\n pandoc   3.2 @ /Applications/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version    date (UTC) lib source\n arrow       * 17.0.0.1   2024-08-21 [1] CRAN (R 4.3.3)\n assertthat    0.2.1      2019-03-21 [1] CRAN (R 4.3.0)\n backports     1.5.0      2024-05-23 [1] CRAN (R 4.3.3)\n bit           4.5.0      2024-09-20 [1] CRAN (R 4.3.3)\n bit64         4.5.2      2024-09-22 [1] CRAN (R 4.3.3)\n checkmate     2.3.2      2024-07-29 [1] CRAN (R 4.3.3)\n cli           3.6.3      2024-06-21 [1] CRAN (R 4.3.3)\n climindi    * 0.0.0.9000 2024-10-16 [1] local\n colorspace    2.1-1      2024-07-26 [1] CRAN (R 4.3.3)\n digest        0.6.37     2024-08-19 [1] CRAN (R 4.3.3)\n dplyr       * 1.1.4      2023-11-17 [1] CRAN (R 4.3.1)\n evaluate      1.0.1      2024-10-10 [1] CRAN (R 4.3.3)\n fansi         1.0.6      2023-12-08 [1] CRAN (R 4.3.1)\n farver        2.1.2      2024-05-13 [1] CRAN (R 4.3.3)\n fastmap       1.2.0      2024-05-15 [1] CRAN (R 4.3.3)\n fs            1.6.4      2024-04-25 [1] CRAN (R 4.3.1)\n generics      0.1.3      2022-07-05 [1] CRAN (R 4.3.0)\n ggplot2     * 3.5.1      2024-04-23 [1] CRAN (R 4.3.1)\n glue          1.8.0      2024-09-30 [1] CRAN (R 4.3.3)\n gt          * 0.11.1     2024-10-04 [1] CRAN (R 4.3.3)\n gtable        0.3.5      2024-04-22 [1] CRAN (R 4.3.1)\n hms           1.1.3      2023-03-21 [1] CRAN (R 4.3.0)\n htmltools     0.5.8.1    2024-04-04 [1] CRAN (R 4.3.1)\n htmlwidgets   1.6.4      2023-12-06 [1] CRAN (R 4.3.1)\n jsonlite      1.8.9      2024-09-20 [1] CRAN (R 4.3.3)\n knitr         1.48       2024-07-07 [1] CRAN (R 4.3.3)\n labeling      0.4.3      2023-08-29 [1] CRAN (R 4.3.0)\n lifecycle     1.0.4      2023-11-07 [1] CRAN (R 4.3.1)\n lubridate   * 1.9.3      2023-09-27 [1] CRAN (R 4.3.1)\n magrittr      2.0.3      2022-03-30 [1] CRAN (R 4.3.0)\n munsell       0.5.1      2024-04-01 [1] CRAN (R 4.3.1)\n pillar        1.9.0      2023-03-22 [1] CRAN (R 4.3.0)\n pkgconfig     2.0.3      2019-09-22 [1] CRAN (R 4.3.0)\n purrr         1.0.2      2023-08-10 [1] CRAN (R 4.3.0)\n R6            2.5.1      2021-08-19 [1] CRAN (R 4.3.0)\n readr       * 2.1.5      2024-01-10 [1] CRAN (R 4.3.1)\n rlang         1.1.4      2024-06-04 [1] CRAN (R 4.3.3)\n rmarkdown     2.28       2024-08-17 [1] CRAN (R 4.3.3)\n sass          0.4.9      2024-03-15 [1] CRAN (R 4.3.1)\n scales        1.3.0      2023-11-28 [1] CRAN (R 4.3.1)\n sessioninfo   1.2.2      2021-12-06 [1] CRAN (R 4.3.0)\n tibble        3.2.1      2023-03-20 [1] CRAN (R 4.3.0)\n tidyr       * 1.3.1      2024-01-24 [1] CRAN (R 4.3.1)\n tidyselect    1.2.1      2024-03-11 [1] CRAN (R 4.3.1)\n timechange    0.3.0      2024-01-18 [1] CRAN (R 4.3.1)\n tzdb          0.4.0      2023-05-12 [1] CRAN (R 4.3.0)\n utf8          1.2.4      2023-10-22 [1] CRAN (R 4.3.1)\n vctrs         0.6.5      2023-12-01 [1] CRAN (R 4.3.1)\n withr         3.0.1      2024-07-31 [1] CRAN (R 4.3.3)\n xfun          0.48       2024-10-03 [1] CRAN (R 4.3.3)\n xml2          1.3.6      2023-12-04 [1] CRAN (R 4.3.1)\n yaml          2.3.10     2024-07-26 [1] CRAN (R 4.3.3)\n zendown     * 0.1.0      2024-04-18 [1] Github (rfsaldanha/zendown@afbd73a)\n\n [1] /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "data-projects/era5land-daily-africa.html#introduction",
    "href": "data-projects/era5land-daily-africa.html#introduction",
    "title": "ERA5-Land selected indicators daily aggregates for Africa",
    "section": "Introduction",
    "text": "Introduction\nThe ERA5-Land reanalysis from The Copernicus Programme is an incredible source of climate data with global coverage of land areas from 1950 to the present, at 10km spatial resolution. Its original data is at hourly interval, and monthly aggregates are also available at the Copernicus Data Store (CDS).\nFor some applications like Climate-Sensitive Diseases (CSD) modelling, the hourly interval may be too much detailed, but the monthly aggregation is too coarse.\nFor this reason, I created daily aggregates from some ERA5-Land indicators for some regions."
  },
  {
    "objectID": "data-projects/era5land-daily-africa.html#methodology",
    "href": "data-projects/era5land-daily-africa.html#methodology",
    "title": "ERA5-Land selected indicators daily aggregates for Africa",
    "section": "Methodology",
    "text": "Methodology\nI developed an R script using the KrigR package (Kusch and Davy 2022). The script downloads a set of indicators, starting on 1950, for a geographical bounding box covering Africa (coordinates -47.11,71.01,34.88,71.38) and aggregates the data from hourly to daily, saving its results as NetCDF files. Each resulting file covers a year’s month and presents data layers for each day of the respective month.\nThe table bellow contains the time aggregation functions applied to each climate indicator.\n\n\n\nIndicator\nDaily aggregation function\n\n\n\n\n2m temperature\nmean, max, min\n\n\n2m dewpoint temperature\nmean\n\n\nu component of wind\nmean\n\n\nv component of wind\nmean\n\n\nsurface pressure\nmean\n\n\ntotal precipitation\nsum"
  },
  {
    "objectID": "data-projects/era5land-daily-africa.html#datasets",
    "href": "data-projects/era5land-daily-africa.html#datasets",
    "title": "ERA5-Land selected indicators daily aggregates for Africa",
    "section": "Datasets",
    "text": "Datasets\nData from 1950 to 2023 is ready to use and available on Zenodo. Data from 1950 to 1969 is being processed and will be made available soon.\n\n\n\nYear\nZenodo deposit\n\n\n\n\n1950\n\n\n\n1951\n\n\n\n1952\n\n\n\n1953\n\n\n\n1954\n\n\n\n1955\n\n\n\n1956\n\n\n\n1957\n\n\n\n1958\n\n\n\n1959\n\n\n\n1960\n\n\n\n1961\n\n\n\n1962\n\n\n\n1963\n\n\n\n1964\n\n\n\n1965\n\n\n\n1966\n\n\n\n1967\n\n\n\n1968\n\n\n\n1969\n\n\n\n1970\n\n\n\n1971\n\n\n\n1972\n\n\n\n1973\n\n\n\n1974\n\n\n\n1975\n\n\n\n1976\n\n\n\n1977\n\n\n\n1978\n\n\n\n1979\n\n\n\n1980\n\n\n\n1981\n\n\n\n1982\n\n\n\n1983\n\n\n\n1984\n\n\n\n1985\n\n\n\n1986\n\n\n\n1987\n\n\n\n1988\n\n\n\n1989\n\n\n\n1990\n\n\n\n1991\n\n\n\n1992\n\n\n\n1993\n\n\n\n1994\n\n\n\n1995\n\n\n\n1996\n\n\n\n1997\n\n\n\n1998\n\n\n\n1999\n\n\n\n2000\n\n\n\n2001\n\n\n\n2002\n\n\n\n2003\n\n\n\n2004\n\n\n\n2005\n\n\n\n2006\n\n\n\n2007\n\n\n\n2008\n\n\n\n2009\n\n\n\n2010\n\n\n\n2011\n\n\n\n2012\n\n\n\n2013\n\n\n\n2014\n\n\n\n2015\n\n\n\n2016\n\n\n\n2017\n\n\n\n2018\n\n\n\n2019\n\n\n\n2020\n\n\n\n2021\n\n\n\n2022\n\n\n\n2023\n\n\n\n\n\nUsage statistics\nUsage statistics of this and other datasets are available here."
  },
  {
    "objectID": "data-projects/health-indicators.html",
    "href": "data-projects/health-indicators.html",
    "title": "Health indicators",
    "section": "",
    "text": "Computed health indicators for Brazilian municipalities, health regions and UFs ready to download on parquet format. The definitions are in Portuguese.\n\n\n\n\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nTaxa de mortalidade específica por Acidente Vascular Cerebral\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTaxa de mortalidade específica por causas externas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "data-projects/health_indicators/indi_0002.html",
    "href": "data-projects/health_indicators/indi_0002.html",
    "title": "Taxa de mortalidade específica por Acidente Vascular Cerebral",
    "section": "",
    "text": "Número de óbitos por Acidente Vascular Cerebral (AVC), por 100 mil habitantes, na população residente em determinado espaço geográfico, no ano considerado (RIPSA 2008).\n\n\n\nDados de mortalidade: Sistema de Informação de Mortalidade (SIM), indexados na PCDaS\nEstimativas populacionais: DataSUS"
  },
  {
    "objectID": "data-projects/health_indicators/indi_0002.html#conceituação",
    "href": "data-projects/health_indicators/indi_0002.html#conceituação",
    "title": "Taxa de mortalidade específica por Acidente Vascular Cerebral",
    "section": "",
    "text": "Número de óbitos por Acidente Vascular Cerebral (AVC), por 100 mil habitantes, na população residente em determinado espaço geográfico, no ano considerado (RIPSA 2008).\n\n\n\nDados de mortalidade: Sistema de Informação de Mortalidade (SIM), indexados na PCDaS\nEstimativas populacionais: DataSUS"
  },
  {
    "objectID": "data-projects/health_indicators/indi_0002.html#agregações-computadas",
    "href": "data-projects/health_indicators/indi_0002.html#agregações-computadas",
    "title": "Taxa de mortalidade específica por Acidente Vascular Cerebral",
    "section": "Agregações computadas",
    "text": "Agregações computadas\n\nUF de residência, ano, mês e semana.\nRegião de saúde de residência, ano, mês e semana.\nMunicípio de residência, ano, mês e semana.\nAnos de 2010 a 2021"
  },
  {
    "objectID": "data-projects/health_indicators/indi_0002.html#download",
    "href": "data-projects/health_indicators/indi_0002.html#download",
    "title": "Taxa de mortalidade específica por Acidente Vascular Cerebral",
    "section": "Download",
    "text": "Download"
  },
  {
    "objectID": "data-projects/health_indicators/indi_0002.html#gráfico",
    "href": "data-projects/health_indicators/indi_0002.html#gráfico",
    "title": "Taxa de mortalidade específica por Acidente Vascular Cerebral",
    "section": "Gráfico",
    "text": "Gráfico\n\n\nCódigo\nlibrary(tidyverse)\nlibrary(arrow)\n\nnames_helper &lt;- tibble(\n  uf_code = c(\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"31\",\"32\",\"33\",\"35\",\"41\",\"42\",\"43\",\"50\",\"51\",\"52\",\"53\"),\n  uf_name = c(\"Rondônia\",\"Acre\",\"Amazonas\",\"Roraima\",\"Pará\",\"Amapá\",\"Tocantins\",\"Maranhão\",\"Piauí\",\"Ceará\",\"Rio Grande do Norte\",\"Paraíba\",\"Pernambuco\",\"Alagoas\",\"Sergipe\",\"Bahia\",\"Minas Gerais\",\"Espírito Santo\",\"Rio de Janeiro\",\"São Paulo\",\"Paraná\",\"Santa Catarina\",\"Rio Grande do Sul\",\"Mato Grosso do Sul\",\"Mato Grosso\",\"Goiás\",\"Distrito Federal\"),\n  cap_code7 = c(1100205,1200401,1302603,1400100,1501402,1600303,1721000,2111300,2211001,2304400,2408102,2507507,2611606,2704302,2800308,2927408,3106200,3205309,3304557,3550308,4106902,4205407,4314902,5002704,5103403,5208707,5300108),\n  cap_code6 = c(110020,120040,130260,140010,150140,160030,172100,211130,221100,230440,240810,250750,261160,270430,280030,292740,310620,320530,330455,355030,410690,420540,431490,500270,510340,520870,530010),\n  cap_name = c(\"Porto Velho\",\"Rio Branco\",\"Manaus\",\"Boa Vista\",\"Belém\",\"Macapá\",\"Palmas\",\"São Luís\",\"Teresina\",\"Fortaleza\",\"Natal\",\"João Pessoa\",\"Recife\",\"Maceió\",\"Aracaju\",\"Salvador\",\"Belo Horizonte\",\"Vitória\",\"Rio de Janeiro\",\"São Paulo\",\"Curitiba\",\"Florianópolis\",\"Porto Alegre\",\"Campo Grande\",\"Cuiabá\",\"Goiânia\",\"Brasília\")\n)\n\nread_parquet(\"health_indicators_data/indi_0002_uf_res_year.parquet\") |&gt;\n  filter(cod != 5) |&gt;\n  mutate(cod = as.character(cod)) |&gt;\n  left_join(names_helper, by = c(\"cod\" = \"uf_code\")) |&gt;\n  ggplot(aes(x = date, y = value, group = uf_name, color = uf_name)) + \n  geom_line() +\n  facet_wrap(~uf_name, ncol = 4) +\n  theme_bw() +\n  theme(legend.position = \"none\") +\n  labs(\n    title = \"Taxa de mortalidade específica por Acidente Vascular Cerebral\",\n    x = \"Ano\", y = \"Taxa\"\n  )"
  },
  {
    "objectID": "data-projects/health_indicators/indi_0002.html#computação",
    "href": "data-projects/health_indicators/indi_0002.html#computação",
    "title": "Taxa de mortalidade específica por Acidente Vascular Cerebral",
    "section": "Computação",
    "text": "Computação\n\n\nCódigo\n# UF of residence and year\nbrindi::expand_indi_parquet(\n  agg = \"uf_res\", agg_time = \"year\", \n  anos = years_range, \n  dir = \"health_indicators_data/\", \n  indi = \"indi_0002\"\n)\nbrindi::expand_indi_parquet(\n  agg = \"uf_res\", agg_time = \"month\", \n  anos = years_range, \n  dir = \"health_indicators_data/\", \n  indi = \"indi_0002\"\n)\nbrindi::expand_indi_parquet(\n  agg = \"uf_res\", agg_time = \"week\", \n  anos = years_range, \n  dir = \"health_indicators_data/\", \n  indi = \"indi_0002\"\n)\n\n# Health region of residence and year\nbrindi::expand_indi_parquet(\n  agg = \"regsaude_res\", agg_time = \"year\", \n  anos = years_range, \n  dir = \"health_indicators_data/\", \n  indi = \"indi_0002\"\n)\nbrindi::expand_indi_parquet(\n  agg = \"regsaude_res\", agg_time = \"month\", \n  anos = years_range, \n  dir = \"health_indicators_data/\", \n  indi = \"indi_0002\"\n)\nbrindi::expand_indi_parquet(\n  agg = \"regsaude_res\", agg_time = \"week\", \n  anos = years_range, \n  dir = \"health_indicators_data/\", \n  indi = \"indi_0002\"\n)\n\n# Municipality of residence and year\nbrindi::expand_indi_parquet(\n  agg = \"mun_res\", agg_time = \"year\", \n  anos = years_range, \n  dir = \"health_indicators_data/\", \n  indi = \"indi_0002\"\n)\nbrindi::expand_indi_parquet(\n  agg = \"mun_res\", agg_time = \"month\", \n  anos = years_range, \n  dir = \"health_indicators_data/\", \n  indi = \"indi_0002\"\n)\nbrindi::expand_indi_parquet(\n  agg = \"mun_res\", agg_time = \"week\", \n  anos = years_range, \n  dir = \"health_indicators_data/\", \n  indi = \"indi_0002\"\n)"
  },
  {
    "objectID": "data-projects/std_br_covid_rates_data.html",
    "href": "data-projects/std_br_covid_rates_data.html",
    "title": "Age-adjusted COVID-19 mortality rates for Brazilian municipalities",
    "section": "",
    "text": "This dataset present crude and age-adjusted COVID-19 mortality rates for Brazilian municipalities, from 2020 to 2022 per epidemiological weeks.\nMore details about the methodology are available at this blog post.\nDownload:\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Datasets",
    "section": "",
    "text": "S3 DataSUS FTP mirror\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZonal statistics of climate indicators for Brazilian municipalities\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClimatological normals and indicators of Brazilian municipalities\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nERA5-Land selected indicators daily aggregates for Latin America\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nERA5-Land selected indicators daily aggregates for Africa\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoad distances and trip duration matrix for Brazilian municipalities\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoad distances and trip duration matrix for USA counties\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHealth indicators\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAge-adjusted COVID-19 mortality rates for Brazilian municipalities\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "photos/photos.html",
    "href": "photos/photos.html",
    "title": "Photos",
    "section": "",
    "text": "Greater flamingo\n\n\n\n\n\nLittle Egret\n\n\n\n\n\nPhragmites australis\n\n\n\n\n\nWhite Stork"
  },
  {
    "objectID": "photos/photos.html#parc-naturel-du-mejean-lattes-fr",
    "href": "photos/photos.html#parc-naturel-du-mejean-lattes-fr",
    "title": "Photos",
    "section": "",
    "text": "Greater flamingo\n\n\n\n\n\nLittle Egret\n\n\n\n\n\nPhragmites australis\n\n\n\n\n\nWhite Stork"
  },
  {
    "objectID": "photos/photos.html#iceland",
    "href": "photos/photos.html#iceland",
    "title": "Photos",
    "section": "Iceland",
    "text": "Iceland\n\n\n\n\nKirkjufell\n\n\n\n\n\nBlack pebble beach of Djúpalónssandur\n\n\n\n\n\nArnarstapi\n\n\n\n\n\nBúðakirkja\n\n\n\n\n\nDiamond beach\n\n\n\n\n\nSkógafoss\n\n\n\n\n\nReykjanes Peninsula\n\n\n\n\n\nReykjanes Peninsula"
  },
  {
    "objectID": "photos/photos.html#montpellier-fr",
    "href": "photos/photos.html#montpellier-fr",
    "title": "Photos",
    "section": "Montpellier, FR",
    "text": "Montpellier, FR\n\n\n\n\nLe Peyrou\n\n\n\n\n\nArc de triomphe\n\n\n\n\n\nLes Arceaux\n\n\n\n\n\n\n\nSONY ILCE-6400 Lens E 70-350mm F4.5-6.3 G OSS  350mm f/6.3 1/2500 ISO 800  2024-02-11 08:21:16 UTC\n\n\nSONY ILCE-6400 Lens E 70-350mm F4.5-6.3 G OSS  350mm f/6.3 1/1000 ISO 500  2024-02-11 06:51:58 UTC\n\n\nSONY ILCE-6400 Lens E 70-350mm F4.5-6.3 G OSS  350mm f/6.3 1/2500 ISO 1250  2024-02-11 08:00:08 UTC\n\n\nSONY ILCE-6400 Lens E 70-350mm F4.5-6.3 G OSS  350mm f/6.3 1/2500 ISO 640  2024-02-11 08:54:53 UTC\n\n\nSONY ILCE-6400 Lens 16mm F1.4 DC DN | Contemporary 017  16mm f/1.4 1/1000 ISO 3200  2024-01-09 09:29:01 UTC\n\n\nSONY ILCE-6400 Lens 16mm F1.4 DC DN | Contemporary 017  16mm f/1.4 1/1000 ISO 640  2024-01-09 11:11:06 UTC\n\n\nSONY ILCE-6400 Lens 16mm F1.4 DC DN | Contemporary 017  16mm f/1.4 1/800 ISO 400  2024-01-09 12:14:14 UTC\n\n\nSONY ILCE-6400 Lens 16mm F1.4 DC DN | Contemporary 017  16mm f/1.4 1/800 ISO 320  2024-01-09 12:54:17 UTC\n\n\nSONY ILCE-6400 Lens E PZ 16-50mm F3.5-5.6 OSS  49mm f/5.6 1/640 ISO 400  2024-01-10 11:04:13 UTC\n\n\nSONY ILCE-6400 Lens 16mm F1.4 DC DN | Contemporary 017  16mm f/5.6 1/1 ISO 100  2024-01-11 08:47:17 UTC\n\n\nSONY ILCE-6400 Lens 16mm F1.4 DC DN | Contemporary 017  16mm f/2 1/100 ISO 100  2024-01-13 08:35:43 UTC\n\n\nSONY ILCE-6400 Lens 16mm F1.4 DC DN | Contemporary 017  16mm f/1.8 1/100 ISO 100  2024-01-13 08:39:19 UTC\n\n\nSONY ILCE-6400 Lens E PZ 16-50mm F3.5-5.6 OSS  22mm f/5.6 1/800 ISO 100  2023-11-05 09:00:50 UTC\n\n\nSONY ILCE-6400 Lens E PZ 16-50mm F3.5-5.6 OSS  16mm f/9 1/200 ISO 100  2023-11-05 08:51:52 UTC\n\n\nSONY ILCE-6400 Lens E PZ 16-50mm F3.5-5.6 OSS  34mm f/16 1/60 ISO 100  2023-11-05 09:12:28 UTC"
  },
  {
    "objectID": "posts/hospital_admissions_flow.html",
    "href": "posts/hospital_admissions_flow.html",
    "title": "Handling 187 millions hospital admissions in Brazil with DuckDB",
    "section": "",
    "text": "On ideal circumstances, any hospital admission would take place at the same city of residence of the patient. This facilitates the patient and family dislocation to the hospital, staying in a more familiar surrounding, and making the whole process less stressful. But, to a management point of view, this requires that all cities equal present a complete and extremely expensive set of hospital units, able to take care from simple diagnosis to complex organs transfusions\nThus, a national health system is usually organized under hierarchy and centralization principles. On this setting, smaller cities with less population have more simple resources and bigger cities have more complex resources at disposal. This administrative optimization comes with a cost: patients will likely need to travel to another city in order to receive a more complex health treatment, creating a flow of patients seeking for health care.\nWe will study here the flow of patients for hospital admissions using Brazilian datasets from its Universal Health System (SUS). This data is originally from the DataSUS, the informatics department of the Brazilian Health Ministry."
  },
  {
    "objectID": "posts/hospital_admissions_flow.html#dataset",
    "href": "posts/hospital_admissions_flow.html#dataset",
    "title": "Handling 187 millions hospital admissions in Brazil with DuckDB",
    "section": "Dataset",
    "text": "Dataset\nWe will use the Hospital Admissions data prepared by the PCDaS/Fiocruz with the original DataSUS data. This dataset is already cleaned, enriched and fully documented. I downloaded the CSV version, which contains 5,210 files, totaling 315.7 GB.\nOn these files, each row represents a hospital admission, and several variables of interest are available, including the patient’s municipality code of residence, the hospital’s code municipality and the date of the hospital admission.\nIt is a lot of data to process, making it very unpractical to just load the files into the computer memory.\nThe DuckDB database is very interesting for this case:\n\nI will not need to create or have access to a database server, the DuckDB database is a just a file on your computer.\nDuckDB have dedicated functions to parse and import CSV files straight to the database.\nIt is very fast for aggregate and other analytical functions that need to access all rows.\nIt is seamless integrated with R and {dplyr} verbs."
  },
  {
    "objectID": "posts/hospital_admissions_flow.html#database-creation",
    "href": "posts/hospital_admissions_flow.html#database-creation",
    "title": "Handling 187 millions hospital admissions in Brazil with DuckDB",
    "section": "Database creation",
    "text": "Database creation\nFirst, we need to call some packages and create the database and create an empty table on it.\n\nlibrary(duckdb)\nlibrary(duckplyr)\nlibrary(glue)\n\n\n1con &lt;- dbConnect(duckdb(), dbdir = \"pcdas_sih.duckdb\", read_only = FALSE)\n\ndbExecute(con, \"CREATE TABLE sih (\n    cod_res VARCHAR,\n    cod_int VARCHAR,\n    dt_inter DATE,\n    def_ident VARCHAR,\n    def_car_int VARCHAR\n2)\")\n\n\n1\n\nCreate an empty database on the computer.\n\n2\n\nCreate the sih table with a schema.\n\n\n\n\nNow, we will populate the sih table with a loop.\n\nyears &lt;- 2008:2023\n\nfor(y in years){\n  message(y)\n  query &lt;- glue(\"INSERT INTO sih \n          SELECT res_codigo_adotado AS cod_res, \n          int_MUNCOD AS cod_int, \n          DT_INTER AS dt_inter, def_ident, def_car_int \n          FROM read_csv('/media/raphael/lacie/pcdas_sih/csv/*[y]*.csv',\n          delim = ',',\n          header = true,\n          dateformat = '%Y%m%d',\n          types = {'res_codigo_adotado': 'VARCHAR',\n          'int_MUNCOD': 'VARCHAR',\n          'dt_inter': 'DATE',\n          'def_ident': 'VARCHAR',\n          'def_car_int': 'VARCHAR'},\n          union_by_name = true\n          )\", .open = \"[\", .close = \"]\")\n  \n  dbExecute(con, query)\n}\n\ndbDisconnect(con)\n\nThe query may seem a little complicated, but it is simple if we look at it by parts:\n\nFor each year y from 2008 to 2023…\nUsing the connection con, INSERT INTO the table sih the selected columns (changing its original names) FROM the csv files. This is achieved with the DuckDB’s function read_csv. Here, DuckDB will parse the CSV files contents that have the year y on its name ([y].csv).\n\n\n\n\n\n\n\nNote\n\n\n\nI tried to read all 5,210 files at once, but I received messages about “Too many files open”. This loop required fewer files to be open at the same time, being very effective.\n\n\n\n\n\n\n\n\nTip\n\n\n\nOne nice thing here is that we are reading into our table only the variables we want from the CSV files, reducing the database size and saving time.\n\n\nAfter importing all CSV files, the database occupies 829MB on disk and the sih table have 187,735,977 rows."
  },
  {
    "objectID": "posts/hospital_admissions_flow.html#flow-computation",
    "href": "posts/hospital_admissions_flow.html#flow-computation",
    "title": "Handling 187 millions hospital admissions in Brazil with DuckDB",
    "section": "Flow computation",
    "text": "Flow computation\nLet’s compute the yearly flow of patients starting in 2008.\nTo compute the amount of patients that goes from a municipality to another on one year, we will do a grouping operation and observe the number of rows on each group.\n\n1con &lt;- dbConnect(duckdb(), dbdir = \"../../flowsus/pcdas_sih.duckdb\", read_only = FALSE)\n\n2sih_tbl &lt;- dplyr::tbl(con, \"sih\")\n\n3res_geral &lt;- sih_tbl |&gt;\n4  filter(def_ident == \"Normal\") |&gt;\n  filter(year(dt_inter) &gt;= 2008 & year(dt_inter) &lt;= 2023) |&gt;\n5  mutate(year = year(dt_inter)) |&gt;\n  summarise( #&lt;56\n6    weight = n(),\n    .by = c(year, cod_res, cod_int)\n  ) |&gt;\n7  collect()\n\n8dbDisconnect(con)\n\n\n1\n\nConnect with the database on read-only mode.\n\n2\n\nCreate a virtual connection to the sih table.\n\n3\n\nres_geral will be the object that will receive the results.\n\n4\n\nFilter the hospital admissions, considering only the typical admissions (this excludes long-stays admissions, like mental healthcare) and filter hospital admissions that took place between 2008 and 2023.\n\n5\n\nCreate a new variable: the year of the hospital admission.\n\n6\n\nSummarize the table, by year and municipality of residence and destination, creating a weight variable that will receive the amount with rows.\n\n7\n\nExecutes the query on the DuckDB database, returning a tibble ready-to-use.\n\n8\n\nDisconnect the database.\n\n\n\n\nThe res_geral tibble present 1,395,512 rows.\n\nhead(res_geral)\n\n# A tibble: 6 × 4\n   year cod_res cod_int weight\n  &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;    &lt;dbl&gt;\n1  2008 292060  292250     100\n2  2008 290323  290323     757\n3  2008 292910  292250       5\n4  2008 292260  291120      81\n5  2008 291690  291760      36\n6  2008 292340  292340    1327\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere are rows where the municipality of origin and destiny are the same. This represents the hospital admissions that took place at the same municipality of residence of the patient.\n\n\nDiscarding these loop-cases, the biggest observed flow occurred on 2021, when 28,375 hospital admissions occurred with patients from Jaboatão dos Guararapes, PE being admitted to hospitals from Recife, PE\n\nres_geral |&gt; \n  filter(cod_res != cod_int) |&gt;\n  arrange(-weight) |&gt;\n  head(10)\n\n# A tibble: 10 × 4\n    year cod_res cod_int weight\n   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;    &lt;dbl&gt;\n 1  2021 260790  261160   28375\n 2  2023 260790  261160   28329\n 3  2022 260790  261160   28142\n 4  2019 260790  261160   27268\n 5  2017 260790  261160   27222\n 6  2018 260790  261160   27025\n 7  2016 260790  261160   25254\n 8  2020 260790  261160   24134\n 9  2015 260790  261160   23970\n10  2013 260790  261160   23718\n\n\nIn total, 58,216,831 hospital admissions in Brazil occurred outside the patient’s residence municipality between 2008 and 2023.\n\nres_geral |&gt; \n  filter(cod_res != cod_int) |&gt;\n  pull(weight) |&gt;\n  sum()\n\n[1] 58216831\n\n\nAnd 124,835,812 hospital admissions in Brazil occurred at the same patient’s residence municipality between 2008 and 2023.\n\nres_geral |&gt; \n  filter(cod_res == cod_int) |&gt;\n  pull(weight) |&gt;\n  sum()\n\n[1] 124835812"
  },
  {
    "objectID": "posts/hospital_admissions_flow.html#map",
    "href": "posts/hospital_admissions_flow.html#map",
    "title": "Handling 187 millions hospital admissions in Brazil with DuckDB",
    "section": "Map",
    "text": "Map\nThe flow of patients deserves a map! Let’s take a look at the last year available (2023).\nWe will need more packages.\n\nlibrary(dplyr)\nlibrary(geobr)\nlibrary(sf)\nlibrary(edgebundle)\nlibrary(igraph)\nlibrary(ggplot2)\n\nFor the map, we will need the geographical coordinates of the municipalities and the state boundaries. The geobr package is very handy for this.\n\nseats &lt;- read_municipal_seat(showProgress = FALSE) |&gt;\n  mutate(code_muni = substr(code_muni, 0, 6))\n\nUsing year/date 2010\n\nstates &lt;- read_state(showProgress = FALSE)\n\nUsing year/date 2010\n\n\nPrepare the municipal seats database.\n\nseats &lt;- seats |&gt;\n  mutate(code_muni = substr(code_muni, 0, 6)) |&gt;\n  mutate(longitude = st_coordinates(seats)[,1],\n         latitude = st_coordinates(seats)[,2]) |&gt;\n  st_drop_geometry() |&gt;\n  select(code_muni, longitude, latitude)\n\nhead(seats)\n\n  code_muni longitude   latitude\n1    110001 -61.99982 -11.935540\n2    110002 -63.03327  -9.908463\n3    110003 -60.54431 -13.499763\n4    110004 -61.44294 -11.433865\n5    110005 -60.81843 -13.195033\n6    110006 -60.55507 -13.130564\n\n\nAnd prepare the flow data for 2023. I will remove flows with less than 10 hospital admissions.\n\nres_2023 &lt;- res_geral |&gt;\n  filter(weight &gt;= 10) |&gt;\n  filter(cod_res %in% seats$code_muni & cod_int %in% seats$code_muni) |&gt;\n  filter(cod_res != cod_int) |&gt;\n  filter(year == 2023) |&gt;\n  select(2:4)\n\nhead(res_2023)\n\n# A tibble: 6 × 3\n  cod_res cod_int weight\n  &lt;chr&gt;   &lt;chr&gt;    &lt;dbl&gt;\n1 291990  292400      91\n2 293060  292720      30\n3 290687  292720      88\n4 291125  292720      16\n5 292045  292840      44\n6 292240  292870     565\n\n\nNow, we will create an igraph object to represent the network.\n\ng_2023 &lt;- graph_from_data_frame(d = res_2023, directed = TRUE, vertices = seats)\n\nAnd pairs of coordinates and vertices objects.\n\nxy_2023 &lt;- cbind(V(g_2023)$longitude, V(g_2023)$latitude)\nverts_2023 &lt;- data.frame(x = V(g_2023)$longitude, y = V(g_2023)$latitude)\n\nTo plot the vertices, we will use a nice function from the edgebundle package. This may take some time to be computed.\n\npbundle_2023 &lt;- edge_bundle_path(g_2023, xy_2023, max_distortion = 12, weight_fac = 2, segments = 50)\n\npbundle_2023 &lt;- pbundle_2023 %&gt;% \n  left_join(res_2023 |&gt; mutate(id = row_number()) |&gt; select(c(id, weight)), by=c('group' = 'id')) |&gt; \n  st_as_sf(coords=c('x', 'y'), crs=4326) |&gt;\n  group_by(group) |&gt;\n  summarise(weight=mean(weight), do_union=FALSE) |&gt;\n  arrange(weight) |&gt;\n  st_cast(\"LINESTRING\")\n\nNow, all needed objects are ready. Let’s plot it.\n\nggplot() +\n  geom_sf(\n    data = states,\n    col = \"white\", linewidth = 0.1, fill = NA\n  ) +\n  geom_sf(\n    data = pbundle_2023, aes(group = group, linewidth = log(weight), color = log(weight), alpha = 0.5),\n  ) +\n  scale_linewidth(range = c(0.001, .2)) +\n  scale_colour_gradient(low = \"#3d0038\", high = \"#f993f1\") +\n  geom_point(\n    data = verts_2023, aes(x, y),\n    stroke = 0,\n    col = \"white\", size = 0.1, alpha = 0.3\n  ) +\n  labs(title = \"Patient's flow for hospital admission, 2023\") +\n  ggraph::theme_graph(background = \"black\") +\n  theme(plot.title = element_text(color = \"white\"), legend.position = \"none\")\n\n\n\n\n\n\n\n\nYou can observe that some of the patient’s flow are its state of residence. On those cases, the patient is likely traveling to the state capital or to some big city within the state to receive healthcare. But the states size in Brazil varies a lot: the Amazonas state, by example, has an area bigger than Spain and France together. Also, other kind of flow occurs between states, typically from the interior or capital city from one state to the capital or regional center of the other state.\nOne important thing to observe is that some of those flows are expected, like the case where patients need to be admitted by hospitals with very specific capacities due to the patient’s disease and conditions.\nThe analysis of patient flows in a health system is very interesting and can guide health managers to better understand and organize the health system. I published with some colleagues on this theme some time ago ((Saldanha et al. 2019), (Xavier et al. 2019) and (Fonseca et al. 2022))."
  },
  {
    "objectID": "posts/hospital_admissions_flow.html#session-info",
    "href": "posts/hospital_admissions_flow.html#session-info",
    "title": "Handling 187 millions hospital admissions in Brazil with DuckDB",
    "section": "Session info",
    "text": "Session info\n\nsessioninfo::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.4.0 (2024-04-24)\n os       Ubuntu 22.04.4 LTS\n system   x86_64, linux-gnu\n ui       X11\n language en_US:en\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Paris\n date     2024-06-12\n pandoc   3.1.11 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/x86_64/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package      * version date (UTC) lib source\n blob           1.2.4   2023-03-17 [1] CRAN (R 4.4.0)\n cachem         1.1.0   2024-05-16 [1] CRAN (R 4.4.0)\n class          7.3-22  2023-05-03 [4] CRAN (R 4.3.1)\n classInt       0.4-10  2023-09-05 [1] CRAN (R 4.4.0)\n cli            3.6.2   2023-12-11 [1] CRAN (R 4.4.0)\n collections    0.3.7   2023-01-05 [1] CRAN (R 4.4.0)\n colorspace     2.1-0   2023-01-23 [1] CRAN (R 4.4.0)\n curl           5.2.1   2024-03-01 [1] CRAN (R 4.4.0)\n data.table     1.15.4  2024-03-30 [1] CRAN (R 4.4.0)\n DBI          * 1.2.3   2024-06-02 [1] CRAN (R 4.4.0)\n dbplyr         2.5.0   2024-03-19 [1] CRAN (R 4.4.0)\n digest         0.6.35  2024-03-11 [1] CRAN (R 4.4.0)\n dplyr        * 1.1.4   2023-11-17 [1] CRAN (R 4.4.0)\n duckdb       * 0.10.2  2024-05-01 [1] CRAN (R 4.4.0)\n duckplyr     * 0.4.0   2024-05-21 [1] CRAN (R 4.4.0)\n e1071          1.7-14  2023-12-06 [1] CRAN (R 4.4.0)\n edgebundle   * 0.4.2   2023-12-16 [1] CRAN (R 4.4.0)\n evaluate       0.24.0  2024-06-10 [1] CRAN (R 4.4.0)\n fansi          1.0.6   2023-12-08 [1] CRAN (R 4.4.0)\n farver         2.1.2   2024-05-13 [1] CRAN (R 4.4.0)\n fastmap        1.2.0   2024-05-15 [1] CRAN (R 4.4.0)\n generics       0.1.3   2022-07-05 [1] CRAN (R 4.4.0)\n geobr        * 1.9.0   2024-04-18 [1] CRAN (R 4.4.0)\n ggforce        0.4.2   2024-02-19 [1] CRAN (R 4.4.0)\n ggplot2      * 3.5.1   2024-04-23 [1] CRAN (R 4.4.0)\n ggraph         2.2.1   2024-03-07 [1] CRAN (R 4.4.0)\n ggrepel        0.9.5   2024-01-10 [1] CRAN (R 4.4.0)\n glue         * 1.7.0   2024-01-09 [1] CRAN (R 4.4.0)\n graphlayouts   1.1.1   2024-03-09 [1] CRAN (R 4.4.0)\n gridExtra      2.3     2017-09-09 [1] CRAN (R 4.4.0)\n gtable         0.3.5   2024-04-22 [1] CRAN (R 4.4.0)\n htmltools      0.5.8.1 2024-04-04 [1] CRAN (R 4.4.0)\n htmlwidgets    1.6.4   2023-12-06 [1] CRAN (R 4.4.0)\n httr           1.4.7   2023-08-15 [1] CRAN (R 4.4.0)\n igraph       * 2.0.3   2024-03-13 [1] CRAN (R 4.4.0)\n jsonlite       1.8.8   2023-12-04 [1] CRAN (R 4.4.0)\n KernSmooth     2.23-24 2024-05-17 [4] CRAN (R 4.4.0)\n knitr          1.47    2024-05-29 [1] CRAN (R 4.4.0)\n labeling       0.4.3   2023-08-29 [1] CRAN (R 4.4.0)\n lattice        0.22-5  2023-10-24 [4] CRAN (R 4.3.1)\n lifecycle      1.0.4   2023-11-07 [1] CRAN (R 4.4.0)\n magrittr       2.0.3   2022-03-30 [1] CRAN (R 4.4.0)\n MASS           7.3-60  2023-05-04 [4] CRAN (R 4.3.1)\n Matrix         1.6-5   2024-01-11 [4] CRAN (R 4.3.3)\n memoise        2.0.1   2021-11-26 [1] CRAN (R 4.4.0)\n munsell        0.5.1   2024-04-01 [1] CRAN (R 4.4.0)\n pillar         1.9.0   2023-03-22 [1] CRAN (R 4.4.0)\n pkgconfig      2.0.3   2019-09-22 [1] CRAN (R 4.4.0)\n png            0.1-8   2022-11-29 [1] CRAN (R 4.4.0)\n polyclip       1.10-6  2023-09-27 [1] CRAN (R 4.4.0)\n proxy          0.4-27  2022-06-09 [1] CRAN (R 4.4.0)\n purrr          1.0.2   2023-08-10 [1] CRAN (R 4.4.0)\n R6             2.5.1   2021-08-19 [1] CRAN (R 4.4.0)\n Rcpp           1.0.12  2024-01-09 [1] CRAN (R 4.4.0)\n reticulate     1.37.0  2024-05-21 [1] CRAN (R 4.4.0)\n rlang          1.1.4   2024-06-04 [1] CRAN (R 4.4.0)\n rmarkdown      2.27    2024-05-17 [1] CRAN (R 4.4.0)\n rstudioapi     0.16.0  2024-03-24 [1] CRAN (R 4.4.0)\n scales         1.3.0   2023-11-28 [1] CRAN (R 4.4.0)\n sessioninfo    1.2.2   2021-12-06 [1] CRAN (R 4.4.0)\n sf           * 1.0-16  2024-03-24 [1] CRAN (R 4.4.0)\n tibble         3.2.1   2023-03-20 [1] CRAN (R 4.4.0)\n tidygraph      1.3.1   2024-01-30 [1] CRAN (R 4.4.0)\n tidyr          1.3.1   2024-01-24 [1] CRAN (R 4.4.0)\n tidyselect     1.2.1   2024-03-11 [1] CRAN (R 4.4.0)\n tweenr         2.0.3   2024-02-26 [1] CRAN (R 4.4.0)\n units          0.8-5   2023-11-28 [1] CRAN (R 4.4.0)\n utf8           1.2.4   2023-10-22 [1] CRAN (R 4.4.0)\n vctrs          0.6.5   2023-12-01 [1] CRAN (R 4.4.0)\n viridis        0.6.5   2024-01-29 [1] CRAN (R 4.4.0)\n viridisLite    0.4.2   2023-05-02 [1] CRAN (R 4.4.0)\n withr          3.0.0   2024-01-16 [1] CRAN (R 4.4.0)\n xfun           0.44    2024-05-15 [1] CRAN (R 4.4.0)\n yaml           2.3.8   2023-12-11 [1] CRAN (R 4.4.0)\n\n [1] /home/raphael/R/x86_64-pc-linux-gnu-library/4.4\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/persistent_heat.html",
    "href": "posts/persistent_heat.html",
    "title": "Persistent heat in Brazil: 1993 and 2023",
    "section": "",
    "text": "Persistent heat, or heat waves, are defined as sequences of days with temperatures above a reference value. These sequences of days of extreme heat are direct consequences of the global warming and impact the health of the inhabitants, specially those with less access to basic resources.\nLet’s take a look at the occurrence of persistent heat in Brazilian municipalities in two years: 2023 and 1993 (thirty years ago).\nFor this, I will use the datasets of Zonal Statistics of Climate Indicators created with ERA5-Land data. More details about this dataset is available here."
  },
  {
    "objectID": "posts/persistent_heat.html#packages",
    "href": "posts/persistent_heat.html#packages",
    "title": "Persistent heat in Brazil: 1993 and 2023",
    "section": "Packages",
    "text": "Packages\n\n1library(tidyverse)\n2library(arrow)\n3library(zendown)\n4library(nseq)\n\n\n1\n\nFunctions for data manipulation.\n\n2\n\nFunctions to work with parquet files.\n\n3\n\nDownload and cache data from Zenodo. More details about this package here.\n\n4\n\nCompute sequences of events. More details about this package here."
  },
  {
    "objectID": "posts/persistent_heat.html#data-download",
    "href": "posts/persistent_heat.html#data-download",
    "title": "Persistent heat in Brazil: 1993 and 2023",
    "section": "Data download",
    "text": "Data download\nWe will use data about the daily average maximum temperature on Brazilian municipalities.\n\n2023 data\n\n1temp_max_2023 &lt;- zen_file(10947952, \"2m_temperature_max.parquet\") |&gt;\n2  open_dataset() |&gt;\n3  filter(name == \"2m_temperature_max_mean\") |&gt;\n  select(-name) |&gt;\n4  mutate(value = value - 272.15) |&gt;\n5  arrange(code_muni, date) |&gt;\n6  collect()\n\n\n1\n\nDownload and cache parquet file from Zenodo.\n\n2\n\nCreates a connection to the file, but do not load it to the memory. This file is pretty big.\n\n3\n\nKeep only the average maximum temperature indicator.\n\n4\n\nConvert from Kelvin to Celsius.\n\n5\n\nArrange rows by municipality code and date.\n\n6\n\nCollect the filtered data to memory.\n\n\n\n\n\n\n1993 data\n\ntemp_max_1993 &lt;- zen_file(10036212, \"2m_temperature_max.parquet\") |&gt;\n  open_dataset() |&gt;\n  filter(name == \"2m_temperature_max_mean\") |&gt;\n  select(-name) |&gt; \n  filter(year(date) == 1993) |&gt;\n  mutate(value = value - 272.15) |&gt;\n  arrange(code_muni, date) |&gt;\n  collect()\n\nThe steps are basically the same, but we filter the 1993 data."
  },
  {
    "objectID": "posts/persistent_heat.html#persistent-heat-sequences",
    "href": "posts/persistent_heat.html#persistent-heat-sequences",
    "title": "Persistent heat in Brazil: 1993 and 2023",
    "section": "Persistent heat sequences",
    "text": "Persistent heat sequences\nLet’s compute some persistent heat sequences for all municipalities using the dplyr::summary and the nseq::trle_cond functions.\n\nres_2023 &lt;- temp_max_2023 |&gt;\n  summarise(\n    temp_3_35 = trle_cond(value, a_op = \"gte\", a = 3, b_op = \"gte\", b = 35),\n    temp_5_35 = trle_cond(value, a_op = \"gte\", a = 5, b_op = \"gte\", b = 35),\n    temp_3_40 = trle_cond(value, a_op = \"gte\", a = 3, b_op = \"gte\", b = 40),\n    temp_5_40 = trle_cond(value, a_op = \"gte\", a = 5, b_op = \"gte\", b = 40),\n    .by = code_muni\n  )\n\nFor the year of 2023, on each municipality, we are answering these questions:\n\nHow many times we got sequences of 3 days or more with temperatures of 35 Celsius or more?\nHow many times we got sequences of 5 days or more with temperatures of 35 Celsius or more?\nHow many times we got sequences of 3 days or more with temperatures of 40 Celsius or more?\nHow many times we got sequences of 5 days or more with temperatures of 40 Celsius or more?\n\nLet’s do the same with the 1993 data.\n\nres_1993 &lt;- temp_max_1993 |&gt;\n  summarise(\n    temp_3_35 = trle_cond(value, a_op = \"gte\", a = 3, b_op = \"gte\", b = 35),\n    temp_5_35 = trle_cond(value, a_op = \"gte\", a = 5, b_op = \"gte\", b = 35),\n    temp_3_40 = trle_cond(value, a_op = \"gte\", a = 3, b_op = \"gte\", b = 40),\n    temp_5_40 = trle_cond(value, a_op = \"gte\", a = 5, b_op = \"gte\", b = 40),\n    .by = code_muni\n  )\n\nLet’s take a look first at the more extreme result: five days or more with temperatures above 40 Celsius degrees.\nOn 2023, 14 municipalities presented those conditions. This conditions of extreme heat occurred 4 times at the municipality of Barão de Melgaço, MT. On 1993, these conditions did not occur on any municipality."
  },
  {
    "objectID": "posts/persistent_heat.html#maps",
    "href": "posts/persistent_heat.html#maps",
    "title": "Persistent heat in Brazil: 1993 and 2023",
    "section": "Maps",
    "text": "Maps\n\nlibrary(geobr)\nlibrary(sf)\nlibrary(viridisLite)\n\nuf &lt;- read_state(showProgress = FALSE)\ncoords &lt;- read_municipality(showProgress = FALSE) %&gt;%\n  st_make_valid() %&gt;%\n  st_centroid()\n\n\nres_2023_map &lt;- left_join(coords, res_2023, by = \"code_muni\") |&gt;\n  drop_na()\n\nres_1993_map &lt;- left_join(coords, res_1993, by = \"code_muni\") |&gt;\n  drop_na()\n\n\nggplot() +\n  geom_sf(data = uf, fill = \"grey30\", color = \"grey50\", size=.15, show.legend = FALSE) +\n  geom_sf(data = subset(res_2023_map, temp_3_35 &gt; 0), aes(color = temp_3_35), size = 1, alpha = 0.5) +\n  scale_colour_viridis_c(option = \"turbo\") +\n  theme_minimal() +\n  labs(title = \"Número de ocorrências de 3 dias ou mais \", subtitle = \"com temperatura máxima média acima de 35 graus Celsius em 2023\", color = NULL)\n\n\n\n\n\n\n\n\n\nggplot() +\n  geom_sf(data = uf, fill = \"grey30\", color = \"grey50\", size=.15, show.legend = FALSE) +\n  geom_sf(data = subset(res_1993_map, temp_3_35 &gt; 0), aes(color = temp_3_35), size = 1, alpha = 0.5) +\n  scale_colour_viridis_c(option = \"turbo\") +\n  theme_minimal() +\n  labs(title = \"Número de ocorrências de 3 dias ou mais \", subtitle = \"com temperatura máxima média acima de 35 graus Celsius em 1993\", color = NULL)\n\n\n\n\n\n\n\n\n\nggplot() +\n  geom_sf(data = uf, fill = \"grey30\", color = \"grey50\", size=.15, show.legend = FALSE) +\n  geom_sf(data = subset(res_2023_map, temp_5_35 &gt; 0), aes(color = temp_5_35), size = 1, alpha = 0.5) +\n  scale_colour_viridis_c(option = \"turbo\") +\n  theme_minimal() +\n  labs(title = \"Número de ocorrências de 5 dias ou mais\", subtitle = \"com temperatura máxima média acima de 35 graus Celsius em 2023\", color = NULL)\n\n\n\n\n\n\n\n\n\nggplot() +\n  geom_sf(data = uf, fill = \"grey30\", color = \"grey50\", size=.15, show.legend = FALSE) +\n  geom_sf(data = subset(res_1993_map, temp_5_35 &gt; 0), aes(color = temp_5_35), size = 1, alpha = 0.5) +\n  scale_colour_viridis_c(option = \"turbo\") +\n  theme_minimal() +\n  labs(title = \"Número de ocorrências de 5 dias ou mais\", subtitle = \"com temperatura máxima média acima de 35 graus Celsius em 1993\", color = NULL)\n\n\n\n\n\n\n\n\n\nggplot() +\n  geom_sf(data = uf, fill = \"grey30\", color = \"grey50\", size=.15, show.legend = FALSE) +\n  geom_sf(data = subset(res_2023_map, temp_3_40 &gt; 0), aes(color = temp_3_40), size = 1, alpha = 0.5) +\n  scale_colour_viridis_c(option = \"turbo\") +\n  theme_minimal() +\n  labs(title = \"Número de ocorrências de 3 dias ou mais\", subtitle = \"com temperatura máxima média acima de 40 graus Celsius em 2023\", color = NULL)\n\n\n\n\n\n\n\n\n\nggplot() +\n  geom_sf(data = uf, fill = \"grey30\", color = \"grey50\", size=.15, show.legend = FALSE) +\n  geom_sf(data = subset(res_1993_map, temp_3_40 &gt; 0), aes(color = temp_3_40), size = 1, alpha = 0.5) +\n  scale_colour_viridis_c(option = \"turbo\") +\n  theme_minimal() +\n  labs(title = \"Número de ocorrências de 3 dias ou mais\", subtitle = \"com temperatura máxima média acima de 40 graus Celsius em 1993\", color = NULL)\n\n\n\n\n\n\n\n\n\nggplot() +\n  geom_sf(data = uf, fill = \"grey30\", color = \"grey50\", size=.15, show.legend = FALSE) +\n  geom_sf(data = subset(res_2023_map, temp_5_40 &gt; 0), aes(color = temp_5_40), size = 1, alpha = 0.5) +\n  scale_colour_viridis_c(option = \"turbo\") +\n  theme_minimal() +\n  labs(title = \"Número de ocorrências de 5 dias ou mais\", subtitle = \"com temperatura máxima média acima de 40 graus Celsius em 2023\", color = NULL)\n\n\n\n\n\n\n\n\n\nggplot() +\n  geom_sf(data = uf, fill = \"grey30\", color = \"grey50\", size=.15, show.legend = FALSE) +\n  geom_sf(data = subset(res_1993_map, temp_5_40 &gt; 0), aes(color = temp_5_40), size = 1, alpha = 0.5) +\n  scale_colour_viridis_c(option = \"turbo\") +\n  theme_minimal() +\n  labs(title = \"Número de ocorrências de 5 dias ou mais\", subtitle = \"com temperatura máxima média acima de 40 graus Celsius em 1993\", color = NULL)"
  },
  {
    "objectID": "posts/persistent_heat.html#session-info",
    "href": "posts/persistent_heat.html#session-info",
    "title": "Persistent heat in Brazil: 1993 and 2023",
    "section": "Session info",
    "text": "Session info\n\nsessioninfo::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.3 (2024-02-29)\n os       Ubuntu 22.04.4 LTS\n system   x86_64, linux-gnu\n ui       X11\n language en_US:en\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Paris\n date     2024-04-22\n pandoc   3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n arrow       * 15.0.1  2024-03-12 [1] CRAN (R 4.3.3)\n assertthat    0.2.1   2019-03-21 [1] CRAN (R 4.3.1)\n backports     1.4.1   2021-12-13 [1] CRAN (R 4.3.1)\n bit           4.0.5   2022-11-15 [1] CRAN (R 4.3.1)\n bit64         4.0.5   2020-08-30 [1] CRAN (R 4.3.1)\n checkmate     2.3.1   2023-12-04 [1] CRAN (R 4.3.2)\n class         7.3-22  2023-05-03 [4] CRAN (R 4.3.1)\n classInt      0.4-10  2023-09-05 [1] CRAN (R 4.3.1)\n cli           3.6.2   2023-12-11 [1] CRAN (R 4.3.2)\n colorspace    2.1-0   2023-01-23 [1] CRAN (R 4.3.1)\n curl          5.2.1   2024-03-01 [1] CRAN (R 4.3.3)\n data.table    1.15.4  2024-03-30 [1] CRAN (R 4.3.3)\n DBI           1.2.2   2024-02-16 [1] CRAN (R 4.3.2)\n digest        0.6.35  2024-03-11 [1] CRAN (R 4.3.3)\n dplyr       * 1.1.4   2023-11-17 [1] CRAN (R 4.3.2)\n e1071         1.7-14  2023-12-06 [1] CRAN (R 4.3.2)\n evaluate      0.23    2023-11-01 [1] CRAN (R 4.3.1)\n fansi         1.0.6   2023-12-08 [1] CRAN (R 4.3.2)\n farver        2.1.1   2022-07-06 [1] CRAN (R 4.3.1)\n fastmap       1.1.1   2023-02-24 [1] CRAN (R 4.3.1)\n forcats     * 1.0.0   2023-01-29 [1] CRAN (R 4.3.1)\n fs            1.6.3   2023-07-20 [1] CRAN (R 4.3.1)\n generics      0.1.3   2022-07-05 [1] CRAN (R 4.3.1)\n geobr       * 1.9.0   2024-04-18 [1] CRAN (R 4.3.3)\n ggplot2     * 3.5.0   2024-02-23 [1] CRAN (R 4.3.2)\n glue          1.7.0   2024-01-09 [1] CRAN (R 4.3.2)\n gtable        0.3.4   2023-08-21 [1] CRAN (R 4.3.1)\n hms           1.1.3   2023-03-21 [1] CRAN (R 4.3.1)\n htmltools     0.5.8.1 2024-04-04 [1] CRAN (R 4.3.3)\n htmlwidgets   1.6.4   2023-12-06 [1] CRAN (R 4.3.2)\n httr          1.4.7   2023-08-15 [1] CRAN (R 4.3.1)\n jsonlite      1.8.8   2023-12-04 [1] CRAN (R 4.3.2)\n KernSmooth    2.23-22 2023-07-10 [4] CRAN (R 4.3.1)\n knitr         1.46    2024-04-06 [1] CRAN (R 4.3.3)\n labeling      0.4.3   2023-08-29 [1] CRAN (R 4.3.1)\n lifecycle     1.0.4   2023-11-07 [1] CRAN (R 4.3.2)\n lubridate   * 1.9.3   2023-09-27 [1] CRAN (R 4.3.1)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.3.1)\n munsell       0.5.1   2024-04-01 [1] CRAN (R 4.3.3)\n nseq        * 0.0.1   2024-04-22 [1] local\n pillar        1.9.0   2023-03-22 [1] CRAN (R 4.3.1)\n pkgconfig     2.0.3   2019-09-22 [1] CRAN (R 4.3.1)\n proxy         0.4-27  2022-06-09 [1] CRAN (R 4.3.1)\n purrr       * 1.0.2   2023-08-10 [1] CRAN (R 4.3.1)\n R6            2.5.1   2021-08-19 [1] CRAN (R 4.3.1)\n Rcpp          1.0.12  2024-01-09 [1] CRAN (R 4.3.2)\n readr       * 2.1.5   2024-01-10 [1] CRAN (R 4.3.2)\n rlang         1.1.3   2024-01-10 [1] CRAN (R 4.3.2)\n rmarkdown     2.26    2024-03-05 [1] CRAN (R 4.3.3)\n rstudioapi    0.16.0  2024-03-24 [1] CRAN (R 4.3.3)\n s2            1.1.6   2023-12-19 [1] CRAN (R 4.3.2)\n scales        1.3.0   2023-11-28 [1] CRAN (R 4.3.2)\n sessioninfo   1.2.2   2021-12-06 [1] CRAN (R 4.3.1)\n sf          * 1.0-16  2024-03-24 [1] CRAN (R 4.3.3)\n stringi       1.8.3   2023-12-11 [1] CRAN (R 4.3.2)\n stringr     * 1.5.1   2023-11-14 [1] CRAN (R 4.3.2)\n tibble      * 3.2.1   2023-03-20 [1] CRAN (R 4.3.1)\n tidyr       * 1.3.1   2024-01-24 [1] CRAN (R 4.3.2)\n tidyselect    1.2.1   2024-03-11 [1] CRAN (R 4.3.3)\n tidyverse   * 2.0.0   2023-02-22 [1] CRAN (R 4.3.1)\n timechange    0.3.0   2024-01-18 [1] CRAN (R 4.3.2)\n tzdb          0.4.0   2023-05-12 [1] CRAN (R 4.3.1)\n units         0.8-5   2023-11-28 [1] CRAN (R 4.3.2)\n utf8          1.2.4   2023-10-22 [1] CRAN (R 4.3.1)\n vctrs         0.6.5   2023-12-01 [1] CRAN (R 4.3.2)\n viridisLite * 0.4.2   2023-05-02 [1] CRAN (R 4.3.1)\n withr         3.0.0   2024-01-16 [1] CRAN (R 4.3.2)\n wk            0.9.1   2023-11-29 [1] CRAN (R 4.3.2)\n xfun          0.43    2024-03-25 [1] CRAN (R 4.3.3)\n yaml          2.3.8   2023-12-11 [1] CRAN (R 4.3.2)\n zendown     * 0.1.0   2024-04-15 [1] local\n\n [1] /home/raphael/R/x86_64-pc-linux-gnu-library/4.3\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/query_remot_parquet_file.html",
    "href": "posts/query_remot_parquet_file.html",
    "title": "Query remote parquet files with DuckDB",
    "section": "",
    "text": "DuckDB has a very interesting extension called httpfs that allows to query CSV and parquet files remotely, including S3 storage.\nI will gave here an example about how to query a parquet file stored in a GitHub repository release."
  },
  {
    "objectID": "posts/query_remot_parquet_file.html#remote-parquet-file",
    "href": "posts/query_remot_parquet_file.html#remote-parquet-file",
    "title": "Query remote parquet files with DuckDB",
    "section": "Remote parquet file",
    "text": "Remote parquet file\nFirst, I recommend to create an object with the URL of the parquet file. On the example bellow, the parquet file contains the nycflights13 flights database.\n\nparquet_url &lt;- \"https://github.com/rfsaldanha/releases/releases/download/v1/flights.parquet\"\n\nThis is a real URL. If you try to open it with your browser, a download will start."
  },
  {
    "objectID": "posts/query_remot_parquet_file.html#duckdb-database-on-memory",
    "href": "posts/query_remot_parquet_file.html#duckdb-database-on-memory",
    "title": "Query remote parquet files with DuckDB",
    "section": "DuckDB database on memory",
    "text": "DuckDB database on memory\nNext, we create a DuckDB connection. Considering that the data is stored remotely, we can create this DuckDB database on memory and not on disk.\n\nconn &lt;- DBI::dbConnect(\n  duckdb::duckdb(),\n  dbdir = \":memory:\"\n)"
  },
  {
    "objectID": "posts/query_remot_parquet_file.html#httpfs-extension",
    "href": "posts/query_remot_parquet_file.html#httpfs-extension",
    "title": "Query remote parquet files with DuckDB",
    "section": "httpfs extension",
    "text": "httpfs extension\nNow, we need to load the httpfs extension.\n\nDBI::dbExecute(conn, \"INSTALL httpfs;\")\n\n[1] 0\n\nDBI::dbExecute(conn, \"LOAD httpfs;\")\n\n[1] 0\n\n\n\n\n\n\n\n\nWindows users\n\n\n\nThe last time I checked, the httpfs extension was not working on Windows."
  },
  {
    "objectID": "posts/query_remot_parquet_file.html#query",
    "href": "posts/query_remot_parquet_file.html#query",
    "title": "Query remote parquet files with DuckDB",
    "section": "Query",
    "text": "Query\nWe are ready to execute a query over the parquet file!\n\nres &lt;- DBI::dbGetQuery(\n  conn, \n  glue::glue(\"SELECT carrier, flight, tailnum, year FROM '{parquet_url}' WHERE year = 2013 LIMIT 100\")\n)\n\ndplyr::glimpse(res)\n\nRows: 100\nColumns: 4\n$ carrier &lt;chr&gt; \"UA\", \"UA\", \"AA\", \"B6\", \"DL\", \"UA\", \"B6\", \"EV\", \"B6\", \"AA\", \"B…\n$ flight  &lt;int&gt; 1545, 1714, 1141, 725, 461, 1696, 507, 5708, 79, 301, 49, 71, …\n$ tailnum &lt;chr&gt; \"N14228\", \"N24211\", \"N619AA\", \"N804JB\", \"N668DN\", \"N39463\", \"N…\n$ year    &lt;int&gt; 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 20…\n\n\nThis query selects some variables and filter the year, returning the first 100 rows. The query is carried out by DuckDB accessing the remote parquet file, but the file is not downloaded. That’s great!!\n\n\n\n\n\n\nTip\n\n\n\nQueries that needs more data and return more rows takes longer to run, especially transmitting data over the Internet. Craft carefully your queries with this in mind.\n\n\nIf you want to use dplyr verbs with the connection, you can create a view query.\n\nDBI::dbExecute(conn, glue::glue(\"CREATE VIEW flights AS SELECT * FROM PARQUET_SCAN('{parquet_url}')\"))\n\n[1] 0\n\n\n\nDBI::dbListTables(conn)\n\n[1] \"flights\"\n\n\n\nlibrary(dplyr)\n\ntbl(conn, \"flights\") %&gt;%\n  group_by(month) %&gt;%\n  summarise(freq = n()) %&gt;%\n  ungroup() %&gt;%\n  collect()\n\n# A tibble: 12 × 2\n   month  freq\n   &lt;int&gt; &lt;dbl&gt;\n 1     1 27004\n 2     2 24951\n 3     3 28834\n 4     4 28330\n 5     5 28796\n 6     6 28243\n 7     7 29425\n 8     8 29327\n 9     9 27574\n10    10 28889\n11    11 27268\n12    12 28135\n\n\nNow we can close the connection.\n\nDBI::dbDisconnect(conn, shutdown = TRUE)"
  },
  {
    "objectID": "posts/query_remot_parquet_file.html#extra-duckdbfs-package",
    "href": "posts/query_remot_parquet_file.html#extra-duckdbfs-package",
    "title": "Query remote parquet files with DuckDB",
    "section": "Extra: duckdbfs package",
    "text": "Extra: duckdbfs package\nThe duckdbfs offers a neat way to connect to remote parquet files and other connections. The same example using the package:\n\nds &lt;- duckdbfs::open_dataset(parquet_url)\n\n\nds %&gt;%\n  group_by(month) %&gt;%\n  summarise(freq = n()) %&gt;%\n  ungroup() %&gt;%\n  collect()\n\n# A tibble: 12 × 2\n   month  freq\n   &lt;int&gt; &lt;dbl&gt;\n 1     1 27004\n 2     2 24951\n 3     3 28834\n 4     4 28330\n 5     5 28796\n 6     6 28243\n 7     7 29425\n 8     8 29327\n 9     9 27574\n10    10 28889\n11    11 27268\n12    12 28135"
  },
  {
    "objectID": "posts/query_remot_parquet_file.html#session-info",
    "href": "posts/query_remot_parquet_file.html#session-info",
    "title": "Query remote parquet files with DuckDB",
    "section": "Session info",
    "text": "Session info\n\nsessionInfo()\n\nR version 4.3.2 (2023-10-31)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 22.04.3 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.10.0 \nLAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.10.0\n\nlocale:\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_CA.UTF-8        LC_COLLATE=en_US.UTF-8    \n [5] LC_MONETARY=en_CA.UTF-8    LC_MESSAGES=en_US.UTF-8   \n [7] LC_PAPER=en_CA.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_CA.UTF-8 LC_IDENTIFICATION=C       \n\ntime zone: Europe/Paris\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] dplyr_1.1.3\n\nloaded via a namespace (and not attached):\n [1] vctrs_0.6.4       cli_3.6.1         knitr_1.45        rlang_1.1.2      \n [5] xfun_0.41         DBI_1.1.3         purrr_1.0.2       duckdbfs_0.0.3   \n [9] generics_0.1.3    jsonlite_1.8.7    glue_1.6.2        dbplyr_2.4.0     \n[13] htmltools_0.5.7   fansi_1.0.5       rmarkdown_2.25    evaluate_0.23    \n[17] tibble_3.2.1      fastmap_1.1.1     yaml_2.3.7        lifecycle_1.0.3  \n[21] duckdb_0.9.1-1    compiler_4.3.2    blob_1.2.4        htmlwidgets_1.6.2\n[25] pkgconfig_2.0.3   rstudioapi_0.15.0 digest_0.6.33     R6_2.5.1         \n[29] tidyselect_1.2.0  utf8_1.2.4        pillar_1.9.0      magrittr_2.0.3   \n[33] tools_4.3.2"
  },
  {
    "objectID": "posts/run_length_encoding.html",
    "href": "posts/run_length_encoding.html",
    "title": "Counting consecutive sequences of events: run length encoding and warm spell occurence example",
    "section": "",
    "text": "Some days ago I was trying to count how many times consecutive sequences with values higher than a reference appears in a data frame.\nFor example:\n\\(x = {2,2,3,4,4,5,6,7,3,7,7,1,6,7,8}\\)\nOn \\(x\\), how many times values higher than four appears in consecutive sequences with three or more elements?\nTwo times: the sequences \\(5,6,7\\) and \\(6,7,8\\).\nBut how to figure this out with R?"
  },
  {
    "objectID": "posts/run_length_encoding.html#run-length-encoding",
    "href": "posts/run_length_encoding.html#run-length-encoding",
    "title": "Counting consecutive sequences of events: run length encoding and warm spell occurence example",
    "section": "Run length encoding",
    "text": "Run length encoding\nBase R has an interesting function called Run Length Encoding: rle(x). Let’s see how this works with our example.\n\nx &lt;- c(2,2,3,4,4,5,6,7,3,7,7,1,6,7,8)\n\nrle(x)\n\nRun Length Encoding\n  lengths: int [1:12] 2 1 2 1 1 1 1 2 1 1 ...\n  values : num [1:12] 2 3 4 5 6 7 3 7 1 6 ...\n\n\nThe rle function returns a list of lengths and values. The “lengths” says how many times the “values” appears. The number two appears two times in sequence, then the number three appears one time, followed by a fours appearing two times, and so on…"
  },
  {
    "objectID": "posts/run_length_encoding.html#count-consecutive-sequences",
    "href": "posts/run_length_encoding.html#count-consecutive-sequences",
    "title": "Counting consecutive sequences of events: run length encoding and warm spell occurence example",
    "section": "Count consecutive sequences",
    "text": "Count consecutive sequences\nI created a little function (trle) to perform the following task: with a vector x, count consecutive sequences of length equal or higher than l that contains values equal or higher than v.\n\ntrle &lt;- function(x, l, v){\n1  x_logical &lt;- x &gt;= v\n  \n2  rle_list &lt;- rle(x_logical)\n\n3  rle_df &lt;- data.frame(\n    length = rle_list$lengths,\n    value = rle_list$values\n  )\n\n4  res_df &lt;- subset(rle_df, value == TRUE & length &gt;= l)\n  \n5  res &lt;- nrow(res_df)\n\n  return(res)\n}\n\n\n1\n\nCheck if each element of x is higher or equal to v. This will return a vector of true and false values.\n\n2\n\nRun the rle function over the sequence of true/false values.\n\n3\n\nConvert the list in a data frame object.\n\n4\n\nFilter rows where the value is equal to TRUE AND (&) have length higher than l.\n\n5\n\nCount the rows. That’s the result!\n\n\n\n\nLet’s test it with our example.\n\ntrle(x, l = 3, v = 4)\n\n[1] 2\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere are probably better and faster ways to implement this. I focused on being simple and readable."
  },
  {
    "objectID": "posts/run_length_encoding.html#warm-spell-example",
    "href": "posts/run_length_encoding.html#warm-spell-example",
    "title": "Counting consecutive sequences of events: run length encoding and warm spell occurence example",
    "section": "Warm spell example",
    "text": "Warm spell example\nIn climatology there is an indicator called Warm Spell Duration Index (WSDI). A warm spell consist of at least six consecutive days with maximum temperatures higher than the climatological normal maximum temperature. A more formal definition can be found here.\nWe can get some temperature values with the brclimr package for Rio de Janeiro, Brazil.\n\nrio &lt;- brclimr::fetch_data(\n  code_muni = 3304557,\n  product = \"brdwgd\", \n  indicator = \"tmax\", \n  statistics = \"mean\", \n  date_start = as.Date(\"2020-01-01\"),\n  date_end = as.Date(\"2020-12-31\")\n)\n\nhead(rio, 10)\n\n# A tibble: 10 × 2\n   date       value\n   &lt;date&gt;     &lt;dbl&gt;\n 1 2020-01-01  36.0\n 2 2020-01-02  31.1\n 3 2020-01-03  27.9\n 4 2020-01-04  28.5\n 5 2020-01-05  28.4\n 6 2020-01-06  34.1\n 7 2020-01-07  35.8\n 8 2020-01-08  32.8\n 9 2020-01-09  33.4\n10 2020-01-10  33.7\n\n\nLet’s assume the reference value as 30 Celsius degrees. How many sequences of six days or more had temperatures equal or higher than 30?\n\ntrle(x = rio$value, l = 6, v = 30)\n\n[1] 3\n\n\nThis happened three times on 2020. Try to find them at the graph bellow:\n\nlibrary(ggplot2)\nlibrary(scales)\n\nggplot(data = rio, aes(x = date, y = value)) +\n  geom_line(color = \"purple\", alpha = .7) +\n  geom_point(color = \"purple\", alpha = .7) +\n  geom_hline(yintercept = 30, color = \"red\", alpha = .7) +\n  theme_classic()"
  },
  {
    "objectID": "posts/run_length_encoding.html#length-and-positions-of-consecutive-events",
    "href": "posts/run_length_encoding.html#length-and-positions-of-consecutive-events",
    "title": "Counting consecutive sequences of events: run length encoding and warm spell occurence example",
    "section": "Length and positions of consecutive events",
    "text": "Length and positions of consecutive events\nWould be nice if the function returned also when those sequences happened, right? We can change our little function a little bit to return the dates and actual lengths of those sequences, adding an index vector i as argument.\n\ntrle2 &lt;- function(x, i, l, v){\n1  x_logical &lt;- x &gt;= v\n  \n2  rle_list &lt;- rle(x_logical)\n\n3  rle_df &lt;- data.frame(\n    length = rle_list$lengths,\n    value = rle_list$values\n  )\n\n4  rle_df$pos_2 &lt;- cumsum(rle_df$length)\n  rle_df$pos_1 &lt;- rle_df$pos_2 - rle_df$length + 1\n\n5  rle_df &lt;- data.frame(\n    pos_1 = i[rle_df$pos_1],\n    pos_2 = i[rle_df$pos_2],\n    length = rle_df$length,\n    value = rle_df$value\n  )\n\n6  res &lt;- subset(rle_df, value == TRUE & length &gt;= l)\n7  res$value &lt;- NULL\n8  rownames(res) &lt;- NULL\n\n9  return(res)\n}\n\n\n1\n\nCheck if each element of x is higher or equal to v. This will return a vector of true and false values.\n\n2\n\nRun the rle function over the sequence of true/false values.\n\n3\n\nConvert the list in a data frame object.\n\n4\n\nEstablish the start (pos_1) and end (pos_2) positions of each sequence. The end position is the cumulative sum of the lengths. The start position is equivalent to the end position minus the length plus one.\n\n5\n\nCreate a data frame with the equivalent positions on the vector i and the length and values of the sequences\n\n6\n\nFilter rows where the value is equal to TRUE AND (&) have length higher than l.\n\n7\n\nRemove the value variable.\n\n8\n\nRemove row names.\n\n9\n\nReturn the data frame as result.\n\n\n\n\n\nres &lt;- trle2(x = rio$value, i = rio$date, l = 6, v = 30)\n\nres\n\n       pos_1      pos_2 length\n1 2020-01-06 2020-01-12      7\n2 2020-01-26 2020-02-03      9\n3 2020-02-15 2020-02-21      7\n\n\nTo see that on the graph, we can use some lubridate functions. First, we create a list of date intervals.\n\nlibrary(lubridate)\n\nintervals &lt;- as.list(lubridate::interval(res$pos_1, res$pos_2))\n\nThen, we check if the dates are within those intervals.\n\nrio$test &lt;- rio$date %within% intervals\n\nhead(rio, 10)\n\n# A tibble: 10 × 3\n   date       value test \n   &lt;date&gt;     &lt;dbl&gt; &lt;lgl&gt;\n 1 2020-01-01  36.0 FALSE\n 2 2020-01-02  31.1 FALSE\n 3 2020-01-03  27.9 FALSE\n 4 2020-01-04  28.5 FALSE\n 5 2020-01-05  28.4 FALSE\n 6 2020-01-06  34.1 TRUE \n 7 2020-01-07  35.8 TRUE \n 8 2020-01-08  32.8 TRUE \n 9 2020-01-09  33.4 TRUE \n10 2020-01-10  33.7 TRUE \n\n\nAnd plot it!\n\nggplot(data = rio, aes(x = date, y = value)) +\n  geom_line(color = \"purple\", alpha = .7) +\n  geom_point(aes(color = test), alpha = .7) +\n  geom_hline(yintercept = 30, color = \"red\", alpha = .7) +\n  scale_color_manual(values=c(\"#999999\", \"#E69F00\")) +\n  theme_classic() +\n  theme(legend.position = \"bottom\", legend.direction = \"horizontal\")"
  },
  {
    "objectID": "posts/run_length_encoding.html#session-info",
    "href": "posts/run_length_encoding.html#session-info",
    "title": "Counting consecutive sequences of events: run length encoding and warm spell occurence example",
    "section": "Session info",
    "text": "Session info\n\nsessionInfo()\n\nR version 4.3.2 (2023-10-31)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 22.04.3 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.10.0 \nLAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.10.0\n\nlocale:\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_CA.UTF-8        LC_COLLATE=en_US.UTF-8    \n [5] LC_MONETARY=en_CA.UTF-8    LC_MESSAGES=en_US.UTF-8   \n [7] LC_PAPER=en_CA.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_CA.UTF-8 LC_IDENTIFICATION=C       \n\ntime zone: Europe/Paris\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] lubridate_1.9.3 scales_1.2.1    ggplot2_3.4.4  \n\nloaded via a namespace (and not attached):\n [1] bit_4.0.5         gtable_0.3.4      jsonlite_1.8.7    dplyr_1.1.4      \n [5] compiler_4.3.2    tidyselect_1.2.0  assertthat_0.2.1  arrow_14.0.0     \n [9] yaml_2.3.7        fastmap_1.1.1     R6_2.5.1          labeling_0.4.3   \n[13] generics_0.1.3    brclimr_0.1.2     knitr_1.45        htmlwidgets_1.6.2\n[17] backports_1.4.1   checkmate_2.3.0   tibble_3.2.1      munsell_0.5.0    \n[21] pillar_1.9.0      rlang_1.1.2       utf8_1.2.4        xfun_0.41        \n[25] bit64_4.0.5       timechange_0.2.0  cli_3.6.1         withr_2.5.2      \n[29] magrittr_2.0.3    digest_0.6.33     grid_4.3.2        rstudioapi_0.15.0\n[33] lifecycle_1.0.4   vctrs_0.6.4       evaluate_0.23     glue_1.6.2       \n[37] farver_2.1.1      colorspace_2.1-0  fansi_1.0.5       rmarkdown_2.25   \n[41] purrr_1.0.2       tools_4.3.2       pkgconfig_2.0.3   htmltools_0.5.7"
  },
  {
    "objectID": "posts/sqlite_to_duckdb_and_parquet.html",
    "href": "posts/sqlite_to_duckdb_and_parquet.html",
    "title": "SQLite database conversion to DuckDB and Parquet files",
    "section": "",
    "text": "DuckDB is a relatively new database that works in a file, just like SQLite, but is very fast and designed for data science workflows.\nI am writing this post to cover the following scenario: you already have a SQLite database and want to convert it to DuckDB, and also export it to a parquet file."
  },
  {
    "objectID": "posts/sqlite_to_duckdb_and_parquet.html#sqlite-database",
    "href": "posts/sqlite_to_duckdb_and_parquet.html#sqlite-database",
    "title": "SQLite database conversion to DuckDB and Parquet files",
    "section": "SQLite database",
    "text": "SQLite database\nWe need a SQLite database example to later convert it to DuckDB. Let’s use the mtcars dataset.\n\nlibrary(dplyr)\nlibrary(lubridate)\n\nglimpse(mtcars)\n\nRows: 32\nColumns: 11\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ hp   &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…\n$ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…\n$ wt   &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n$ vs   &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,…\n$ am   &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,…\n$ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,…\n$ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,…\n\n\nAnd write mtcars in a SQLite database.\n\nsqlite_database_file &lt;- tempfile()\n\nconn_sqlite &lt;- DBI::dbConnect(\n  RSQLite::SQLite(), \n  sqlite_database_file, \n  extended_types = TRUE\n)\n\nDBI::dbWriteTable(conn_sqlite, name = \"mtcars_table\", value = mtcars, overwrite = TRUE)\n\nLet’s take a look.\n\ntbl(conn_sqlite, \"mtcars_table\") %&gt;% head() %&gt;% collect()\n\n# A tibble: 6 × 11\n    mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  21       6   160   110  3.9   2.62  16.5     0     1     4     4\n2  21       6   160   110  3.9   2.88  17.0     0     1     4     4\n3  22.8     4   108    93  3.85  2.32  18.6     1     1     4     1\n4  21.4     6   258   110  3.08  3.22  19.4     1     0     3     1\n5  18.7     8   360   175  3.15  3.44  17.0     0     0     3     2\n6  18.1     6   225   105  2.76  3.46  20.2     1     0     3     1\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that this database could be written directly using DuckDB, but this is an example about database conversion."
  },
  {
    "objectID": "posts/sqlite_to_duckdb_and_parquet.html#from-sqlite-to-duckdb",
    "href": "posts/sqlite_to_duckdb_and_parquet.html#from-sqlite-to-duckdb",
    "title": "SQLite database conversion to DuckDB and Parquet files",
    "section": "From SQLite to DuckDB",
    "text": "From SQLite to DuckDB\nFirst, we need to create our DuckDB database.\n\nduckdb_database_file &lt;- tempfile()\n\nconn_duckdb &lt;- DBI::dbConnect(\n  duckdb::duckdb(), \n  duckdb_database_file\n)\n\nTo import our data, we can use a DuckDB extension to read SQLite databases.\n\nDBI::dbExecute(conn_duckdb, \"INSTALL sqlite;\")\n\n[1] 0\n\nDBI::dbExecute(conn_duckdb, \"LOAD sqlite;\")\n\n[1] 0\n\n\n\nDBI::dbExecute(conn_duckdb, glue::glue(\"CREATE TABLE mtcars_table AS SELECT * FROM sqlite_scan('{sqlite_database_file}', 'mtcars_table');\"))\n\n[1] 32\n\n\nGreat! Now we have the same database on DuckDB."
  },
  {
    "objectID": "posts/sqlite_to_duckdb_and_parquet.html#from-duckdb-to-parquet",
    "href": "posts/sqlite_to_duckdb_and_parquet.html#from-duckdb-to-parquet",
    "title": "SQLite database conversion to DuckDB and Parquet files",
    "section": "From DuckDB to Parquet",
    "text": "From DuckDB to Parquet\nIt is very simple to export a DuckDB table to a parquet file.\n\nparquet_file &lt;- tempfile()\n\nDBI::dbExecute(conn_duckdb, glue::glue(\"COPY (SELECT * FROM 'mtcars_table') TO '{parquet_file}' (FORMAT 'PARQUET')\"))\n\n[1] 32\n\n\nAnd that’s it! Let’s close the connections.\n\nDBI::dbDisconnect(conn_sqlite)\nDBI::dbDisconnect(conn_duckdb, shutdown = TRUE)"
  },
  {
    "objectID": "posts/sqlite_to_duckdb_and_parquet.html#session-info",
    "href": "posts/sqlite_to_duckdb_and_parquet.html#session-info",
    "title": "SQLite database conversion to DuckDB and Parquet files",
    "section": "Session info",
    "text": "Session info\n\nsessionInfo()\n\nR version 4.3.1 (2023-06-16)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 22.04.3 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.10.0 \nLAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.10.0\n\nlocale:\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_CA.UTF-8        LC_COLLATE=en_US.UTF-8    \n [5] LC_MONETARY=en_CA.UTF-8    LC_MESSAGES=en_US.UTF-8   \n [7] LC_PAPER=en_CA.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_CA.UTF-8 LC_IDENTIFICATION=C       \n\ntime zone: Europe/Paris\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] lubridate_1.9.3 dplyr_1.1.3    \n\nloaded via a namespace (and not attached):\n [1] vctrs_0.6.4       cli_3.6.1         knitr_1.44        rlang_1.1.1      \n [5] xfun_0.40         DBI_1.1.3         purrr_1.0.2       generics_0.1.3   \n [9] jsonlite_1.8.7    bit_4.0.5         glue_1.6.2        dbplyr_2.3.4     \n[13] htmltools_0.5.6.1 hms_1.1.3         fansi_1.0.5       rmarkdown_2.25   \n[17] evaluate_0.22     tibble_3.2.1      fastmap_1.1.1     yaml_2.3.7       \n[21] lifecycle_1.0.3   memoise_2.0.1     duckdb_0.9.1      compiler_4.3.1   \n[25] blob_1.2.4        RSQLite_2.3.1     htmlwidgets_1.6.2 timechange_0.2.0 \n[29] pkgconfig_2.0.3   rstudioapi_0.15.0 digest_0.6.33     R6_2.5.1         \n[33] tidyselect_1.2.0  utf8_1.2.4        pillar_1.9.0      magrittr_2.0.3   \n[37] bit64_4.0.5       tools_4.3.1       cachem_1.0.8"
  },
  {
    "objectID": "posts/time_series_clustering.html",
    "href": "posts/time_series_clustering.html",
    "title": "Univariate and multivariate time series clustering",
    "section": "",
    "text": "On this post we will try some strategies to cluster univariate and multivariate time series in R with the {dtwclust} package."
  },
  {
    "objectID": "posts/time_series_clustering.html#packages",
    "href": "posts/time_series_clustering.html#packages",
    "title": "Univariate and multivariate time series clustering",
    "section": "Packages",
    "text": "Packages\nLet’s load some useful packages.\n\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(timetk)\nlibrary(arrow)\nlibrary(dtwclust)\nlibrary(geobr)\nlibrary(sf)"
  },
  {
    "objectID": "posts/time_series_clustering.html#data",
    "href": "posts/time_series_clustering.html#data",
    "title": "Univariate and multivariate time series clustering",
    "section": "Data",
    "text": "Data\n\nStates and capital general data\nWe will use some spatial data to plot the clustering results. The uf_sf contains the geographical boundaries from the Brazilian states.\n\nuf_sf &lt;- read_state(showProgress = FALSE)\n\nUsing year 2010\n\n\nThe cod_cap object (code folded bellow) is a data frame with Brazilian capitals name, codes and geographical coordinates.\n\n\nCode\ncod_cap &lt;- data.frame(\n  name_muni = c(\"Porto Velho, RO\", \"Manaus, AM\", \"Rio Branco, AC\", \n                \"Campo Grande, MS\", \"Macapá, AP\", \"Brasília, DF\",\n                \"Boa Vista, RR\", \"Cuiabá, MT\", \"Palmas, TO\",\n                \"São Paulo, SP\", \"Teresina, PI\", \"Rio de Janeiro, RJ\",\n                \"Belém, PA\", \"Goiânia, GO\", \"Salvador, BA\", \n                \"Florianópolis, SC\", \"São Luiz, MA\", \"Maceió, AL\",\n                \"Porto Alegre, RS\", \"Curitiba, PR\", \"Belo Horizonte, MG\",\n                \"Fortaleza, CE\", \"Recife, PE\", \"João Pessoa, PB\",\n                \"Aracaju, SE\", \"Natal, RN\", \"Vitória, ES\"),\n  code_muni = c(1100205, 1302603, 1200401, 5002704, 1600303,\n                5300108, 1400100, 5103403, 1721000, 3550308,\n                2211001, 3304557, 1501402, 5208707, 2927408, \n                4205407, 2111300, 2704302, 4314902, 4106902, \n                3106200, 2304400, 2611606, 2507507, 2800308,\n                2408102, 3205309),\n  lat = c(-8.760770, -3.118660, -9.974990, \n          -20.448600, 0.034934, -15.779500, \n          2.823840, -15.601000, -10.240000,\n          -23.532900, -5.091940, -22.912900, \n          -1.455400, -16.686400, -12.971800, \n          -27.594500, -2.538740, -9.665990, \n          -30.031800, -25.419500, -19.910200, \n          -3.716640, -8.046660, -7.115090,\n          -10.909100, -5.793570, -20.315500),\n  lon = c(-63.8999, -60.0212, -67.8243, \n          -54.6295, -51.0694, -47.9297, \n          -60.6753, -56.0974, -48.3558,\n          -46.6395, -42.8034, -43.2003, \n          -48.4898, -49.2643, -38.5011, \n          -48.5477, -44.2825, -35.7350,\n          -51.2065, -49.2646, -43.9266, \n          -38.5423, -34.8771, -34.8641,\n          -37.0677, -35.1986, -40.3128)\n) %&gt;%\n  st_as_sf(coords = c(\"lon\", \"lat\"), crs = 4236)\n\n\n\n\nClimate data\nWe use climate data from the Brazilian capitals as example dataset. The parquet files are from the brclimr package and can be downloaded here.\n\n1clim &lt;- open_dataset(\n  sources = c(\"../../brclim/parquet/brdwgd/pr.parquet\",\n              \"../../brclim/parquet/brdwgd/tmax.parquet\",\n              \"../../brclim/parquet/brdwgd/tmin.parquet\")\n) %&gt;%\n2  filter(date &gt;= as.Date(\"2011-01-01\") &\n           date &lt; as.Date(\"2021-01-01\")) %&gt;%\n3  filter(name %in% c(\"pr_sum\", \"Tmax_mean\", \"Tmin_mean\")) %&gt;%\n4  filter(code_muni %in% cod_cap$code_muni) %&gt;%\n5  collect() %&gt;%\n6  group_by(code_muni) %&gt;%\n7  arrange(date) %&gt;%\n8  pivot_wider(names_from = name, values_from = value) %&gt;%\n9  summarise_by_time(\n    .date_var = date,\n    .by = \"week\",\n    pr_sum = sum(pr_sum, na.rm = TRUE),\n    Tmax_mean = mean(Tmax_mean, na.rm = TRUE),\n    Tmin_mean = mean(Tmin_mean, na.rm = TRUE),\n  ) %&gt;%\n10  pivot_longer(cols = c(pr_sum, Tmax_mean, Tmin_mean)) %&gt;%\n11  ungroup() %&gt;%\n12  arrange(code_muni, name, date)\n\n\n1\n\nOpen parquet files without loading them to the memory.\n\n2\n\nThe parquet files contains daily data from 1961 to 2020. Let’s filter the most 10 years recent data.\n\n3\n\nThose parquet files contains several zonal statistics, let’s keep only the precipitation sum and maximun and minimum temperature averages.\n\n4\n\nKeep only data from the Brazilian capitals.\n\n5\n\nBased on the filters above, load the remaining data into memory.\n\n6\n\nNow we need to do some data manipulation per municipality.\n\n7\n\nOrder the municipality data per date.\n\n8\n\nPivot the data to wide format. The precipitation, maximum and minimum temperature data will form three columns, one for each.\n\n9\n\nWe will use the summarise_by_time from the {timetk} package to aggregate the daily data into weekly aggregates. This will help the clustering algorithm with a less noisy data.\n\n10\n\nNow we pivot the data back to the original long format.\n\n11\n\nUngroup the data.\n\n12\n\nArrange the data by municipality code, indicator’s name, and date.\n\n\n\n\nLet’s take a look at the resulting dataset.\n\nglimpse(clim)\n\nRows: 40,581\nColumns: 4\n$ code_muni &lt;int&gt; 1100205, 1100205, 1100205, 1100205, 1100205, 1100205, 110020…\n$ date      &lt;date&gt; 2010-12-26, 2011-01-02, 2011-01-09, 2011-01-16, 2011-01-23,…\n$ name      &lt;chr&gt; \"Tmax_mean\", \"Tmax_mean\", \"Tmax_mean\", \"Tmax_mean\", \"Tmax_me…\n$ value     &lt;dbl&gt; 32.08503, 30.86310, 31.73320, 29.87361, 30.09459, 30.67788, …"
  },
  {
    "objectID": "posts/time_series_clustering.html#univariate-clustering",
    "href": "posts/time_series_clustering.html#univariate-clustering",
    "title": "Univariate and multivariate time series clustering",
    "section": "Univariate clustering",
    "text": "Univariate clustering\nFirst, we will cluster the Brazilian capitals based only on the maximum temperature data. We need to extract this specific data from the above dataset and prepare it to be used by the dtwclust package, which requires a matrix of time series.\n\n1uclim &lt;- clim %&gt;%\n2  filter(name == \"Tmax_mean\") %&gt;%\n3  arrange(code_muni, date) %&gt;%\n4  select(-name) %&gt;%\n5  pivot_wider(names_from = \"code_muni\", values_from = \"value\") %&gt;%\n6  select(-date) %&gt;%\n7  t() %&gt;%\n8  tslist()\n\n\n1\n\nLoad the clim dataset that was just created.\n\n2\n\nFilter the average maximum temperature data.\n\n3\n\nArrange the data by municipality code and date. This will be important as we will see later bellow.\n\n4\n\nAs we have only one indicator, we can remove the indicator name variable from the dataset.\n\n5\n\nPivot the dataset into wide format.\n\n6\n\nNow we remove the date variable, as the {dtwclust} package functions will not use it. For this package, the time series is implicit by the sequence of the values.\n\n7\n\nNow we transpose the data. This will result in a matrix.\n\n8\n\nAnd now we coarce this matrix in a list of time series recognized by the {dtwclust} package.\n\n\n\n\nLet’s see the result.\n\nglimpse(uclim)\n\nList of 27\n $ 1100205: num [1:501] 32.1 30.9 31.7 29.9 30.1 ...\n $ 1200401: num [1:501] 31.1 30 31 30.4 29.1 ...\n $ 1302603: num [1:501] 33.1 30.8 32 29.8 30.8 ...\n $ 1400100: num [1:501] 32.8 32.5 32.3 33 34 ...\n $ 1501402: num [1:501] 29.7 32 31.8 30.3 30.4 ...\n $ 1600303: num [1:501] 29.7 30.8 30.5 29.6 30.9 ...\n $ 1721000: num [1:501] 30.6 31.4 30 30.8 27.6 ...\n $ 2111300: num [1:501] 32.2 31.6 31 29.2 29.6 ...\n $ 2211001: num [1:501] 32.6 32.6 32.2 31.9 31.2 ...\n $ 2304400: num [1:501] 32.5 31.7 31 29.8 29.9 ...\n $ 2408102: num [1:501] 30.7 29.6 29.9 29.6 29.8 ...\n $ 2507507: num [1:501] 31 30.6 30.4 30.6 30 ...\n $ 2611606: num [1:501] 30.2 30.2 30.6 30.3 29.7 ...\n $ 2704302: num [1:501] 31.5 31.4 31.1 30.4 29.9 ...\n $ 2800308: num [1:501] 32.4 32.5 31.8 32.3 31.3 ...\n $ 2927408: num [1:501] 31 30.5 29.5 29.2 29.7 ...\n $ 3106200: num [1:501] 21.6 26.2 27.7 30.4 29.9 ...\n $ 3205309: num [1:501] 30.6 30.5 33.1 32.8 31.8 ...\n $ 3304557: num [1:501] 28.2 30.9 32.8 33.8 35 ...\n $ 3550308: num [1:501] 26.6 26.5 28.6 29.9 32.6 ...\n $ 4106902: num [1:501] 24.3 27 27.3 27.5 30.2 ...\n $ 4205407: num [1:501] 26.9 29.7 29.7 29 31.4 ...\n $ 4314902: num [1:501] 30.1 32.3 30.6 30.6 32.5 ...\n $ 5002704: num [1:501] 30.5 32.2 30.3 29.7 32.4 ...\n $ 5103403: num [1:501] 30.2 32.5 31.5 30.6 32.3 ...\n $ 5208707: num [1:501] 28.7 28 28.5 31.5 31.9 ...\n $ 5300108: num [1:501] 25.1 24.9 26.8 29.1 28.9 ...\n\n\nThe uclim object is list of vectors. Each element of the list is named with the municipality code and contains the time series of the average maximum temperature.\nNow we will cluster the capitals based on the maximum temperature averagres using the tsclust function from the {dtwclust} package. This functions accepts several arguments variations and the package vignette reading is recommended.\nWe will cluster the municipalities from 2 to 10 groups partitions using the Soft-DTW algorithm.\n\nuclust &lt;- tsclust(\n  series = uclim, \n  type = \"partitional\",\n  k = 2:10, \n  distance = \"sdtw\", \n  seed = 13\n)\n\nNow let’s check the validity indices of each $k$ cluster approach. The table bellow is sorted by the silhouette statistic.\n\nnames(uclust) &lt;- paste0(\"k_\", 2:10)\nres_cvi &lt;- sapply(uclust, cvi, type = \"internal\") %&gt;% \n  t() %&gt;%\n  as_tibble(rownames = \"k\") %&gt;%\n  arrange(-Sil)\n\nres_cvi\n\n# A tibble: 9 × 8\n  k       Sil    SF    CH    DB DBstar      D    COP\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 k_3   0.551     0 30.7  0.618  0.799 0.0331 0.0852\n2 k_4   0.477     0 22.5  0.813  1.19  0.0610 0.0602\n3 k_2   0.443     0 28.1  0.774  0.774 0.0202 0.276 \n4 k_5   0.398     0 21.0  0.937  1.48  0.0610 0.0564\n5 k_6   0.390     0 16.5  0.831  1.73  0.133  0.0435\n6 k_8   0.354     0 13.7  1.00   1.88  0.133  0.0378\n7 k_7   0.285     0 15.1  0.982  1.48  0.0653 0.0414\n8 k_9   0.248     0 14.0  0.795  2.08  0.0496 0.0274\n9 k_10  0.109     0  9.32 1.05   4.05  0.0495 0.0432\n\n\nConsidering the silhouette statistic, the \\(k=3\\) clustering results present the best results. Let’s plot the results from it.\n\nu_sel_clust &lt;- uclust[[res_cvi[[1,1]]]]\n\nplot(u_sel_clust)\n\n\n\n\n\n\n\n\nAt this plot, each line is the time series of one capital. We see that partition #3 contains the smaller maximum averages values and partition $2 contains the higher maximum averages values. The table bellow tell us how many capitals are inside into each partition.\n\ntable(u_sel_clust@cluster)\n\n\n 1  2  3 \n12 11  4 \n\n\nLet’s see those results in a map. First we extract from the clustering result the labeled partitions and join it with the capitals metadata, and then plot the cluster partitions in a map\n\nu_cluster_ids &lt;- tibble(\n  code_muni = as.numeric(names(uclim)),\n  group = as.character(u_sel_clust@cluster)\n) %&gt;% \n  left_join(cod_cap, by = \"code_muni\") %&gt;%\n  arrange(group, name_muni) %&gt;%\n  st_as_sf()\n\n\nggplot() +\n  geom_sf(data = uf_sf, fill = \"lightgray\", color = \"grey20\", size=.15, show.legend = FALSE) +\n  geom_sf(data = u_cluster_ids, aes(color = group), size = 3) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nPartition #1 contains some capitals from the north and central regions. The partition #2 is formed from some capitals from the northeast, southeast and central regions. Partition #3 contains the some capitals from the southeast and south regions.\nInteresting to note that capitals from central regions are clustered together with capitals from the northeast considering only the average maximum temperature."
  },
  {
    "objectID": "posts/time_series_clustering.html#multivariate-clustering",
    "href": "posts/time_series_clustering.html#multivariate-clustering",
    "title": "Univariate and multivariate time series clustering",
    "section": "Multivariate clustering",
    "text": "Multivariate clustering\nNow we will cluster the Brazilian capitals based on more climate indicators, by using the precipitation, maximum and minimum temperature.\n\n1mclim_g &lt;- clim %&gt;%\n2  arrange(name, code_muni, date) %&gt;%\n3  group_by(code_muni) %&gt;%\n4  pivot_wider(names_from = \"name\", values_from = \"value\") %&gt;%\n5  mutate(across(c(pr_sum, Tmax_mean, Tmin_mean), ~ standardize_vec(.x, silent = TRUE))) %&gt;%\n6  select(-date)\n\n7mclim &lt;- group_split(mclim_g, .keep = FALSE) %&gt;%\n8  tslist(simplify = TRUE)\n\n9names(mclim) &lt;- group_keys(mclim_g)$code_muni\n\n\n1\n\nLoad the clim object with all the data.\n\n2\n\nArrange the values by indicator’s name, municipality code and date.\n\n3\n\nGroup the dataset to perform some manipulation on municipality level.\n\n4\n\nPivot the indicators to a wide format.\n\n5\n\nThe clustering algorithms may be influenced by the magnitude and units of the indicators. We will standarize the values with the standardize_vec from the {timetk} package.\n\n6\n\nRemove the date variable.\n\n7\n\nThe mclim_g object is a grouped data frame. We will split it to list with the group_split function, removing the code_muni variable (keep = FALSE argument).\n\n8\n\nThen we coerce this to a time series list.\n\n9\n\nFinally, we update the list names with the municipality codes, which are the grouping keys from the mclim_g intermediary object.\n\n\n\n\nLet’s take a look at the first three list elements of the mclim object.\n\nglimpse(mclim[1:3])\n\nList of 3\n $ 1100205: num [1:501, 1:3] -0.159 -0.905 -0.374 -1.509 -1.375 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : NULL\n  .. ..$ : chr [1:3] \"Tmax_mean\" \"Tmin_mean\" \"pr_sum\"\n $ 1200401: num [1:501, 1:3] -0.222 -0.856 -0.276 -0.654 -1.371 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : NULL\n  .. ..$ : chr [1:3] \"Tmax_mean\" \"Tmin_mean\" \"pr_sum\"\n $ 1302603: num [1:501, 1:3] 0.473 -1.066 -0.276 -1.72 -1.079 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : NULL\n  .. ..$ : chr [1:3] \"Tmax_mean\" \"Tmin_mean\" \"pr_sum\"\n\n\nEach object of the named list is a named matrix with the temperature and precipitation data.\nWe will use the same clustering function, but passing the multivariate list as series argument. Next, we will check the silhouette statistic from the different partition numbers.\n\nmclust &lt;- tsclust(\n  series = mclim, \n  type = \"partitional\", k = 2:10, \n  distance = \"sdtw\", \n  seed = 13\n)\n\n\nnames(mclust) &lt;- paste0(\"k_\", 2:10)\nres_cvi &lt;- sapply(mclust, cvi, type = \"internal\") %&gt;% \n  t() %&gt;%\n  as_tibble(rownames = \"k\") %&gt;%\n  arrange(-Sil)\n\nres_cvi\n\n# A tibble: 9 × 8\n  k       Sil    SF    CH    DB DBstar     D   COP\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 k_4   0.257     0 12.7  1.15    1.23 0.508 0.402\n2 k_3   0.218     0 18.2  1.14    1.18 0.402 0.472\n3 k_8   0.170     0  5.67 1.06    1.25 0.501 0.314\n4 k_2   0.164     0 29.7  1.15    1.15 0.235 0.644\n5 k_6   0.160     0  7.63 1.10    1.39 0.352 0.359\n6 k_5   0.135     0  7.61 1.23    1.28 0.328 0.401\n7 k_7   0.120     0  6.23 1.01    1.35 0.341 0.348\n8 k_10  0.119     0  4.14 1.03    1.19 0.458 0.268\n9 k_9   0.110     0  5.13 0.950   1.18 0.383 0.287\n\n\n\\(k=3\\) partitions present the higher silhouette statistic. Let’s plot it.\n\nm_sel_clust &lt;- mclust[[res_cvi[[1,1]]]]\n\nplot(m_sel_clust)\n\n\n\n\n\n\n\n\nThe plot present the time series from each capital per partition. On each subplot there are three sections, one for each indicator (minimum temperature, maximum temperature and precipitation).\nThe table bellow shows how many capitals are inside each partition.\n\ntable(m_sel_clust@cluster)\n\n\n1 2 3 4 \n6 7 7 7 \n\n\nNow, let’s plot these results in a map.\n\nm_cluster_ids &lt;- tibble(\n  code_muni = as.numeric(names(mclim)),\n  group = as.character(m_sel_clust@cluster)\n) %&gt;% \n  left_join(cod_cap, by = \"code_muni\") %&gt;%\n  arrange(group, name_muni) %&gt;%\n  st_as_sf()\n\nm_cluster_ids\n\nSimple feature collection with 27 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -67.8243 ymin: -30.0318 xmax: -34.8641 ymax: 2.82384\nGeodetic CRS:  Hu Tzu Shan 1950\n# A tibble: 27 × 4\n   code_muni group name_muni                     geometry\n       &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;                      &lt;POINT [°]&gt;\n 1   2800308 1     Aracaju, SE        (-37.0677 -10.9091)\n 2   2507507 1     João Pessoa, PB    (-34.8641 -7.11509)\n 3   2704302 1     Maceió, AL          (-35.735 -9.66599)\n 4   2408102 1     Natal, RN          (-35.1986 -5.79357)\n 5   2611606 1     Recife, PE         (-34.8771 -8.04666)\n 6   2927408 1     Salvador, BA       (-38.5011 -12.9718)\n 7   3106200 2     Belo Horizonte, MG (-43.9266 -19.9102)\n 8   5300108 2     Brasília, DF       (-47.9297 -15.7795)\n 9   5103403 2     Cuiabá, MT          (-56.0974 -15.601)\n10   5208707 2     Goiânia, GO        (-49.2643 -16.6864)\n# ℹ 17 more rows\n\n\n\nggplot() +\n  geom_sf(data = uf_sf, fill = \"lightgray\", color = \"grey20\", size=.15, show.legend = FALSE) +\n  geom_sf(data = m_cluster_ids, aes(color = group), size = 3) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe multivariate clustering result appears to be more interesting. The partition #1 is formed by the capital from the northeast, partition #2 with capitals from more central regions (except from Teresina in the north), partition #3 with capitals from the north, and partition #4 with capitals from the southeast and south capitals."
  },
  {
    "objectID": "posts/time_series_clustering.html#session-info",
    "href": "posts/time_series_clustering.html#session-info",
    "title": "Univariate and multivariate time series clustering",
    "section": "Session info",
    "text": "Session info\n\nsessionInfo()\n\nR version 4.2.0 (2022-04-22)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS 14.0\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n\nRandom number generation:\n RNG:     L'Ecuyer-CMRG \n Normal:  Inversion \n Sample:  Rejection \n \nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] sf_1.0-14       geobr_1.8.1     dtwclust_5.5.12 dtw_1.23-1     \n [5] proxy_0.4-27    arrow_13.0.0.1  timetk_2.9.0    ggplot2_3.4.4  \n [9] dplyr_1.1.3     tidyr_1.3.0    \n\nloaded via a namespace (and not attached):\n  [1] xts_0.13.1          lubridate_1.9.3     bit64_4.0.5        \n  [4] httr_1.4.7          DiceDesign_1.9      tools_4.2.0        \n  [7] utf8_1.2.4          R6_2.5.1            KernSmooth_2.23-22 \n [10] rpart_4.1.21        DBI_1.1.3           colorspace_2.1-0   \n [13] yardstick_1.2.0     nnet_7.3-19         withr_2.5.2        \n [16] tidyselect_1.2.0    curl_5.1.0          bit_4.0.5          \n [19] compiler_4.2.0      flexclust_1.4-1     cli_3.6.1          \n [22] shinyjs_2.1.0       labeling_0.4.3      scales_1.2.1       \n [25] classInt_0.4-10     tune_1.1.2          stringr_1.5.0      \n [28] digest_0.6.33       rmarkdown_2.25      pkgconfig_2.0.3    \n [31] htmltools_0.5.7     parallelly_1.36.0   lhs_1.1.6          \n [34] fastmap_1.1.1       htmlwidgets_1.6.2   rlang_1.1.2        \n [37] rstudioapi_0.15.0   shiny_1.7.5.1       farver_2.1.1       \n [40] generics_0.1.3      zoo_1.8-12          jsonlite_1.8.7     \n [43] magrittr_2.0.3      modeltools_0.2-23   Matrix_1.5-4.1     \n [46] Rcpp_1.0.11         munsell_0.5.0       fansi_1.0.5        \n [49] GPfit_1.0-8         lifecycle_1.0.4     furrr_0.3.1        \n [52] stringi_1.8.1       yaml_2.3.7          MASS_7.3-60        \n [55] plyr_1.8.9          recipes_1.0.8       grid_4.2.0         \n [58] parallel_4.2.0      listenv_0.9.0       promises_1.2.1     \n [61] ggrepel_0.9.4       lattice_0.22-5      splines_4.2.0      \n [64] knitr_1.45          pillar_1.9.0        dials_1.2.0        \n [67] future.apply_1.11.0 reshape2_1.4.4      codetools_0.2-19   \n [70] parsnip_1.1.1       stats4_4.2.0        glue_1.6.2         \n [73] evaluate_0.23       rsample_1.2.0       data.table_1.14.8  \n [76] RcppParallel_5.1.7  vctrs_0.6.4         httpuv_1.6.12      \n [79] foreach_1.5.2       gtable_0.3.4        purrr_1.0.2        \n [82] clue_0.3-65         future_1.33.0       assertthat_0.2.1   \n [85] xfun_0.41           gower_1.0.1         mime_0.12          \n [88] prodlim_2023.08.28  xtable_1.8-4        e1071_1.7-13       \n [91] RSpectra_0.16-1     later_1.3.1         class_7.3-22       \n [94] survival_3.5-7      timeDate_4022.108   tibble_3.2.1       \n [97] iterators_1.0.14    hardhat_1.3.0       units_0.8-4        \n[100] cluster_2.1.4       lava_1.7.3          workflows_1.1.3    \n[103] timechange_0.2.0    globals_0.16.2      ellipsis_0.3.2     \n[106] ipred_0.9-14"
  },
  {
    "objectID": "pres/ird2024.html#about-me",
    "href": "pres/ird2024.html#about-me",
    "title": "Introduction and Research Proposal",
    "section": "About me",
    "text": "About me\n\nBrazilian from Rio de Janeiro\nHusband, father of two dogs and a cat\nLoves cycling and take pictures\nResearcher on Health Geography and Climate-Sensitive Diseases\nR developer"
  },
  {
    "objectID": "pres/ird2024.html#academic-formation",
    "href": "pres/ird2024.html#academic-formation",
    "title": "Introduction and Research Proposal",
    "section": "Academic formation",
    "text": "Academic formation\n\nBSc on Geography (2007) and Specialization on Statistics (2009). Regionalization proposal based on Census data.\nMaster’s on Public Health (2017). Spatio-temporal models to study the relationship between traffic violence and income.\nDoctorate on Health Information (Fiocruz, 2021). Applications of Data Science methods on Public Health.\nFinal year of a postdoc position at Inria. Machine Learning methods application on Dengue forecast."
  },
  {
    "objectID": "pres/ird2024.html#main-fields-of-work-and-projects",
    "href": "pres/ird2024.html#main-fields-of-work-and-projects",
    "title": "Introduction and Research Proposal",
    "section": "Main fields of work and projects",
    "text": "Main fields of work and projects\n\nData Science methods applied to Public Health challenges\nData: R packages to handle Brazilian health datasets, indicators, and climate data (self initiative)\n\n{microdatasus}, {brpop}, {tidyrates}, {brclimr}\n\nVisualization: malaria and COVID-19 interactive visualization dashboards (IRD and Fiocruz)\nAnalysis and modeling of climate-sensitive diseases (Fiocruz, LNCC, BSC, Inria, IRD)"
  },
  {
    "objectID": "pres/ird2024.html#cross-border-malaria-transmission",
    "href": "pres/ird2024.html#cross-border-malaria-transmission",
    "title": "Introduction and Research Proposal",
    "section": "Cross-border malaria transmission",
    "text": "Cross-border malaria transmission\n\n\n\nFiocruz and IRD partnership\nBrazil (Amapá) and French Guiana border region\nDifferent languages and cultures, visualization is the key\nWork at MTD and a field work at Cayenne\nPublication at Journal of Medical Internet Research (JMIR, 2019)"
  },
  {
    "objectID": "pres/ird2024.html#covid-19-monitoring",
    "href": "pres/ird2024.html#covid-19-monitoring",
    "title": "Introduction and Research Proposal",
    "section": "COVID-19 Monitoring",
    "text": "COVID-19 Monitoring\n\nFiocruz official institutional dashboard for the pandemics\nMore than 10 data sources covering diverse aspects\nData gathering, harmonization, visualization and analysis\nChallenges on data availability, standardization, and information communication\nDaily work basis, high press demand for data and insights\nSeveral technical notes, papers and a book chapter\nProject awarded an government prize"
  },
  {
    "objectID": "pres/ird2024.html#postdoc-project",
    "href": "pres/ird2024.html#postdoc-project",
    "title": "Introduction and Research Proposal",
    "section": "Postdoc project",
    "text": "Postdoc project\n\nMontpellier University Inria Antenne, LIRMM\nDengue inflicts an important health burden in Brazil\n\n1 million new cases reported just in 2024, 214 deaths, official emergency status declaration\n\nProject for dengue incidence forecast on Brazil with machine learning methods\nNovel subsets approach, cluster based models with better performance"
  },
  {
    "objectID": "pres/ird2024.html#research-interface-and-working-axis",
    "href": "pres/ird2024.html#research-interface-and-working-axis",
    "title": "Introduction and Research Proposal",
    "section": "Research interface and working axis",
    "text": "Research interface and working axis"
  },
  {
    "objectID": "pres/ird2024.html#research-plans-for-the-next-years",
    "href": "pres/ird2024.html#research-plans-for-the-next-years",
    "title": "Introduction and Research Proposal",
    "section": "Research plans for the next years",
    "text": "Research plans for the next years\n\nApplication of Data Science methods to Public Health challenges on the Global South\nStreamline health and climate indicators projects, expansion to other countries and continents\nUsage of Machine Learning methods for data analysis and forecast, Early Warning Systems"
  },
  {
    "objectID": "pres/ird2024.html#axis-1.-data-and-indicators",
    "href": "pres/ird2024.html#axis-1.-data-and-indicators",
    "title": "Introduction and Research Proposal",
    "section": "Axis 1. Data and indicators",
    "text": "Axis 1. Data and indicators\n\n\n\nStandardized methodology to build health and climate indicators\nAdoption of common time and spatial units (administrative regions)\nApplication of zonal statistics\nPublication at Environmental Data Science journal (2024)"
  },
  {
    "objectID": "pres/ird2024.html#health-and-climate-indicators",
    "href": "pres/ird2024.html#health-and-climate-indicators",
    "title": "Introduction and Research Proposal",
    "section": "Health and climate indicators",
    "text": "Health and climate indicators\n\nProvide regularly updated datasets of health and climate indicators at different hierarchical administrative geographic boundaries\nExpand coverage to other countries\nUsage of population estimates as weighting factor\nBuild synthetic indicators for water cycle and droughts, warm and cold spells, and extreme events\nWork close with the ClimatSuds project"
  },
  {
    "objectID": "pres/ird2024.html#axis-2.-early-warning-system-for-climate-sensitive-diseases",
    "href": "pres/ird2024.html#axis-2.-early-warning-system-for-climate-sensitive-diseases",
    "title": "Introduction and Research Proposal",
    "section": "Axis 2. Early Warning System for Climate-Sensitive Diseases",
    "text": "Axis 2. Early Warning System for Climate-Sensitive Diseases\n\nFramework proposal to prevent and early detect CSD outbreaks on Global South countries, with most vulnerable populations to climate change\nActual EWS are limited on countries coverage, quality, interpretation and usage for decision making\nAligned with WHO guidelines, replicable in other countries"
  },
  {
    "objectID": "pres/ird2024.html#early-warning-system-for-climate-sensitive-diseases",
    "href": "pres/ird2024.html#early-warning-system-for-climate-sensitive-diseases",
    "title": "Introduction and Research Proposal",
    "section": "Early Warning System for Climate-Sensitive Diseases",
    "text": "Early Warning System for Climate-Sensitive Diseases\n\nMultivariate models with climate, environmental and socio-economic predictors\nTraditional and machine learning methods to forecast incidence for short and medium-time horizons (ARIMA, random forest, XGBoost, LSTM…)\nSubsets models to boost performance\nContinuous model monitoring and update (MLOps)"
  },
  {
    "objectID": "pres/ird2024.html#early-warning-system-for-climate-sensitive-diseases-1",
    "href": "pres/ird2024.html#early-warning-system-for-climate-sensitive-diseases-1",
    "title": "Introduction and Research Proposal",
    "section": "Early Warning System for Climate-Sensitive Diseases",
    "text": "Early Warning System for Climate-Sensitive Diseases\n\nLay an EWS framework adaptive to other regions’ contexts and society priorities\nClose work with health managers and civil society representatives for project usage and tools appropriation"
  },
  {
    "objectID": "pres/ird2024.html#axis-2.-healthcare-access-research",
    "href": "pres/ird2024.html#axis-2.-healthcare-access-research",
    "title": "Introduction and Research Proposal",
    "section": "Axis 2. Healthcare access research",
    "text": "Axis 2. Healthcare access research\n\nStudy the healthcare access networks\nObservation of empirical data on the patients address and healthcare unit location\nAdaptation of Social Network Analysis (SNA) methods\nAddresses are nodes, healthcare events are links\nMetrics of centrality of healthcare units, simulations on the effect of units creation and closing"
  },
  {
    "objectID": "pres/ird2024.html#integration-with-ird",
    "href": "pres/ird2024.html#integration-with-ird",
    "title": "Introduction and Research Proposal",
    "section": "Integration with IRD",
    "text": "Integration with IRD\n\nResearch aligned with the SDGs Good Health and Well-Being (3), Climate Action (13), and Partnership for the Goals (17)\nIRD equitable partnerships with the developing countries\nUMR EspaceDev expertise on earth observation and modeling of health and climate data\nESOR group focus on environment, society, and health relationship on time and space, reach on South America, Africa and Asia countries"
  },
  {
    "objectID": "pres/ird2024.html#thanks",
    "href": "pres/ird2024.html#thanks",
    "title": "Introduction and Research Proposal",
    "section": "Thanks!",
    "text": "Thanks!\nMore information about me and projects at\nrfsaldanha.github.io"
  },
  {
    "objectID": "projects/agua-saude.html",
    "href": "projects/agua-saude.html",
    "title": "Água & Saúde",
    "section": "",
    "text": "Água & Saúde main webpage.\n\n\nThe first project that I got involved at Fiocruz with the Climate and Health Observatory. It was a project in partnership with ANA (Agência Nacional das Águas).\nI was in charge to create a database of health and water indicators and a data dashboard. The project was fully developed with R, Shiny, and SQLite technologies.\nThe project has been discontinued.\n\n\n\n Back to top"
  },
  {
    "objectID": "projects/cidade-e-saude.html",
    "href": "projects/cidade-e-saude.html",
    "title": "Cidade & Saúde",
    "section": "",
    "text": "Cidade & Saúde main webpage.\n\n\n“Cidade & Saúde” is a self-funded Project that I started just after my master’s and before my Ph.D.\nThe project objective is to collect and aggregate health data at the municipality level and provide simple and visually compelling access with satellite images and graphs. The user is asked to type a city name and the system presents a webpage with the satellite image of that city and several socio-demographic and health information with graphs.\nThe satellite image is used to put the socio-demographic and health data in context, with graphs and indicators definitions covering demographic aspects, mortality, hospital admissions, health units, and health budget.\nThe project website is accessible by this address: https://cidadesaude.io. The project data is out of date, and waiting for financing.\n\n\n\n Back to top"
  },
  {
    "objectID": "projects/harmonize.html",
    "href": "projects/harmonize.html",
    "title": "Harmonize",
    "section": "",
    "text": "Harmonizing multi-scale spatiotemporal data for health in climate change hotspots.\nHarmonize is a project in partnership with the Health and Climate Observatory and several international partners, including the Barcelona Supercomputing Center (BSC).\nThe project’s objective is to collect, organize and analyze multi-scale spatiotemporal data for health in climate hotspots.\nI am working on this project to provide expert knowledge on Brazilian health data collection and analysis.\nThis work is supported by the Wellcome Trust grant number 224694/Z/21/Z.\n\n\n\n Back to top"
  },
  {
    "objectID": "projects/monitoracovid19.html",
    "href": "projects/monitoracovid19.html",
    "title": "MonitoraCovid-19",
    "section": "",
    "text": "MonitoraCovid-19 dashboard.\n\n\nMonitoraCovid-19 project is an institutional response from Fiocruz to the Covid-19 pandemic.\nThe project started in March 2020, when the first cases of Covid-19 occurred in Brazil. Since then, we monitored the pandemic daily, unifying more than 10 data sources into a unique data dashboard with several graphs and maps.\nI am responsible to the ETL process and data dashboard. The project website was accessed more than 700,000 times by more than 300,000 users worldwide.\nWe published 25 technical notes and 12 brief texts about the pandemic, along with papers and a book chapter.\nThis work was supported by the Fiocruz Inova and received the ENAP prize.\nThe dashboard address: https://bigdata-covid19.icict.fiocruz.br\nInterview at Fantástico (Rede Globo, May 2020): https://g1.globo.com/fantastico/noticia/2020/05/10/com-medidas-mais-rigorosas-paises-vizinhos-ao-brasil-dao-exemplo-no-combate-a-covid-19.ghtml\nTalk at ICICT Centro de Estudos, with Atila Iamarino and Diego Xavier (August 19, 2022): https://www.youtube.com/watch?v=GeyPs9yMzSk&t=3867s\n\n\n\n Back to top"
  },
  {
    "objectID": "projects/ouvsus.html",
    "href": "projects/ouvsus.html",
    "title": "OuvSUS",
    "section": "",
    "text": "Manifestações dashboard.\n\n\nA partnership between PCDaS, Instituto Aggeu Magalhães – Fiocruz Pernambuco, and the Brazilian Ministry of Health ombudsman. The project aims to collect, organize and present data about population inquiries and information requisitions.\nI oversee the team at PCDaS responsible for the ETL (extraction, transform and load) process and data dashboard.\nThe project website address: https://www.gov.br/saude/pt-br/canais-de-atendimento/ouvidoria-do-sus/ouvidoria-em-numeros/paineis-de-dados\nThis work is supported by Brazilian Health Ministry.\n\n\n\n Back to top"
  },
  {
    "objectID": "projects/pns.html",
    "href": "projects/pns.html",
    "title": "PNS",
    "section": "",
    "text": "PNS main webpage.\n\n\nThis project is a partnership between PCDaS and other researchers at Fiocruz and IBGE. The PNS is a national inquiry about the population health conditions with two editions: 2013 and 2019. Its results cover major aspects of populations’ health and involve special techniques for complex samples.\nI oversee the team at PCDaS responsible to create reproducible code notebooks about sample expansion and developing and maintain a website with project documentation and a data dashboard.\nProject website address: https://www.pns.icict.fiocruz.br\nThis work is supported by Brazilian Health Ministry.\n\n\n\n Back to top"
  },
  {
    "objectID": "projects/sauders.html",
    "href": "projects/sauders.html",
    "title": "Saúde RS",
    "section": "",
    "text": "Saúde RS is a fast response project to the Rio Grande do Sul flooding disaster that occurred in Brazil on May 2024. It presents maps and data sources about the flooding, with emphasis to map health units that are in risk areas.\nAccess: https://rfsaldanha.github.io/sauders\n\n\n\n Back to top"
  },
  {
    "objectID": "publications/araujoWHATHAVEWE2019a.html",
    "href": "publications/araujoWHATHAVEWE2019a.html",
    "title": "What have we learned from Mariana? The importance of names, places and affections",
    "section": "",
    "text": "ARAÚJO, N. et al. WHAT HAVE WE LEARNED FROM MARIANA? THE IMPORTANCE OF NAMES, PLACES AND AFFECTIONS. Ambiente & Sociedade, v. 22, p. e00002, May 2019.\n\n\nNaming is an ambitious task. Naming an object or event means recognizing, distinguishing and giving importance to it. Whoever names acquires power over what was previously indescribable. The name is sign and meaning, skewed in origin and use. Naming and understanding the complexity of what occurred in the city of Mariana and its causes and consequences represents a challenge for which disciplinary approaches centered on scientific knowledge are insufficient. Thus, our search for studies on what happened in Mariana after the failure of the Fundão Dam in November 2015 should be complemented by the stories of residents of that place, by their knowledge. We recognize that this approach to examining such an issue would only be a limited contribution and thus constitutes the primary condition for an interdisciplinary exercise."
  },
  {
    "objectID": "publications/araujoWHATHAVEWE2019a.html#reference",
    "href": "publications/araujoWHATHAVEWE2019a.html#reference",
    "title": "What have we learned from Mariana? The importance of names, places and affections",
    "section": "",
    "text": "ARAÚJO, N. et al. WHAT HAVE WE LEARNED FROM MARIANA? THE IMPORTANCE OF NAMES, PLACES AND AFFECTIONS. Ambiente & Sociedade, v. 22, p. e00002, May 2019.\n\n\nNaming is an ambitious task. Naming an object or event means recognizing, distinguishing and giving importance to it. Whoever names acquires power over what was previously indescribable. The name is sign and meaning, skewed in origin and use. Naming and understanding the complexity of what occurred in the city of Mariana and its causes and consequences represents a challenge for which disciplinary approaches centered on scientific knowledge are insufficient. Thus, our search for studies on what happened in Mariana after the failure of the Fundão Dam in November 2015 should be complemented by the stories of residents of that place, by their knowledge. We recognize that this approach to examining such an issue would only be a limited contribution and thus constitutes the primary condition for an interdisciplinary exercise."
  },
  {
    "objectID": "publications/camposFatoresAssociadosAo2020b.html#reference",
    "href": "publications/camposFatoresAssociadosAo2020b.html#reference",
    "title": "Fatores associados ao letramento funcional em saúde de mulheres atendidas pela Estratégia de Saúde da Família",
    "section": "Reference",
    "text": "Reference\n\n\nCAMPOS, A. A. L. et al. Fatores associados ao letramento funcional em saúde de mulheres atendidas pela Estratégia de Saúde da Família. Cadernos Saúde Coletiva, v. 28, p. 66–76, Apr. 2020."
  },
  {
    "objectID": "publications/camposFatoresAssociadosAo2020b.html#abstract",
    "href": "publications/camposFatoresAssociadosAo2020b.html#abstract",
    "title": "Fatores associados ao letramento funcional em saúde de mulheres atendidas pela Estratégia de Saúde da Família",
    "section": "Abstract",
    "text": "Abstract\nIntrodução. O letramento funcional em saúde (LFS) diz respeito à habilidade dos indivíduos em compreender as informações relacionadas à saúde e está relacionado a diversos desfechos de saúde. Objetivo. Investigar a associação do LFS com fatores sociodemográficos, apoio social, autoavaliação do estado de saúde e perfil de acesso aos serviços de saúde em mulheres assistidas pela Estratégia de Saúde da Família (ESF). Método. Estudo transversal, conduzido em 2015-2016, em duas Unidades de Atenção Primária à Saúde cobertas pela ESF, de um município da região Sudeste do Brasil. A amostra foi composta por 439 mulheres, entre 25 e 64 anos. O LFS foi avaliado por meio do Brief Test of Functional Health Literacy in Adults (B-TOFHLA). Efetuaram-se cálculos da razão de prevalência (RP). Posteriormente, construiu-se um modelo de regressão de Poisson de variância robusta, sendo admitida significância estatística quando p ≤0,05. Resultados. Foi constatado que 53,5% das mulheres apresentaram um baixo LFS, o qual associou-se à idade superior aos 40 anos (RP = 1,18; IC 95%: 1,07-1,31), ao grau de instrução inferior ao ensino médio completo (RP = 1,26; IC 95%: 1,15-1,38), à baixa renda (RP = 1,13; IC 95%: 1,04-1,23) e à autodeclaração da cor parda ou preta (RP= 1,06; IC 95%: 1,01-1,12). Conclusão. Os resultados acentuaram a importância do LFS como estratégia para a abordagem de populações com maior vulnerabilidade socioeconômica."
  },
  {
    "objectID": "publications/goncalvesCrescimentoEmpregoIndustrial2019a.html#reference",
    "href": "publications/goncalvesCrescimentoEmpregoIndustrial2019a.html#reference",
    "title": "Crescimento do emprego industrial local no Brasil: o grau de especialização por intensidade tecnológica importa?",
    "section": "Reference",
    "text": "Reference\n\n\nGONÇALVES, E. et al. Crescimento do emprego industrial local no Brasil: o grau de especialização por intensidade tecnológica importa? Nova Economia, v. 29, p. 41–74, May 2019."
  },
  {
    "objectID": "publications/goncalvesCrescimentoEmpregoIndustrial2019a.html#abstract",
    "href": "publications/goncalvesCrescimentoEmpregoIndustrial2019a.html#abstract",
    "title": "Crescimento do emprego industrial local no Brasil: o grau de especialização por intensidade tecnológica importa?",
    "section": "Abstract",
    "text": "Abstract\nThe aim of this paper is to revisit the debate on the degree of specialization and industrial diversification and the growth of local manufacturing employment in Brazil. A matrix of sectoral spillovers is built in which it is verified whether sectors, grouped by technological intensity, influence the performance of disaggregated manufacturing groups. Spatial data panel techniques are used to control non-observed local effects and possible spatial dependence over the period 1995-2014. The results show that specializations in low-technology manufacturing groups create stimulus for several other manufacturing groups regardless of the level of technological intensity. The spillovers coming from high technology manufacturing industries are less frequent, though they also occur depending on the industrial manufacturing group considered. In general, sectors of higher and lower technological intensity flourish with the presence of MAR externalities. We conclude that the diversification/specialization debate can vary considerably, requiring specific industrial and regional policies by manufacturing industries."
  },
  {
    "objectID": "publications/guimaraesIncreasingImpactCOVID192021b.html#reference",
    "href": "publications/guimaraesIncreasingImpactCOVID192021b.html#reference",
    "title": "Increasing impact of COVID-19 on young adults: evidence from hospitalisations in Brazil",
    "section": "Reference",
    "text": "Reference\n\n\nGUIMARÃES, R. et al. Increasing impact of COVID-19 on young adults: Evidence from hospitalisations in Brazil. Public Health, v. 198, p. 297–300, Sep. 2021."
  },
  {
    "objectID": "publications/guimaraesIncreasingImpactCOVID192021b.html#abstract",
    "href": "publications/guimaraesIncreasingImpactCOVID192021b.html#abstract",
    "title": "Increasing impact of COVID-19 on young adults: evidence from hospitalisations in Brazil",
    "section": "Abstract",
    "text": "Abstract\n\nObjectives\nConcerns about the increasing impact of severe COVID-19 in younger individuals in Brazil came after a recent synchronised country-wide wave of cases in Brazil. This communication analyses how hospitalisations due to COVID-19 changed in the age groups 18–49 years and ≥70 years.\n###Study design Longitudinal study based on secondary data.\n\n\nMethods\nData from SIVEP-Gripe, a public and open-access database of Severe Acute Respiratory Illness records (including COVID-19 notifications), were used in this study. Statistical control charts examined changes in the magnitude and variation of younger (18–49 years) and older (≥70 years) adults who were hospitalised between 15th March 2020 and 19th June 2021.\n\n\nResults\nDuring the few first weeks of the pandemic in Brazil, the number of COVID-19 hospitalisations increased in older adults but decreased in younger adults. Subsequently, hospitalisations reached statistical control zones in epidemiological weeks (EW) 19–48 of 2020 (EW 19-48/2020) and EW 03-05/2021 (18–49 y, mean = 26.1%; ≥70 y, mean = 32.8%). Between EW 49/2020 and EW 02/2021, the number of hospitalisations of younger adults dropped to levels below the lower control limit. In contrast, the number of hospitalisations of older adults surpassed the upper limit of the corresponding statistical control zones. However, from EW 06/2021, numbers of hospitalisations changed from statistical control zones, with hospitalisations of younger adults increasing and reaching 44.9% in EW 24/2021 and hospitalisations of older adults decreasing until EW 19/2021 (14.1%) and reaching 17.3% in EW 24/2021.\n\n\nConclusions\nAn increasing number of COVID-19 hospitalisations were observed in younger adults from EW 06/2021. This could be a result of the successful vaccination programme in older adults, who were initially prioritised, and possibly an increased exposure to highly transmissible variants of COVID-19 in younger adults who had to go to work in the absence of social protection (i.e. government financial support). Potential consequences of COVID-19 hospitalisations in younger adults could include a reduced life expectancy of the population and an increased number of people unable to perform daily activities due to post-COVID-19 conditions."
  },
  {
    "objectID": "publications/marinhoBurdenDiseaseBrazil2018.html#reference",
    "href": "publications/marinhoBurdenDiseaseBrazil2018.html#reference",
    "title": "Burden of disease in Brazil, 1990–2016: a systematic subnational analysis for the Global Burden of Disease Study 2016",
    "section": "Reference",
    "text": "Reference\n\n\nMARINHO, F. et al. Burden of disease in Brazil, 1990: A systematic subnational analysis for the Global Burden of Disease Study 2016. The Lancet, v. 392, n. 10149, p. 760–775, Sep. 2018."
  },
  {
    "objectID": "publications/marinhoBurdenDiseaseBrazil2018.html#abstract",
    "href": "publications/marinhoBurdenDiseaseBrazil2018.html#abstract",
    "title": "Burden of disease in Brazil, 1990–2016: a systematic subnational analysis for the Global Burden of Disease Study 2016",
    "section": "Abstract",
    "text": "Abstract\n\nBackground\nPolitical, economic, and epidemiological changes in Brazil have affected health and the health system. We used the Global Burden of Disease Study 2016 (GBD 2016) results to understand changing health patterns and inform policy responses.\n\n\nMethods\nWe analysed GBD 2016 estimates for life expectancy at birth (LE), healthy life expectancy (HALE), all-cause and cause-specific mortality, years of life lost (YLLs), years lived with disability (YLDs), disability-adjusted life-years (DALYs), and risk factors for Brazil, its 26 states, and the Federal District from 1990 to 2016, and compared these with national estimates for ten comparator countries.\n\n\nFindings\nNationally, LE increased from 68·4 years (95% uncertainty interval [UI] 68·0–68·9) in 1990 to 75·2 years (74·7–75·7) in 2016, and HALE increased from 59·8 years (57·1–62·1) to 65·5 years (62·5–68·0). All-cause agestandardised mortality rates decreased by 34·0% (33·4–34·5), while all-cause age-standardised DALY rates decreased by 30·2% (27·7–32·8); the magnitude of declines varied among states. In 2016, ischaemic heart disease was the leading cause of age-standardised YLLs, followed by interpersonal violence. Low back and neck pain, sense organ diseases, and skin diseases were the main causes of YLDs in 1990 and 2016. Leading risk factors contributing to DALYs in 2016 were alcohol and drug use, high blood pressure, and high body-mass index.\n\n\nInterpretation\nHealth improved from 1990 to 2016, but improvements and disease burden varied between states. An epidemiological transition towards non-communicable diseases and related risks occurred nationally, but later in some states, while interpersonal violence grew as a health concern. Policy makers can use these results to address health disparities.\n\n\nFunding\nBill & Melinda Gates Foundation and the Brazilian Ministry of Health."
  },
  {
    "objectID": "publications/paixaoEstimationCOVID19UnderReporting2021a.html#reference",
    "href": "publications/paixaoEstimationCOVID19UnderReporting2021a.html#reference",
    "title": "Estimation of COVID-19 Under-Reporting in the Brazilian States Through SARI",
    "section": "Reference",
    "text": "Reference\n\n\nPAIXÃO, B. et al. Estimation of COVID-19 Under-Reporting in the Brazilian States Through SARI. New Generation Computing, v. 39, n. 3, p. 623–645, Nov. 2021."
  },
  {
    "objectID": "publications/paixaoEstimationCOVID19UnderReporting2021a.html#abstract",
    "href": "publications/paixaoEstimationCOVID19UnderReporting2021a.html#abstract",
    "title": "Estimation of COVID-19 Under-Reporting in the Brazilian States Through SARI",
    "section": "Abstract",
    "text": "Abstract\nDue to its impact, COVID-19 has been stressing the academy to search for curing, mitigating, or controlling it. It is believed that under-reporting is a relevant factor in determining the actual mortality rate and, if not considered, can cause significant misinformation. Therefore, this work aims to estimate the under-reporting of cases and deaths of COVID-19 in Brazilian states using data from the InfoGripe. InfoGripe targets notifications of Severe Acute Respiratory Infection (SARI). The methodology is based on the combination of data analytics (event detection methods) and time series modeling (inertia and novelty concepts) over hospitalized SARI cases. The estimate of real cases of the disease, called novelty, is calculated by comparing the difference in SARI cases in 2020 (after COVID-19) with the total expected cases in recent years (2016–2019). The expected cases are derived from a seasonal exponential moving average. The results show that under-reporting rates vary significantly between states and that there are no general patterns for states in the same region in Brazil. The states of Minas Gerais and Mato Grosso have the highest rates of under-reporting of cases. The rate of under-reporting of deaths is high in the Rio Grande do Sul and the Minas Gerais. This work can be highlighted for the combination of data analytics and time series modeling. Our calculation of under-reporting rates based on SARI is conservative and better characterized by deaths than for cases."
  },
  {
    "objectID": "publications/saldanhaCienciaDadosBig2021a.html#reference",
    "href": "publications/saldanhaCienciaDadosBig2021a.html#reference",
    "title": "Ciência de dados e big data: o que isso significa para estudos populacionais e da saúde?",
    "section": "Reference",
    "text": "Reference\n\n\nSALDANHA, R. DE F.; BARCELLOS, C.; PEDROSO, M. DE M. Ciência de dados e big data: o que isso significa para estudos populacionais e da saúde? Cadernos Saúde Coletiva, v. 29, p. 51–58, Nov. 2021."
  },
  {
    "objectID": "publications/saldanhaCienciaDadosBig2021a.html#abstract",
    "href": "publications/saldanhaCienciaDadosBig2021a.html#abstract",
    "title": "Ciência de dados e big data: o que isso significa para estudos populacionais e da saúde?",
    "section": "Abstract",
    "text": "Abstract\n\nBackground\nThe term big data is no longer new in the academic environment and has become more common in scientific publications and research grants, leading to a profound revision of the way science is being made and taught.\n\n\nObjective\nTo reflect on the possible changes that data science can induce in population and health related studies\n\n\nMethod\nTo foster this debate, scientific articles selected from the big data field in health and demography were contrasted with books and other scientific productions.\n\n\nResults\nIt is argued that volume is not the most promising characteristic of big data for population and health related studies, but rather the complexity of data and the possibilities of integration with traditional studies by means of interdisciplinary teams.\n\n\nConclusion\nIn population and health related studies, the possibilities of integration between new and traditional methods are broad, and include new toolboxes for analysis, monitoring, prediction of events (cases) and health-disease processes in the population, and for the study of sociodemographic and environmental determinants."
  },
  {
    "objectID": "publications/saldanhaEstudoAnaliseRede2019.html#reference",
    "href": "publications/saldanhaEstudoAnaliseRede2019.html#reference",
    "title": "Estudo de análise de rede do fluxo de pacientes de câncer de mama no Brasil entre 2014 e 2016",
    "section": "Reference",
    "text": "Reference\n\n\nSALDANHA, R. DE F. et al. Estudo de análise de rede do fluxo de pacientes de câncer de mama no Brasil entre 2014 e 2016. Cadernos de Saúde Pública, v. 35, p. e00090918, Jul. 2019."
  },
  {
    "objectID": "publications/saldanhaEstudoAnaliseRede2019.html#abstract",
    "href": "publications/saldanhaEstudoAnaliseRede2019.html#abstract",
    "title": "Estudo de análise de rede do fluxo de pacientes de câncer de mama no Brasil entre 2014 e 2016",
    "section": "Abstract",
    "text": "Abstract\nThis study aims to analyze the flow of breast cancer patients treated outside of their municipality of residence, based on hospital admissions and chemotherapy and radiotherapy in the Brazilian Unified National Health System (SUS) from 2014 to 2016. Network analysis was used, considering the municipality of residence and of treatment as nodes in a graph, thus consisting of a “health system organizational network study”. In addition, highway distances and travel time were estimated via the best feasible route according to the Open Street Maps highway project. According to the results, 51.34% of breast cancer patients in Brazil were treated outside their municipality of residence, following regionalized flows that respect state borders, generally towards the state capital or other large cities. The results also point to specific exceptions, where some municipalities occupy outstanding positions that extrapolate state borders. Median travel time from the municipality of residence to the municipality of care was nearly 3 hours, and 75% of trips totaled 324km for chemotherapy, 287km for radiotherapy, and 282km for hospitalizations. These results are indicative of the difficulties in access to oncology services, potentially aggravating the illness experience with cancer in terms of impact on the individuals and their families."
  },
  {
    "objectID": "publications/saldanhaMicrodatasusPacotePara2019.html#reference",
    "href": "publications/saldanhaMicrodatasusPacotePara2019.html#reference",
    "title": "Microdatasus: pacote para download e pré-processamento de microdados do Departamento de Informática do SUS (DATASUS)",
    "section": "Reference",
    "text": "Reference\n\n\nSALDANHA, R. DE F.; BASTOS, R. R.; BARCELLOS, C. Microdatasus: pacote para download e pré-processamento de microdados do Departamento de Informática do SUS (DATASUS). Cadernos de Saúde Pública, v. 35, p. e00032419, Sep. 2019."
  },
  {
    "objectID": "publications/saldanhaMicrodatasusPacotePara2019.html#abstract",
    "href": "publications/saldanhaMicrodatasusPacotePara2019.html#abstract",
    "title": "Microdatasus: pacote para download e pré-processamento de microdados do Departamento de Informática do SUS (DATASUS)",
    "section": "Abstract",
    "text": "Abstract\nThis study aimed to develop an algorithm for downloading and preprocessing microdata furnished by the Brazilian Health Informatics Department (DATASUS) for various health information systems, using the R statistical programming language. The package allows downloading and preprocessing data from various health information systems, with the inclusion of labeling categorical fields in the files. The download function was capable of directly accessing and reducing the workload for the selection of microdata files and variables in DATASUS, while the preprocessing function enabled automatic coding of various categorical fields. The package thus enables a continuous workflow in the same program, in which the algorithm allows downloading and preprocessing and other packages in R allow analyzing data from the health information systems in the Brazilian Unified National Health System (SUS)."
  },
  {
    "objectID": "publications/saldanhaPropostaUmObservatorio2017.html#reference",
    "href": "publications/saldanhaPropostaUmObservatorio2017.html#reference",
    "title": "Proposta de um observatório epidemiológico do Sistema Único de Saúde",
    "section": "Reference",
    "text": "Reference\n\n\nSALDANHA, R. DE F. et al. Proposta de um observatório epidemiológico do Sistema Único de Saúde. Cadernos de Saúde Pública, v. 33, p. e00113216, Jan. 2017."
  },
  {
    "objectID": "publications/saldanhaPropostaUmObservatorio2017.html#abstract",
    "href": "publications/saldanhaPropostaUmObservatorio2017.html#abstract",
    "title": "Proposta de um observatório epidemiológico do Sistema Único de Saúde",
    "section": "Abstract",
    "text": "Abstract\nFollowing the creation of the Brazilian Unified National Health System (SUS), the Brazilian Health Informatics Department (DATASUS) was established in 1991, aimed at organizing information systems and databases in health. Online data access and viewing is free and open, using tables and graphs of aggregate data and access to raw data. However, the current form of data access does not fully meet the demands by health system administrators and other users for a flexible, user-friendly tool that allows dealing with various relevant health issues in the knowledge search and decision-making. We propose an ancillary system capable of generating monthly summary reports that are easy to access and understand, with an emphasis on viewing information through graphs and maps."
  },
  {
    "objectID": "publications/saldanhaZonalStatistics2024.html#reference",
    "href": "publications/saldanhaZonalStatistics2024.html#reference",
    "title": "Zonal statistics datasets of climate indicators for Brazilian municipalities",
    "section": "Reference",
    "text": "Reference\n\n\nSALDANHA, R. et al. Zonal statistics datasets of climate indicators for Brazilian municipalities. Environmental Data Science, v. 3, p. e2, 2024."
  },
  {
    "objectID": "publications/saldanhaZonalStatistics2024.html#abstract",
    "href": "publications/saldanhaZonalStatistics2024.html#abstract",
    "title": "Zonal statistics datasets of climate indicators for Brazilian municipalities",
    "section": "Abstract",
    "text": "Abstract\nClimate trends and weather indicators are used in several research fields due to their importance in statistical modeling, frequently used as covariates. Usually, climate indicators are available as grid files with different spatial and time resolutions. The availability of a time series of climate indicators compatible with administrative boundaries is scattered in Brazil, not fully available for several years, and produced with diverse methodologies. In this paper, we propose time series of climate indicators for the Brazilian municipalities produced using zonal statistics derived from the ERA5-Land reanalysis indicators. As a result, we present datasets with zonal statistics of climate indicators with daily data, covering the period from 1950 to 2022."
  },
  {
    "objectID": "publications/silvaTemporalSpatialDistribution2022.html#reference",
    "href": "publications/silvaTemporalSpatialDistribution2022.html#reference",
    "title": "Temporal and spatial distribution trends of polio vaccine coverage in children in Brazil, 2011-2021",
    "section": "Reference",
    "text": "Reference\n\n\nSILVA, T. M. R. D. et al. Temporal and spatial distribution trends of polio vaccine coverage in children in Brazil, 2011-2021. [s.l.] In Review, Aug. 2022. Acesso em: 30 sep. 2023."
  },
  {
    "objectID": "publications/silvaTemporalSpatialDistribution2022.html#abstract",
    "href": "publications/silvaTemporalSpatialDistribution2022.html#abstract",
    "title": "Temporal and spatial distribution trends of polio vaccine coverage in children in Brazil, 2011-2021",
    "section": "Abstract",
    "text": "Abstract\nBackground: Low polio vaccine coverage can result in the spread of Poliovirus to areas free from viral circulation. This study analyzed the temporal trends and spatial distribution of polio vaccine coverage for children under five years of age in Brazil, between 2011 and 2021. Methods: This is an ecological, time-series study (2011 to 2021) with annual vaccine coverages against poliomyelitis, extracted from the Information System of the National Immunization Program from regions of the 27 Brazilian states. The percentage reductions in vaccination coverage in Brazil and in the Regions were calculated. Prais-Winsten regression models were used to analyze time series for the Regions and States, and spatial analysis identified the distribution of clusters (high-high; low-low; high-low and lowhigh) of vaccination coverages across Brazilian municipalities, using a 5% significance level. Results: From 2011 to 2021, the coverage of polio vaccines decreased by 46.1%. There was a progressive increase observed in clusters resulting in low vaccination coverages (136 low-low Brazilian municipalities in 2011 vs 614 in 2021), mostly reported in the North and Northeast regions of the country. There was a downward trend in vaccination coverages in 8 of the 27 States (p ≤ 0.05). Conclusions: The reduction in polio vaccine coverage, as observed in the North and Northeast regions of Brazil, may favor the spread of Poliovirus. Therefore, vaccination strategies should be prioritized for children residing in areas with sharp and recurrent declines in vaccination coverages, including travelers, migrants and refugees."
  },
  {
    "objectID": "publications/souzajuniorComparisonSamplingDesigns2022b.html#reference",
    "href": "publications/souzajuniorComparisonSamplingDesigns2022b.html#reference",
    "title": "Comparison of sampling designs from the two editions of the Brazilian National Health Survey, 2013 and 2019",
    "section": "Reference",
    "text": "Reference\n\n\nSOUZA JÚNIOR, P. R. B. DE et al. Comparison of sampling designs from the two editions of the Brazilian National Health Survey, 2013 and 2019. Cadernos de Saúde Pública, v. 38, p. e00164321, Jul. 2022."
  },
  {
    "objectID": "publications/souzajuniorComparisonSamplingDesigns2022b.html#abstract",
    "href": "publications/souzajuniorComparisonSamplingDesigns2022b.html#abstract",
    "title": "Comparison of sampling designs from the two editions of the Brazilian National Health Survey, 2013 and 2019",
    "section": "Abstract",
    "text": "Abstract\nOur objective is to describe the differences in the sampling plans of the two editions of the Brazilian National Health Survey (PNS 2013 and 2019) and to evaluate how the changes affected the coefficient of variation (CV) and the design effect (Deff) of some estimated indicators. Variables from different parts of the questionnaire were analyzed to cover proportions with different magnitudes. The prevalence of obesity was included in the analysis since anthropometry measurement in the 2019 survey was performed in a subsample. The value of the point estimate, CV, and the Deff were calculated for each indicator, considering the stratification of the primary sampling units, the weighting of the sampling units, and the clustering effect. The CV and the Deff were lower in the 2019 estimates for most indicators. Concerning the questionnaire indicators of all household members, the Deffs were high and reached values greater than 18 for having a health insurance plan. Regarding the indicators of the individual questionnaire, for the prevalence of obesity, the Deff ranged from 2.7 to 4.2, in 2013, and from 2.7 to 10.2, in 2019. The prevalence of hypertension and diabetes per Federative Unit had a higher CV and lower Deff. Expanding the sample size to meet the diverse health objectives and the high Deff are significant challenges for developing probabilistic household-based national survey. New probabilistic sampling strategies should be considered to reduce costs and clustering effects."
  },
  {
    "objectID": "publications/victor2023.html#reference",
    "href": "publications/victor2023.html#reference",
    "title": "Subset modelling: A domain partitioning strategy for data-efficient machine-learning",
    "section": "Reference",
    "text": "Reference\n\n\nRIBEIRO, V. et al. Subset modelling: A domain partitioning strategy for data-efficient machine-learning. Anais do XXXVIII simpósio brasileiro de bancos de dados. Anais...Porto Alegre, RS, Brasil: SBC, 2023."
  },
  {
    "objectID": "publications/victor2023.html#abstract",
    "href": "publications/victor2023.html#abstract",
    "title": "Subset modelling: A domain partitioning strategy for data-efficient machine-learning",
    "section": "Abstract",
    "text": "Abstract\nThe success of machine learning (ML) systems depends on data availability, volume, quality, and efficient computing resources. A challenge in this context is to reduce computational costs while maintaining adequate accuracy of the models. This paper presents a framework to address this challenge. The idea is to identify “subdomains” within the input space, train local models that produce better predictions for samples from that specific subdomain, instead of training a single global model on the full dataset. We experimentally evaluate our approach on two real-world datasets. Our results indicate that subset modelling (i) improves the predictive performance compared to a single global model and (ii) allows data-efficient training."
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Climate data for Brazilian municipalities\n\n\n\n\n\n\nclimate\n\n\n\n\n\n\n\n\n\nNov 20, 2024\n\n\nRaphael Saldanha et al.\n\n\n\n\n\n\n\n\n\n\n\n\nDengue cases screening\n\n\n\n\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\nNov 5, 2024\n\n\nRaphael Saldanha et al.\n\n\n\n\n\n\n\n\n\n\n\n\nSubset Models for Multivariate Time Series Forecast\n\n\nBDA 2024, Orléans\n\n\n\nsubsets\n\n\nmachine learning\n\n\n\nPresentation at BDA2024 conference.\n\n\n\n\n\nOct 22, 2024\n\n\nRaphael Saldanha, Victor Ribeiro, Eduardo Pena, Marcel Pedroso, Reza Akbarinia, Patrick Valduriez, Fabio Porto\n\n\n\n\n\n\n\n\n\n\n\n\nAdvances in climate features engineering and subsets modeling\n\n\nfor Dengue forecasting\n\n\n\nbrclim\n\n\n\nTalk at the HPDaSc Workshop on Data Driven Science.\n\n\n\n\n\nMay 31, 2024\n\n\nRaphael Saldanha\n\n\n\n\n\n\n\n\n\n\n\n\nSubset Models for Multivariate Time Series Forecast\n\n\n1st MulTiSA, ICDE 2024\n\n\n\nsubsets\n\n\nmachine learning\n\n\n\nPresentation at 1st MulTiSA workshop, ICDE 2024 conference.\n\n\n\n\n\nMay 13, 2024\n\n\nRaphael Saldanha, Victor Ribeiro, Eduardo Pena, Marcel Pedroso, Reza Akbarinia, Patrick Valduriez, Fabio Porto\n\n\n\n\n\n\n\n\n\n\n\n\nDisease and climate data fusion for modeling\n\n\nAn application case for Brazil\n\n\n\nbrclim\n\n\n\nTalk at the Inria Zenith’s Team Seminar.\n\n\n\n\n\nMar 21, 2024\n\n\nRaphael Saldanha\n\n\n\n\n\n\n\n\n\n\n\n\nBILIS\n\n\nAn unified indicators database\n\n\n\nbilis\n\n\n\nTalk at the Climate and Health for Health Situation Studies Workshop, IRD.\n\n\n\n\n\nDec 18, 2023\n\n\nRaphael Saldanha\n\n\n\n\n\n\n\n\n\n\n\n\nDisease and climate data fusion for modelling\n\n\nAn application case for Brazil\n\n\n\nbrclim\n\n\n\nTalk at the Climate and Health for Health Situation Studies Workshop, IRD.\n\n\n\n\n\nDec 18, 2023\n\n\nRaphael Saldanha\n\n\n\n\n\n\n\n\n\n\n\n\nDisease and climate data fusion for modelling\n\n\nAn application case for Brazil\n\n\n\nisntd\n\n\nclimate\n\n\ndata\n\n\n\nTalk at the ISNTD Climate & Health Conference, Global Health Resilience panel\n\n\n\n\n\nNov 22, 2023\n\n\nRaphael Saldanha\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "talks/bilis_2023.html#bilis",
    "href": "talks/bilis_2023.html#bilis",
    "title": "BILIS",
    "section": "BILIS",
    "text": "BILIS\n\nBanco de Indicadores do Laboratório de Informação em Saúde, ICICT\nA proposal to create an unified health indicators database"
  },
  {
    "objectID": "talks/bilis_2023.html#motivation",
    "href": "talks/bilis_2023.html#motivation",
    "title": "BILIS",
    "section": "Motivation",
    "text": "Motivation\n\nHealth indicators are necessary at several projects\nProjects usually hires people to compute the indicators\nSeveral projects uses a common group of basic indicators\nThe indicators may be computed with slight differences in methodology\nDependent on project lifecycle"
  },
  {
    "objectID": "talks/bilis_2023.html#objectives",
    "href": "talks/bilis_2023.html#objectives",
    "title": "BILIS",
    "section": "Objectives",
    "text": "Objectives\n\nProvide a set of health indicators\nStandardized methodology\nShared budget\nLong term, independent initiative"
  },
  {
    "objectID": "talks/bilis_2023.html#solutions",
    "href": "talks/bilis_2023.html#solutions",
    "title": "BILIS",
    "section": "Solutions",
    "text": "Solutions\n\nPrimarly use already existing sources of secondary data\n\nDeath records, new born records, hospitalization records, etc\n\nEasy to use R packages to compute health indicators\nOne package, one specific mission"
  },
  {
    "objectID": "talks/bilis_2023.html#packages",
    "href": "talks/bilis_2023.html#packages",
    "title": "BILIS",
    "section": "Packages",
    "text": "Packages\n\nbrpop: yearly population estimates for Brazilian administrative levels, including age groups and sex (for standardized indicators)\nrpcdas: query SIM, SINASC, SIH and other health information systems stored at PCDaS ElasticSearch stack, returning aggregated data per administrative level and time\nrecbilis: query other health information systems stored at ICICT Postgres instance"
  },
  {
    "objectID": "talks/bilis_2023.html#packages-1",
    "href": "talks/bilis_2023.html#packages-1",
    "title": "BILIS",
    "section": "Packages",
    "text": "Packages\n\nbilis: compute health indicators at several combinations of administrative levels (municipality, health region and state) and time units (weekly, monthly and yearly)"
  },
  {
    "objectID": "talks/bilis_2023.html#overview",
    "href": "talks/bilis_2023.html#overview",
    "title": "BILIS",
    "section": "Overview",
    "text": "Overview\n\n\n\n\n\nflowchart LR\nsources_1(SIM, SINASC, SIH) --&gt; pcdas_api(PCDaS API)\npcdas_api --&gt; rpcdas[rpcdas]\nsources_2(SINAN, \\n Sivep-Malária) --&gt; postgres(PostgreSQL Server)\npostgres --&gt; recbilis[recbilis]\npop(Population count) --&gt; brpop[brpop]\nrpcdas --&gt; bilis[bilis]\nrecbilis --&gt; bilis\nbrpop --&gt; bilis\nbilis --&gt; csv(CSV files)\nbilis --&gt; parquet(Parquet files)\nbilis --&gt; db(Databases and destinations)"
  },
  {
    "objectID": "talks/bilis_2023.html#initial-results",
    "href": "talks/bilis_2023.html#initial-results",
    "title": "BILIS",
    "section": "Initial results",
    "text": "Initial results\n\n21 indicators based on RIPSA formulations\nCrude rates\nAge standardized rates is the next step"
  },
  {
    "objectID": "talks/bilis_2023.html#thanks",
    "href": "talks/bilis_2023.html#thanks",
    "title": "BILIS",
    "section": "Thanks!",
    "text": "Thanks!"
  },
  {
    "objectID": "talks/climate_health_workshop_2023.html#introduction",
    "href": "talks/climate_health_workshop_2023.html#introduction",
    "title": "Disease and climate data fusion for modelling",
    "section": "Introduction",
    "text": "Introduction\n\nPostdoctoral researcher at Inria, a French research institute for digital science and technology\nFiocruz, Climate and Health Observatory collaborator\nBSC, Global Health Resilience collaborator"
  },
  {
    "objectID": "talks/climate_health_workshop_2023.html#climate-sensitive-diseases",
    "href": "talks/climate_health_workshop_2023.html#climate-sensitive-diseases",
    "title": "Disease and climate data fusion for modelling",
    "section": "Climate sensitive diseases",
    "text": "Climate sensitive diseases\n\nDirect relationship: floods, droughts, heat waves…\nIndirect relationship\n\n\n\n\n\n\n%%{\n  init: {\n    'theme': 'base',\n    'themeVariables': {\n      'fontSize': '30px'\n    }\n  }\n}%%\nflowchart LR\n\nclimate(Climate) --&gt; vector(Disease vectors) --&gt; health(Human health)\nclimate --&gt; health\nclimate --&gt; social(Social & economic \\n determinants) --&gt; health\n\n\n\n\n\n\n\nClimate necessary conditions to vector viability, reproduction and disease transmission efficiency\n\nClimate indicators may act as proxy variables to vector distribution on statistical models"
  },
  {
    "objectID": "talks/climate_health_workshop_2023.html#a-time-lagged-relationship",
    "href": "talks/climate_health_workshop_2023.html#a-time-lagged-relationship",
    "title": "Disease and climate data fusion for modelling",
    "section": "A time-lagged relationship",
    "text": "A time-lagged relationship\n\n\n\nVector life cycle in a time perspective\nClimate conditions from the past leads to the disease incidence of tomorrow"
  },
  {
    "objectID": "talks/climate_health_workshop_2023.html#climate-data",
    "href": "talks/climate_health_workshop_2023.html#climate-data",
    "title": "Disease and climate data fusion for modelling",
    "section": "Climate data",
    "text": "Climate data\n\n\n\nData sources\n\nIn situ: Weather stations, rain gauges\nRemote: Satellites, drones\n\n\nData products\n\nStatistical surface interpolations\nModel reanalysis"
  },
  {
    "objectID": "talks/climate_health_workshop_2023.html#climate-reanalysis",
    "href": "talks/climate_health_workshop_2023.html#climate-reanalysis",
    "title": "Disease and climate data fusion for modelling",
    "section": "Climate reanalysis",
    "text": "Climate reanalysis"
  },
  {
    "objectID": "talks/climate_health_workshop_2023.html#era5-land-reanalysis",
    "href": "talks/climate_health_workshop_2023.html#era5-land-reanalysis",
    "title": "Disease and climate data fusion for modelling",
    "section": "ERA5-Land reanalysis",
    "text": "ERA5-Land reanalysis\n\n\n\nCopernicus, ECMWF\nGlobal coverage\nHourly data\n1950 to the present (one week delay)\nSpatial resolution ~9km\nSeveral climate indicators"
  },
  {
    "objectID": "talks/climate_health_workshop_2023.html#data-structures",
    "href": "talks/climate_health_workshop_2023.html#data-structures",
    "title": "Disease and climate data fusion for modelling",
    "section": "Data structures",
    "text": "Data structures\n\nClimate indicators: grid data\nDisease incidence: tabular, individual cases aggregated by spatial regions and time spans"
  },
  {
    "objectID": "talks/climate_health_workshop_2023.html#fusioning-data",
    "href": "talks/climate_health_workshop_2023.html#fusioning-data",
    "title": "Disease and climate data fusion for modelling",
    "section": "Fusioning data",
    "text": "Fusioning data"
  },
  {
    "objectID": "talks/climate_health_workshop_2023.html#case-example",
    "href": "talks/climate_health_workshop_2023.html#case-example",
    "title": "Disease and climate data fusion for modelling",
    "section": "Case example",
    "text": "Case example\nZonal Statistics of Climate Indicators from ERA5-Land for Brazilian Municipalities\n\n\n8 climate indicators: maximum, minimum and average temperature, total precipitation, surface pressure, dewpoint, \\(u\\) and \\(v\\) components of wind\n\n6 zonal statistics computation for the 5,570 Brazilian municipalities\n\nMinimum, maximum, average, sum, standard deviation, cell count\n\n\n\nTime coverage: 1950-2022, daily data"
  },
  {
    "objectID": "talks/climate_health_workshop_2023.html#workflow",
    "href": "talks/climate_health_workshop_2023.html#workflow",
    "title": "Disease and climate data fusion for modelling",
    "section": "Workflow",
    "text": "Workflow\n\n\n\n\n\n%%{\n  init: {\n    'theme': 'base',\n    'themeVariables': {\n      'fontSize': '30px'\n    }\n  }\n}%%\nflowchart TD\n\nera5(ERA5-Land \\n indicators) --&gt; hdata(Hourly data)\nbb(Latin America \\n bounding box) --&gt; hdata\n\nhdata --&gt; agg(Aggregation to \\n daily data)\n\nagg --&gt; mun(Municipal boundaries)\nmun --&gt; zs(Zonal statistics)"
  },
  {
    "objectID": "talks/climate_health_workshop_2023.html#era5-land-hourly-to-daily-aggregation",
    "href": "talks/climate_health_workshop_2023.html#era5-land-hourly-to-daily-aggregation",
    "title": "Disease and climate data fusion for modelling",
    "section": "ERA5-Land hourly to daily aggregation",
    "text": "ERA5-Land hourly to daily aggregation\n\nUsage of {KrigR} package to access the Copernicus Climate Data Store API, crop data at server side, download and perform the time aggregation.\n\n\ndownload_ERA(Variable = \"2m_temperature\", DataSet = \"era5-land\", \n             DateStart = \"2022-12-01\", DateStop = \"2022-12-31\",\n             TResolution = \"day\", TStep = 1,\n             FUN = \"max\",\n             Extent = extent(c(-118.47,-34.1,-56.65, 33.28)), \n             Dir = \"dir_name\", FileName = \"file_name.nc\", \n             API_User = \"api_user\", API_Key = \"api_key\")\n\n\nTook ~15 days to download and process the data from 8 climate indicators covering the Latin America region"
  },
  {
    "objectID": "talks/climate_health_workshop_2023.html#era5-land-daily-datasets",
    "href": "talks/climate_health_workshop_2023.html#era5-land-daily-datasets",
    "title": "Disease and climate data fusion for modelling",
    "section": "ERA5-Land Daily datasets",
    "text": "ERA5-Land Daily datasets\n\n\n\nOpen data, available at Zenodo\n7,105 files, 658.7 GB\nReproducible R scripts\nPlans to continuously update this dataset and add more indicators\n\n\n\n\n\n\n\nhttps://rfsaldanha.github.io/data-projects/era5land-daily-latin-america.html"
  },
  {
    "objectID": "talks/climate_health_workshop_2023.html#zonal-statistics",
    "href": "talks/climate_health_workshop_2023.html#zonal-statistics",
    "title": "Disease and climate data fusion for modelling",
    "section": "Zonal statistics",
    "text": "Zonal statistics\n\n\n\nChallenge to handle the amount of data\n\n\n~ 4 billion computational tasks\nStrategy\n\nGroup the tasks into chunks and compute in parallel\n\nSave results into columnar-oriented databases for fast data retrieval (duckdb and parquet)\nDAG (Directed-Acyclic Graph) approach to orchestrate computation, with the {targets} package"
  },
  {
    "objectID": "talks/climate_health_workshop_2023.html#computation",
    "href": "talks/climate_health_workshop_2023.html#computation",
    "title": "Disease and climate data fusion for modelling",
    "section": "Computation",
    "text": "Computation\n\nZonal statistics weighted by the fraction of the cell that is covered, with the {exactextractr} package\n\n\nexact_extract(\n  x = rst,\n  y = pol,\n  fun = \"mean\"\n)"
  },
  {
    "objectID": "talks/climate_health_workshop_2023.html#results",
    "href": "talks/climate_health_workshop_2023.html#results",
    "title": "Disease and climate data fusion for modelling",
    "section": "Results",
    "text": "Results\n\n\n\n\n\n\n\nERA5-Land indicators\nDaily time-aggregating functions\nSpatial zonal statistics\n\n\n\nTemperature (2m)\nmean, max, min\nmax, min, stdev, count\n\n\nDewpoint temp. (2m)\nmean\nmax, min, stdev, count\n\n\n\n\\(u\\) component of wind\nmean\nmax, min, stdev, count\n\n\n\n\\(v\\) component of wind\nmean\nmax, min, stdev, count\n\n\nSurface pressure\nmean\nmax, min, stdev, count\n\n\nTotal precipitation\nsum\nmax, min, stdev, count, sum"
  },
  {
    "objectID": "talks/climate_health_workshop_2023.html#section",
    "href": "talks/climate_health_workshop_2023.html#section",
    "title": "Disease and climate data fusion for modelling",
    "section": "",
    "text": "6,085,749,761 records"
  },
  {
    "objectID": "talks/climate_health_workshop_2023.html#temperature",
    "href": "talks/climate_health_workshop_2023.html#temperature",
    "title": "Disease and climate data fusion for modelling",
    "section": "Temperature",
    "text": "Temperature"
  },
  {
    "objectID": "talks/climate_health_workshop_2023.html#precipitation",
    "href": "talks/climate_health_workshop_2023.html#precipitation",
    "title": "Disease and climate data fusion for modelling",
    "section": "Precipitation",
    "text": "Precipitation\n\n\n\n\n\n\n\n\n\n\n\n\nRio de Janeiro municipalities. January 1, 2010."
  },
  {
    "objectID": "talks/climate_health_workshop_2023.html#resolution-and-spatial-variability",
    "href": "talks/climate_health_workshop_2023.html#resolution-and-spatial-variability",
    "title": "Disease and climate data fusion for modelling",
    "section": "Resolution and spatial variability",
    "text": "Resolution and spatial variability\n\n\n\nBrazilian municipalities size variation\n\nAltamira (PA): 159,533 km2\nSanta Cruz de Minas (MG): 3 km2\n\n\nERA5-Land cell: ~ 100 km2\nSeveral intersecting cells and different climate regimes\nFew intersecting cells or fractions"
  },
  {
    "objectID": "talks/climate_health_workshop_2023.html#next-steps",
    "href": "talks/climate_health_workshop_2023.html#next-steps",
    "title": "Disease and climate data fusion for modelling",
    "section": "Next steps…",
    "text": "Next steps…\n\n\n\nContinuous update\nHuman settlements, population-weighted zonal statistics\nCompute climate time-series features: heat waves, persistent rains, etc.\nAdopt climate products with finer resolutions when possible (CHIRPS)\nExpand methodology to other countries\nMethodological paper on reviewing phase"
  },
  {
    "objectID": "talks/climate_health_workshop_2023.html#thanks",
    "href": "talks/climate_health_workshop_2023.html#thanks",
    "title": "Disease and climate data fusion for modelling",
    "section": "Thanks!",
    "text": "Thanks!\nContact, data links, R packages and short tutorials available at rfsaldanha.github.io"
  },
  {
    "objectID": "talks/hpdasc_workshop_2024.html#context-of-climate-sensitive-diseases",
    "href": "talks/hpdasc_workshop_2024.html#context-of-climate-sensitive-diseases",
    "title": "Advances in climate features engineering and subsets modeling",
    "section": "Context of Climate-Sensitive diseases",
    "text": "Context of Climate-Sensitive diseases\n\nDirect relationship: floods, droughts, heat waves…\nIndirect relationship\n\n\n\n\n\n\n%%{\n  init: {\n    'theme': 'base',\n    'themeVariables': {\n      'fontSize': '30px'\n    }\n  }\n}%%\nflowchart LR\n\nclimate(Climate) --&gt; vector(Disease vectors) --&gt; health(Human health)\nclimate --&gt; health\nclimate --&gt; social(Social & economic \\n determinants) --&gt; health"
  },
  {
    "objectID": "talks/hpdasc_workshop_2024.html#a-time-lagged-relationship",
    "href": "talks/hpdasc_workshop_2024.html#a-time-lagged-relationship",
    "title": "Advances in climate features engineering and subsets modeling",
    "section": "A time-lagged relationship",
    "text": "A time-lagged relationship\n\n\n\nVector life cycle from a time perspective\nClimate conditions from the past leads to the disease incidence of tomorrow"
  },
  {
    "objectID": "talks/hpdasc_workshop_2024.html#era5-land-reanalysis",
    "href": "talks/hpdasc_workshop_2024.html#era5-land-reanalysis",
    "title": "Advances in climate features engineering and subsets modeling",
    "section": "ERA5-Land reanalysis",
    "text": "ERA5-Land reanalysis\n\n\n\nCopernicus, ECMWF\nGlobal coverage\nHourly data\n1950 to the present (one week delay)\nSpatial resolution ~9km\nSeveral climate indicators"
  },
  {
    "objectID": "talks/hpdasc_workshop_2024.html#challenge-on-data-structures",
    "href": "talks/hpdasc_workshop_2024.html#challenge-on-data-structures",
    "title": "Advances in climate features engineering and subsets modeling",
    "section": "Challenge on data structures",
    "text": "Challenge on data structures\n\nClimate indicators: grid data\nDisease incidence: tabular, individual cases aggregated by spatial regions and time spans"
  },
  {
    "objectID": "talks/hpdasc_workshop_2024.html#zonal-statistics",
    "href": "talks/hpdasc_workshop_2024.html#zonal-statistics",
    "title": "Advances in climate features engineering and subsets modeling",
    "section": "Zonal statistics",
    "text": "Zonal statistics"
  },
  {
    "objectID": "talks/hpdasc_workshop_2024.html#resulting-products",
    "href": "talks/hpdasc_workshop_2024.html#resulting-products",
    "title": "Advances in climate features engineering and subsets modeling",
    "section": "Resulting products",
    "text": "Resulting products\n\n\n\nERA5-Land daily datasets\n\n7,105 files, 658.7 GB\n24,242 downloads on Zenodo\n\nDaily zonal statistics of climate indicators\n\n8 selected indicators, 5,570 municipalities\n6,085,749,761 records covering 1950-2023"
  },
  {
    "objectID": "talks/hpdasc_workshop_2024.html#precipitation",
    "href": "talks/hpdasc_workshop_2024.html#precipitation",
    "title": "Advances in climate features engineering and subsets modeling",
    "section": "Precipitation",
    "text": "Precipitation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRio de Janeiro municipalities. January 1, 2010."
  },
  {
    "objectID": "talks/hpdasc_workshop_2024.html#angra-dos-reis",
    "href": "talks/hpdasc_workshop_2024.html#angra-dos-reis",
    "title": "Advances in climate features engineering and subsets modeling",
    "section": "Angra dos Reis",
    "text": "Angra dos Reis"
  },
  {
    "objectID": "talks/hpdasc_workshop_2024.html#publications-products",
    "href": "talks/hpdasc_workshop_2024.html#publications-products",
    "title": "Advances in climate features engineering and subsets modeling",
    "section": "Publications & Products",
    "text": "Publications & Products\n\n\n\nPaper on Environmental Data Science journal (Saldanha et al. 2024)\nDatasets on Zenodo: more than 34,000 downloads\nbrclimr package to retrieve climate data of Brazilian municipalities, Almost 4,000 downloads on CRAN."
  },
  {
    "objectID": "talks/hpdasc_workshop_2024.html#subset-models-for-multivariate-time-series-forecast",
    "href": "talks/hpdasc_workshop_2024.html#subset-models-for-multivariate-time-series-forecast",
    "title": "Advances in climate features engineering and subsets modeling",
    "section": "Subset models for multivariate time series forecast",
    "text": "Subset models for multivariate time series forecast\n\n\n\nData may present intrinsic diversity of samples, affecting model’s performance on different parts of the input\nGlobal models: use all available time series\nLocal models: use only time series pertaining to each sample\nData subsets models: our proposal\n\n\n\nPaper on ICDE2024, Multivariate Time Series Analytics workshop"
  },
  {
    "objectID": "talks/hpdasc_workshop_2024.html#case-example",
    "href": "talks/hpdasc_workshop_2024.html#case-example",
    "title": "Advances in climate features engineering and subsets modeling",
    "section": "Case example",
    "text": "Case example\n\nDengue disease is transmitted by mosquitoes and is a Public Health concern. Record number cases on 2024 in Brazil, tendency to increase with global warming\nA typical forecasting model is targeted to predict number of cases based on climate indicators (rain and temperature)\nA global model would use data from all municipalities, facing difficulties related to distinct temporal and spatial disease transmission patterns"
  },
  {
    "objectID": "talks/hpdasc_workshop_2024.html#experimental-setup",
    "href": "talks/hpdasc_workshop_2024.html#experimental-setup",
    "title": "Advances in climate features engineering and subsets modeling",
    "section": "Experimental setup",
    "text": "Experimental setup\n\nIdentify data subsets considering dengue cases and covariates patterns across municipalities with DTW distance.\n\nSelect the optimum number of subsets (\\(k\\)) considering silhouette score\n\nTrain random forest Global Model with and without the subset id feature information\nTrain random forest Subsets Models\nEvaluate forecasting model’s performance on test data"
  },
  {
    "objectID": "talks/hpdasc_workshop_2024.html#clustering-results",
    "href": "talks/hpdasc_workshop_2024.html#clustering-results",
    "title": "Advances in climate features engineering and subsets modeling",
    "section": "Clustering results",
    "text": "Clustering results\n\n\n\n\\(k = 5\\) returned the highest silhouette score\nPartition sizes: \\(g_1 = 69\\), \\(g_2 = 62\\), \\(g_3 = 82\\), \\(g_4 = 102\\), \\(g_5 = 18\\)"
  },
  {
    "objectID": "talks/hpdasc_workshop_2024.html#model-results",
    "href": "talks/hpdasc_workshop_2024.html#model-results",
    "title": "Advances in climate features engineering and subsets modeling",
    "section": "Model results",
    "text": "Model results"
  },
  {
    "objectID": "talks/hpdasc_workshop_2024.html#conclusions-and-next-steps",
    "href": "talks/hpdasc_workshop_2024.html#conclusions-and-next-steps",
    "title": "Advances in climate features engineering and subsets modeling",
    "section": "Conclusions and next steps",
    "text": "Conclusions and next steps\n\nSubsets models rendered better performance than global models on 116 municipalities from 333 (34.83%)\nSubsets models overall performance is related to the partition’s size. Bigger partitions (more municipalities) have more training data.\nWe are working on different clustering strategies (constraints in size and featured-based approaches) and apply different learners on model training"
  },
  {
    "objectID": "talks/multisa_subsets_2024.html#introduction",
    "href": "talks/multisa_subsets_2024.html#introduction",
    "title": "Subset Models for Multivariate Time Series Forecast",
    "section": "Introduction",
    "text": "Introduction\n\nAbundant multivariate time series, good opportunity for forecasting machine learning methods\nData may present intrinsic diversity of samples, affecting model’s performance on different parts of the input\nGlobal models: use all available time series\nLocal models: use only time series pertaining to each sample\nData subsets models: our proposal"
  },
  {
    "objectID": "talks/multisa_subsets_2024.html#case-example",
    "href": "talks/multisa_subsets_2024.html#case-example",
    "title": "Subset Models for Multivariate Time Series Forecast",
    "section": "Case example",
    "text": "Case example\n\nDengue disease is transmitted by mosquitoes and is a Public Health concern. Record number cases on 2024 in Brazil, tendency to increase with global warming\nA typical forecasting model is targeted to predict number of cases based on climate indicators (rain and temperature)\nA global model would use data from all municipalities, facing difficulties related to distinct temporal and spatial disease transmission patterns"
  },
  {
    "objectID": "talks/multisa_subsets_2024.html#objective",
    "href": "talks/multisa_subsets_2024.html#objective",
    "title": "Subset Models for Multivariate Time Series Forecast",
    "section": "Objective",
    "text": "Objective\n\nPropose a subset modeling framework\nAccommodate regional variations across diverse units (e.g. municipalities)\nCost-effective training with robust prediction capabilities in comparison with global models"
  },
  {
    "objectID": "talks/multisa_subsets_2024.html#subset-modeling-framework",
    "href": "talks/multisa_subsets_2024.html#subset-modeling-framework",
    "title": "Subset Models for Multivariate Time Series Forecast",
    "section": "Subset modeling framework",
    "text": "Subset modeling framework\n\n\n\nIdentify subsets within the dataset with similar patterns\nTrain models for each subset\nUse the model trained on the subset data for prediction"
  },
  {
    "objectID": "talks/multisa_subsets_2024.html#datasets",
    "href": "talks/multisa_subsets_2024.html#datasets",
    "title": "Subset Models for Multivariate Time Series Forecast",
    "section": "Datasets",
    "text": "Datasets\n\nDengue dataset. Weekly cases count, from 2011 to 2020, for 333 municipalities.\nClimate dataset. Average maximum and minimum temperature, total precipitation. Same time and spatial units and coverage.\nAll indicators were standardized (with zero mean and one SD)"
  },
  {
    "objectID": "talks/multisa_subsets_2024.html#experimental-setup",
    "href": "talks/multisa_subsets_2024.html#experimental-setup",
    "title": "Subset Models for Multivariate Time Series Forecast",
    "section": "Experimental setup",
    "text": "Experimental setup\n\nIdentify data subsets considering dengue cases and covariates patterns across municipalities with DTW distance.\n\nSelect the optimum number of subsets (\\(k\\)) considering silhouette score\n\nTrain random forest Global Model with and without the subset id feature information\nTrain random forest Subsets Models\nEvaluate forecasting model’s performance on test data"
  },
  {
    "objectID": "talks/multisa_subsets_2024.html#clustering-results",
    "href": "talks/multisa_subsets_2024.html#clustering-results",
    "title": "Subset Models for Multivariate Time Series Forecast",
    "section": "Clustering results",
    "text": "Clustering results\n\n\n\n\\(k = 5\\) returned the highest silhouette score\nPartition sizes: \\(g_1 = 69\\), \\(g_2 = 62\\), \\(g_3 = 82\\), \\(g_4 = 102\\), \\(g_5 = 18\\)"
  },
  {
    "objectID": "talks/multisa_subsets_2024.html#model-results",
    "href": "talks/multisa_subsets_2024.html#model-results",
    "title": "Subset Models for Multivariate Time Series Forecast",
    "section": "Model results",
    "text": "Model results"
  },
  {
    "objectID": "talks/multisa_subsets_2024.html#conclusions-and-next-steps",
    "href": "talks/multisa_subsets_2024.html#conclusions-and-next-steps",
    "title": "Subset Models for Multivariate Time Series Forecast",
    "section": "Conclusions and next steps",
    "text": "Conclusions and next steps\n\nWe specifically addressed and formalized the subsets approach in a multivariate time series setting\nSubsets models rendered better performance than global models on 116 municipalities from 333 (34.83%)\nSubsets models overall performance is related to the partition’s size. Bigger partitions (more municipalities) have more training data.\nFurther work may advance on clustering strategies (constraints in size and featured-based approaches) and apply different learners on model training"
  },
  {
    "objectID": "talks/multisa_subsets_2024.html#thanks",
    "href": "talks/multisa_subsets_2024.html#thanks",
    "title": "Subset Models for Multivariate Time Series Forecast",
    "section": "Thanks!",
    "text": "Thanks!\nContact and more info at\nrfsaldanha.github.io"
  }
]